<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Systems Librarianship</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="Entry level book on systems librarianship.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="p1-systems-librarianship.html"><strong aria-hidden="true">1.</strong> Systems Librarianship</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="1a-history-linux-unix.html"><strong aria-hidden="true">1.1.</strong> History of Unix and Linux</a></li><li class="chapter-item expanded "><a href="1b-what-is-linux.html"><strong aria-hidden="true">1.2.</strong> What is Linux?</a></li><li class="chapter-item expanded "><a href="1c-what-is-sysadmin.html"><strong aria-hidden="true">1.3.</strong> What is Systems Administration?</a></li><li class="chapter-item expanded "><a href="1d-what-is-syslib.html"><strong aria-hidden="true">1.4.</strong> What is Systems Librarianship?</a></li></ol></li><li class="chapter-item expanded "><a href="p2-project-management-and-CLI.html"><strong aria-hidden="true">2.</strong> Project Management and CLI (Command Line Interface) Introduction</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="2a-using-gcloud-virtual-machines.html"><strong aria-hidden="true">2.1.</strong> Using gcloud for Virtual Machines</a></li><li class="chapter-item expanded "><a href="2b-learn-the-cli.html"><strong aria-hidden="true">2.2.</strong> Learn the CLI</a></li><li class="chapter-item expanded "><a href="2c-learn-nano.html"><strong aria-hidden="true">2.3.</strong> Using the Nano Text Editor</a></li><li class="chapter-item expanded "><a href="2d-documenting-git-github-markdown.html"><strong aria-hidden="true">2.4.</strong> Documenting with Git, GitHub, and Markdown</a></li></ol></li><li class="chapter-item expanded "><a href="p3-working-on-the-CLI.html"><strong aria-hidden="true">3.</strong> Working on the CLI</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="3a-searching-with-grep.html"><strong aria-hidden="true">3.1.</strong> Searching with grep</a></li><li class="chapter-item expanded "><a href="3b-managing-software.html"><strong aria-hidden="true">3.2.</strong> Managing Software</a></li><li class="chapter-item expanded "><a href="3c-library-search.html"><strong aria-hidden="true">3.3.</strong> Library Search</a></li></ol></li><li class="chapter-item expanded "><a href="p4-creating-a-lamp-server.html"><strong aria-hidden="true">4.</strong> Creating a LAMP Server</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="4a-installing-the-apache-web-server.html"><strong aria-hidden="true">4.1.</strong> Installing the Apache Web Server</a></li><li class="chapter-item expanded "><a href="4b-installing-configuring-php.html"><strong aria-hidden="true">4.2.</strong> Installing and Configuring PHP</a></li><li class="chapter-item expanded "><a href="4c-installing-configuring-mysql.html"><strong aria-hidden="true">4.3.</strong> Installing and Configuring MySQL</a></li></ol></li><li class="chapter-item expanded "><a href="p5-integrated-library-systems.html"><strong aria-hidden="true">5.</strong> DIY Integrated Library Systems</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="5a-introduction-to-relational-databases.html"><strong aria-hidden="true">5.1.</strong> Introduction to Relational Databases</a></li><li class="chapter-item expanded "><a href="5b-basic-opac.html"><strong aria-hidden="true">5.2.</strong> Creating a Bare Bones OPAC</a></li><li class="chapter-item expanded "><a href="5c-basic-opac-admin.html"><strong aria-hidden="true">5.3.</strong> Creating a Bare Bones Cataloging Module</a></li></ol></li><li class="chapter-item expanded "><a href="p6-library-website-project.html"><strong aria-hidden="true">6.</strong> Library Website Project</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="6a-install-wordpress.html"><strong aria-hidden="true">6.1.</strong> Install WordPress</a></li><li class="chapter-item expanded "><a href="6b-install-omeka.html"><strong aria-hidden="true">6.2.</strong> Install Omeka</a></li><li class="chapter-item expanded "><a href="6c-install-koha.html"><strong aria-hidden="true">6.3.</strong> Install the Koha Integrated Library System</a></li></ol></li><li class="chapter-item expanded "><a href="p7-conclusion.html"><strong aria-hidden="true">7.</strong> Conclusion</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Systems Librarianship</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/cseanburns/systems_librarianship" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="systems-librarianship"><a class="header" href="#systems-librarianship">Systems Librarianship</a></h1>
<p>Author: C. Sean Burns<br />
<a href="https://github.com/cseanburns/systems-librarianship/releases/tag/V2">Date, version 2: 2024-04-01</a><br />
Email: <a href="sean.burns@uky.edu">sean.burns@uky.edu</a><br />
Website: <a href="https://cseanburns.net">cseanburns.net</a><br />
GitHub: <a href="https://github.com/cseanburns">@cseanburns</a></p>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>The goal of this book is to provide a technical introduction to the basics of systems librarianship using Linux.
The book is used alongside a course on systems librarianship that the author teaches.</p>
<p>The course teaches the following skills:</p>
<ol start="2">
<li>how to use cloud computing resources and create virtual machines;</li>
<li>how to use the Linux command line in order to become more efficient computer users and more comfortable with using computers in general;</li>
<li>how to document technical information using Git and GitHub;</li>
<li>how to create a LAMP server, websites, and create a bare bones OPAC;</li>
<li>how to install and configure content management systems, and;</li>
<li>how to install and configure an integrated library system.</li>
</ol>
<p>The main overarching goals of this work are:</p>
<ol>
<li>to foster self-efficacy with computers and an enthusiasm for foundational computer technologies, and</li>
<li>to provide library science students a starting point in a career in systems librarianship or related positions.</li>
</ol>
<h2 id="about-this-book"><a class="header" href="#about-this-book">About This Book</a></h2>
<p>I created and began teaching a Systems Librarianship course in 2023 in order
to help librarians become proficient in the kinds of technology used to manage and provide electronic resources.
I also want to help library science students see systems librarianship as a potential career path.</p>
<p>Since I use this book for my Systems Librarianship course, which I teach in the spring semesters, this book will be a live document.
Each semester that I teach this course, I will update the content in order to address changes in the technology and to
edit for clarity when I discover some aspect of the book causes confusion or does not provide enough information.</p>
<p>This book is not a comprehensive introduction to systems librarianship.
For example, this book does not cover software coding nor managerial duties,
like issuing requests for proposals for software products, or budgeting.
It is designed as an entry level course in the technical aspects of systems librarianship and is meant to go hand-in-hand with
other courses taught in our library science program.
That includes my course on <a href="https://cseanburns.github.io/electronic_resource_mgmt/">electronic resource management</a> but also other courses that my colleagues teach.</p>
<p>A small part of this book draws from my course on <a href="https://cseanburns.github.io/linux_sysadmin/">Linux Systems Administration</a>,
which I teach in the fall semesters in our undergraduate ICT program.</p>
<p>If you use this work, in whole or in part, please reach out to me to let me know.
I accept suggestions for improvement, via email or GitHub.</p>
<h3 id="technical-note"><a class="header" href="#technical-note">Technical Note</a></h3>
<p>I write the text for this work in Markdown and use <a href="https://github.com/rust-lang/mdBook">mdBook</a> to build the output.
The Markdown source code is in GitHub: <a href="systems_lib">Systems Librarianship</a>.
Use the search function on this site to search for specific topics or keywords.
If the reader desires a PDF copy of this work, the printer icon at the top right of the page will print to PDFs.</p>
<p>The content in this book is open access and
licensed under the <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0</a> license.
Feel free to fork it on [GitHub][systemslib] and modify it for your own needs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="history-of-unix-and-linux"><a class="header" href="#history-of-unix-and-linux">History of Unix and Linux</a></h1>
<p>An outline of the history of Unix and Linux.</p>
<p><strong>Note</strong>: this section is borrowed from my <a href="https://cseanburns.github.io/linux_sysadmin/">Linux Systems Administration</a> course.</p>
<h2 id="location-bell-labs-part-of-att-new-jersey-late-1960s-through-early-1970s"><a class="header" href="#location-bell-labs-part-of-att-new-jersey-late-1960s-through-early-1970s">Location: Bell Labs, part of AT&amp;T (New Jersey), late 1960s through early 1970s</a></h2>
<p>Before there was Linux, there was (and still is) Unix.
Unix began in the late 1960s and was first released in the early 1970s at Bell Labs, part of AT&amp;T in New Jersey, with an operating system called Multics.
Multics was an early <a href="https://en.wikipedia.org/wiki/Time-sharing">time-sharing system</a>; i.e., it allowed more than one person to use it the system.
Despite its innovative approach, Multics was fraught with issues and slowly abandoned.
In the midst of this abandonment, <a href="http://cs.bell-labs.co/who/ken/">Ken Thompson</a> stumbled upon an old PDP-7 and started writing what would become UNIX.
This specific version of UNIX would later be known as Research Unix.
The project caught the attention of <a href="https://www.bell-labs.com/usr/dmr/www/">Dennis Ritchie</a>, the creator of the C programming language, who joined Thompson's efforts.
Together they laid the groundwork for a revolution in computing.</p>
<h2 id="location-berkeley-ca-university-of-california-berkeley-early-to-mid-1970s"><a class="header" href="#location-berkeley-ca-university-of-california-berkeley-early-to-mid-1970s">Location: Berkeley, CA (University of California, Berkeley), early to mid 1970s</a></h2>
<p>The evolution of Unix continued through the early to mid-1970s at Bell Labs.
Ken Thompson visited the University of California, Berkeley, where he helped install Version 6 of <a href="https://en.wikipedia.org/wiki/Berkeley_Software_Distribution">UNIX</a>.
This marked a significant moment in the system's history.
At Berkeley, several contributors, including <a href="https://en.wikipedia.org/wiki/Bill_Joy">Bill Joy</a>, played vital roles in its development.
Joy was particularly influential.
He created the <a href="https://sites.google.com/a/bostic.com/keithbostic/vi/">vi</a> text editor, a precursor of the still popular <a href="https://www.vim.org/">Vim</a> editor, and many other essential programs.
He also co-founded Sun Microsystems.
This collaborative effort at Berkeley eventually led to the creation of the Berkeley Software Distribution,
or <a href="https://en.wikipedia.org/wiki/Berkeley_Software_Distribution">BSD Unix</a>, a landmark in the history of UNIX and computing as a whole.</p>
<h2 id="att"><a class="header" href="#att">AT&amp;T</a></h2>
<p>Until its breakup in 1984, AT&amp;T operated under a unique agreement with the U.S. government that restricted the company from
profiting off patents not directly related to its telecommunications businesses.
This arrangement helped shield AT&amp;T from monopolistic charges, but it also came with a significant limitation:
they could not commercialize UNIX.
The landscape changed dramatically after the breakup of AT&amp;T.
With the constraints lifted, AT&amp;T was allowed to release and sell System V UNIX,
which would emerge as the standard bearer of commercial UNIX.
This transition marked a turning point in the history of computing, positioning UNIX as a central player in the commercial technology market.</p>
<h2 id="location-boston-ma-mit-early-1980s-through-early-1990s"><a class="header" href="#location-boston-ma-mit-early-1980s-through-early-1990s">Location: Boston, MA (MIT), early 1980s through early 1990s</a></h2>
<p>In Boston, MA, at MIT during the early 1980s through the early 1990s, a significant shift in the software industry was taking place.
In the late 1970s, <a href="https://en.wikipedia.org/wiki/Richard_Stallman">Richard Stallman</a> observed the growing trend in the commercialization of software.
As a result, hardware vendors began to stop sharing the code they developed to make their hardware work.
This paradigm change was further solidified by the Copyright Act of 1976, which made software code eligible for copyright protection.
Stallman battled against this new direction and responded by creating the <a href="https://www.gnu.org/gnu/gnu.html">GNU project</a>,
formalizing and embracing the free software philosophy, and developing influential tools such as GNU Emacs, a popular text editor,
and many other programs.
The GNU project was an ambitious attempt to create a completely free software operating system that was Unix-like, called GNU.
By the early 1990s, Stallman and others had developed all the software needed for a full operating system,
except for a <a href="https://en.wikipedia.org/wiki/Kernel_(operating_system)">kernel</a>.
However, this encompassing project included the creation of the Bash shell,
written by <a href="https://opuslogica.com/">Brian Fox</a>, reflecting a profound commitment to free and open software.</p>
<p>The GNU philosophy includes several propositions that define free software:</p>
<blockquote>
<p>The four freedoms, per GNU Project:
0. The freedom to run the program as you wish,
for any purpose (freedom 0).</p>
<ol>
<li>The freedom to study how the program works,
and change it so it does your computing as you wish (freedom 1).
Access to the source code is a precondition for this.</li>
<li>The freedom to redistribute copies so you can help others (freedom 2).</li>
<li>The freedom to distribute copies of your modified
versions to others (freedom 3).
By doing this you can give the whole community a chance
to benefit from your changes.
Access to the source code is a precondition for this.</li>
</ol>
</blockquote>
<p><a href="https://www.gnu.org/philosophy/free-sw.html">The Four Freedoms</a></p>
<h2 id="the-unix-wars-and-the-lawsuit-late-1980s-through-the-early-1990s"><a class="header" href="#the-unix-wars-and-the-lawsuit-late-1980s-through-the-early-1990s">The Unix wars and the lawsuit, late 1980s through the early 1990s</a></h2>
<p>During the late 1980s through the early 1990s, the so-called "Unix wars" and an ensuing lawsuit marked a contentious period
in the history of computing.
Following its breakup, AT&amp;T began to commercialize Unix.
This lead to distinct differences between the Unix created by AT&amp;T Unix and the Unix developed as BSD Unix.
The former was aimed at commercial markets, while the latter was targeted at researchers and academics.
These contrasting objectives led to legal friction, culminating in UNIX Systems Laboratories, Inc.
(USL, part of AT&amp;T) suing Berkeley Software Design, Inc. (BSDi, part of the University of California, Berkeley)
for copyright and trademark violations.
Ultimately, USL lost the case, but not before the lawsuit had created significant obstacles for BSD Unix.
The legal battle delayed the adoption of BSD Unix and left a lasting impact on the development and dissemination of Unix systems.</p>
<h2 id="linux-linus-torvalds-university-of-helsinki-finland-early-1990s"><a class="header" href="#linux-linus-torvalds-university-of-helsinki-finland-early-1990s">Linux, Linus Torvalds, University of Helsinki, Finland, early 1990s</a></h2>
<p>Meanwhile, on August 25, 1991, at the University of Helsinki in Finland,
<a href="https://www.cs.helsinki.fi/u/torvalds/">Linus Torvalds</a>, a young computer science student,
announced that he had started working on a free operating system kernel for the 386 CPU architecture.
This <a href="https://www.kernel.org/">kernel</a> would later be famously named Linux, a kind of portmanteau of Linus and Unix.
It's essential to understand that <strong>Linux</strong> technically refers only to the kernel,
which handles startup, devices, memory, resources, and more,
but does not provide user land utilities—the kind of software that people use on their computers.</p>
<p>Torvalds' motivation for this project was both to learn about OS development and to have access to a Unix-like system.
He did had access to an Unix-like system called <a href="https://www.minix3.org/">MINIX</a>, but MINIX was limited by technical and copyright restrictions.
Interestingly, Torvalds has stated that if a BSD or GNU Hurd operating system were available at that time,
he might not have created the Linux kernel at all.
However, he and others took the GNU utilities and created what is now widely referred to as Linux or GNU/Linux.
This amalgamation of Torvalds' kernel and GNU utilities marked a critical point in the evolution of free and open-source software,
fostering a global community of developers and users.</p>
<h2 id="distributions-early-1990s-through-today"><a class="header" href="#distributions-early-1990s-through-today">Distributions, early 1990s through today</a></h2>
<p>Soon after the development of Linux in the early 1990s, enthusiasts and developers started creating their own Linux and
GNU-based operating systems.
They customized these systems to suit various needs and preferences and would then distribute these customized versions to others.
As a result of this practice, these Linux operating systems became known as Linux <em>distributions</em>.
This phenomenon has led to a rich ecosystem of Linux distributions, catering to different user bases,
industries, and interests, and has played a central role in the continued growth and diversification of open-source computing.</p>
<p>The two oldest distributions that are still in active development include:</p>
<ul>
<li><a href="http://www.slackware.com/">Slackware</a></li>
<li><a href="https://www.debian.org/">Debian</a></li>
</ul>
<h2 id="short-history-of-bsd-1970s-through-today"><a class="header" href="#short-history-of-bsd-1970s-through-today">Short History of BSD, 1970s through today</a></h2>
<p>Unix and Unix-derivatives continue to exist and thrive today.
The history of the Berkeley Software Distribution (BSD) of Unix spans from the 1970s to today and
is closely intertwined with the evolution of Unix, generally.
Early Unix version numbers 1-6 eventually led to the development of BSD versions 1-4.
By the time of BSD 4.3, all versions still contained some AT&amp;T code.
A desire to remove this proprietary code led to the creation of BSD Net/1.</p>
<p>The effort continued until all AT&amp;T code was successfully removed by BSD Net/2.
This version was then ported to the Intel 386 processor, resulting in 386BSD, made available in 1992,
a year after the Linux kernel was released.</p>
<p>386BSD eventually split into two distinct projects: <a href="https://www.netbsd.org/">NetBSD</a> and <a href="https://www.freebsd.org/">FreeBSD</a>.
Later, NetBSD split into another project, giving rise to <a href="https://www.openbsd.org/">OpenBSD</a>.
All three of these BSDs are still in active development today, and each has a unique focus:</p>
<ul>
<li><strong>NetBSD</strong> is known for its focus on portability, finding applications in various environments such as MacOS and even NASA projects.</li>
<li><strong>FreeBSD</strong> is recognized for its wide applicability and has been utilized by notable companies and products like WhatsApp,
Netflix, PlayStation 4, and MacOS.</li>
<li><strong>OpenBSD</strong> emphasizes security and has contributed several essential applications in this domain.</li>
</ul>
<p>This intricate journey of BSD, marked by splits, adaptations, and varied focuses, has cemented its place in the history of operating systems,
and allowed it to cater to a wide range of applications and audiences.</p>
<blockquote>
<p>MacOS is based on <a href="http://www.puredarwin.org/">Darwin</a>, is <a href="https://www.opengroup.org/membership/forums/platform/unix">technically UNIX</a>, and is partly based on FreeBSD with some code
coming from the other BSDs. See <a href="https://apple.stackexchange.com/questions/401832/why-is-macos-often-referred-to-as-darwin">Why is macOS often referred to as 'Darwin'?</a> for a short history.</p>
</blockquote>
<h2 id="short-history-of-gnu-1980s-through-today"><a class="header" href="#short-history-of-gnu-1980s-through-today">Short History of GNU, 1980s through today</a></h2>
<p>The history of GNU, particularly the GNU Hurd kernel, traces back to the 1980s and continues to evolve today.
The GNU Hurd, despite its long development process, remains in a pre-production state.
The latest release of this kernel was version 0.9, which was released in December 2016.
Even though it has not yet reached full maturity, a complete operating system based on the GNU Hurd can be used.
For example, <a href="https://www.debian.org/ports/hurd/">Debian GNU/Hurd</a> represents one such implementation.
This ongoing work on the GNU Hurd exemplifies the free and open-source community's commitment to innovation and collaboration.</p>
<h2 id="free-and-open-source-licenses"><a class="header" href="#free-and-open-source-licenses">Free and Open Source Licenses</a></h2>
<p>In the free software and open source landscape, there are several important free and/or open source licenses that are used.
The two biggest software licenses are based on the software used by GNU/Linux and the software based on the BSDs.
They each take very different approaches to free and/or open source software. The biggest difference is this:</p>
<ul>
<li>Software based on software licensed under the GPL must also be licensed under the GPL.
<ul>
<li>This is referred to as <a href="https://www.gnu.org/licenses/copyleft.en.html">copyleft</a> software, and the idea is to propagate free software.</li>
<li>See: <a href="https://www.gnu.org/licenses/gpl-3.0.en.html">GNU General Public License (GPL)</a></li>
</ul>
</li>
<li>Software based on software licensed under the BSD license may be closed source and primarily must only attribute the original source code and author.
<ul>
<li><a href="https://opensource.org/licenses/BSD-3-Clause">BSD License</a></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-is-linux"><a class="header" href="#what-is-linux">What is Linux?</a></h1>
<h2 id="the-linux-kernel"><a class="header" href="#the-linux-kernel">The Linux Kernel</a></h2>
<p>Technically, <a href="https://kernel.org/">Linux is a kernel</a>, and a kernel is a part of an operating system that
oversees CPU activity like multitasking, as well as networking, memory management, device management, file systems, and more.
The kernel alone does not make an operating system.
It needs user land applications and programs, the kind we use on a daily basis, to form a whole,
as well as ways for these user land utilities to interact with the kernel.</p>
<h2 id="linux-and-gnu"><a class="header" href="#linux-and-gnu">Linux and GNU</a></h2>
<p>The earliest versions of the Linux kernel were combined with tools, utilities, and programs from the <a href="https://www.gnu.org/software/software.html">GNU project</a>
to form a complete operating system, without necessarily a graphical user interface.
This association continues to this day.
Additional non-GNU, but free and open source programs under different licenses,
have been added to form a more functional and user friendly system.
However, since the Linux kernel needs user land applications to form an operating system, and
since user land applications from GNU cannot work without a kernel,
some argue that the operating system should be called <a href="https://en.wikipedia.org/wiki/GNU/Linux_naming_controversy">GNU/Linux</a> and not just Linux.
This has not gained wide acceptance, though.
Regardless, credit is due to both camps for their contribution, as well as many others who have made substantial contributions
to the operating system.</p>
<h2 id="linux-uses"><a class="header" href="#linux-uses">Linux Uses</a></h2>
<p>We use Linux as a server in this course, which means we will use Linux to provide various services,
such as web services and database services.
Our first focus is to learn to use Linux itself, but by the end of the course,
we will also learn how to provide web and database services.
Linux can be used to provide <a href="https://en.wikipedia.org/wiki/Server_(computing)">other services</a> that we won't cover in this course, such as:</p>
<ul>
<li>file servers</li>
<li>mail servers</li>
<li>print servers</li>
<li>game servers</li>
<li>computing servers</li>
</ul>
<p>Although it's a small overall percentage, many people use Linux as their main desktop/laptop operating system.
I belong in this camp.
Linux has been my main OS since the early 2000s.
While our work on the Linux server means that we will almost entirely work on the command line,
this does not mean that my Linux desktop environment is all command line.
In fact, there are many graphical user environments, often called <a href="https://en.wikipedia.org/wiki/Desktop_environment">desktop environments</a>, available to Linux users.
Since I'm currently using the Ubuntu Desktop distribution, my default desktop environment is called <a href="https://www.gnome.org/">Gnome</a>.
<a href="https://kde.org/">KDE</a> is another popular desktop environment, but there are many other attractive and useful ones.
And it's easy to install and switch between multiple ones on the same OS.</p>
<p>Linux has become quite a pervasive operating system.
Linux powers the fastest supercomputers in the world.
It, or other Unix-like operating systems, are the foundation of most web servers.
The Linux kernel also forms the basis of the Android operating system and of Chrome OS.
The only place where Linux does not dominate is in the desktop/laptop space.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-is-systems-administration"><a class="header" href="#what-is-systems-administration">What is Systems Administration?</a></h1>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<p>What is systems administration or who is a systems administrator (or <strong>sysadmin</strong>)?
Let's start off with some definitions provided by the <a href="https://www.nist.gov/">National Institute of Standards and Technology</a>:</p>
<blockquote>
<p>An individual, group, or organization responsible for setting up and maintaining a system or specific system elements,
implements approved secure baseline configurations, incorporates secure configuration settings for IT products,
and conducts/assists with configuration monitoring activities as needed.</p>
</blockquote>
<p>Or:</p>
<blockquote>
<p>Individual or group responsible for overseeing the day-to-day operability of a computer system or network.
This position normally carries special privileges including access to the protection state and software of a system.</p>
</blockquote>
<p>See: <a href="https://csrc.nist.gov/glossary/term/system_administrator">Systems Administrator @NIST</a></p>
<h2 id="specialized-positions"><a class="header" href="#specialized-positions">Specialized Positions</a></h2>
<p>In addition to the above definitions, which broadly define the role, there are a number of related or specialized positions.
We'll touch on the first three in this course:</p>
<ul>
<li>Web server administrator:
<ul>
<li>"web server administrators are system architects responsible for the
overall design, implementation, and maintenance of Web servers. They may or
may not be responsible for Web content, which is traditionally the
responsibility of the Webmaster (<a href="https://csrc.nist.gov/glossary/term/web_server_administrator">Web Server Administrator"
@NIST</a>).</li>
</ul>
</li>
<li>Database administrator:
<ul>
<li>like web admins, and to paraphrase above, database administrators are system
architects responsible for the overall design, implementation, and
maintenance of database management systems.</li>
</ul>
</li>
<li>Network administrator:
<ul>
<li>"a person who manages a network within an organization. Responsibilities
include network security, installing new applications, distributing software
upgrades, monitoring daily activity, enforcing licensing agreements,
developing a storage management program, and providing for routine backups"
(<a href="https://csrc.nist.gov/glossary/term/network_administrator">Network Administrator @NIST</a>).</li>
</ul>
</li>
<li>Mail server administrator:
<ul>
<li>"mail server administrators are system architects responsible for the
overall design and implementation of mail servers" (<a href="https://csrc.nist.gov/glossary/term/mail_server_administrator">Mail Server
Administrators @NIST</a>).</li>
</ul>
</li>
</ul>
<p>Depending on where a system administrator works, they may specialize in any of the above administrative areas, or
if they work for a small organization, all of the above duties may be rolled into one position.
Some of the positions have evolved quite a bit over the last couple of decades.
For example, it wasn't too long ago when organizations would operate their own mail servers, but
this has largely been outsourced to third-party providers, such as Google (via Gmail) and Microsoft (via Outlook).
People are still needed to work with these third-party email providers, but the nature of the work is different than operating
independent mail servers.</p>
<h2 id="certifications"><a class="header" href="#certifications">Certifications</a></h2>
<p>It's not always necessary to get certified as a systems administrator to work as one, but there might be cases where it is necessary;
for example, some government positions or in large corporations require it.
It also might be the case that you can get work as an entry level systems administrator and then pursue certification with the support of your organization.</p>
<p>Some common starting certifications are:</p>
<ul>
<li><a href="https://www.redhat.com/en/services/certification/rhcsa">Red Hat Certified System Administrator (RHCSA)</a></li>
<li><a href="https://www.comptia.org/certifications/server">CompTIA Server+</a></li>
<li><a href="https://www.comptia.org/certifications/a">CompTIA A+</a></li>
</ul>
<p>Plus, Google offers, via <a href="https://www.coursera.org/">Coursera</a>, a beginners <a href="https://www.coursera.org/professional-certificates/google-it-support">Google IT Support Professional Certificate</a> that may be helpful.</p>
<h2 id="associations"><a class="header" href="#associations">Associations</a></h2>
<p>Getting involved in associations and related organizations is a great way to learn and to connect with others in the field.
Here are few ways to connect.</p>
<p><a href="https://lopsa.org/AboutLOPSA">LOPSA</a>, or The League of Professional System Administrators, is a non-profit association that seeks to advance
the field and membership is free for students.</p>
<p><a href="https://www.acm.org/">ACM</a>, or the Association for Computing Machinery, has a number of relevant <a href="https://www.acm.org/special-interest-groups/alphabetical-listing">special interest groups (SIGs)</a>
that might be beneficial to systems administrators.</p>
<p><a href="http://www.npa.org/">NPA</a>, or the Network Professional Association, is an organization that "supports IT/Network professionals."</p>
<p>The <a href="https://en.wikipedia.org/wiki/Library_and_Information_Technology_Association">Library &amp; Information Technology Association</a> was a division of the American Library Association
that served as a home for systems librarians and librarians working in information technology, generally.
However, the division was dissolved in 2020 and the <a href="https://www.ala.org/core">ALA Core</a> division now serves the community's needs.</p>
<h2 id="codes-of-ethics"><a class="header" href="#codes-of-ethics">Codes of Ethics</a></h2>
<p>Systems administrators manage computer systems that contain a lot of data about us and this raises privacy and competency issues,
which is why some have created code of ethics statements.
Both LOPSA and NPA have created such statements that are well worth reviewing and discussing.</p>
<ul>
<li>LOPSA: <a href="https://lopsa.org/CodeOfEthics">Code of Ethics</a></li>
<li>NPA: <a href="https://www.npa.org/public/about_codeofethics.cfm">Code of Ethics</a></li>
</ul>
<h2 id="keeping-up"><a class="header" href="#keeping-up">Keeping Up</a></h2>
<p>Technology changes fast.
In fact, even though I teach this course about every year, I need to revise the course each time, sometimes substantially,
to reflect changes that have developed over short periods of time.
It's also your responsibility, as sysadmins, to keep up, too.</p>
<p>I therefore suggest that you continue your education by reading and practicing.
For example, there are lots of books on systems administration.
<a href="https://www.npa.org/public/about_codeofethics.cfm">O'Reilly</a> continually publishes on the topic.
RedHat, the makers of the <a href="https://www.redhat.com/en">Red Hat</a> Linux distribution, and sponsor of <a href="https://fedoraproject.org/">Fedora Linux</a>,
provides the <a href="https://www.redhat.com/sysadmin/">Enable Sysadmin</a> site, with new articles each day authored by systems administrators in the field.
Opensource.com, also supported by Red Hat, <a href="https://opensource.com/tags/sysadmin">publishes articles on systems administration</a>.
<a href="https://www.redhat.com/en/command-line-heroes">Command Line Heroes</a> is a fun and informative podcast on technology and sysadmin related topics.
<a href="https://www.linuxjournal.com/">Linux Journal</a> publishes great articles on Linux related topics.</p>
<p>For those interested in Systems Librarianship, you can stay up to date by following
Marshall Breeding's <a href="https://librarytechnology.org/systemslibrarian/">Systems Librarian column</a>.</p>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>In this section I provided definitions of systems administrators and also the related or more specialized positions,
such as database administrator, network administrator, and others.</p>
<p>I provided links to various certifications you might pursue as a systems administrator,
and links to associations that might benefit you and your career.</p>
<p>Technology manages so much of our daily lives, and computer systems store lots of data about us.
Since systems administrators manage these systems, they hold a great amount of responsibility to protect them and our data.
Therefore, I provided links to two code of ethics statements that we will discuss.</p>
<p>It's also important to keep up with the technology, which changes fast.
The work of a systems administrator is much different today than it was ten or twenty years ago, and
that surely indicates that it could be much different in another ten to twenty years.
If we don't keep up, we won't be of much use to the people we serve.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-is-systems-librarianship"><a class="header" href="#what-is-systems-librarianship">What is Systems Librarianship</a></h1>
<h2 id="introduction-2"><a class="header" href="#introduction-2">Introduction</a></h2>
<p>Of course, let's begin with the question, what is systems librarianship?
Normally we might go to the literature to answer a question like this.
Indeed, the literature is helpful, but it's sparse.
The <a href="https://libguides.uky.edu/803">LISTA</a> database only returns 131 results with a 45 year coverage for a search using the thesauri term <strong>SYSTEMS Librarians</strong>.
I can get more results if I expand the search query, but then I get less relevant results, and the main idea is the same:
this is an understudied area of librarianship.</p>
<p>It's been that way for a while. <a href="https://doi.org/10.1300/J111v09n04_06">Susan K. Martin</a> wrote the following over 35 years ago:</p>
<blockquote>
<p>Of the specialist positions that exist in libraries, none is as underexamined as those of the systems
librarians—the people who identify the needs of the library for automated systems, cause these systems to be
implemented, and analyze the operations of the library (p. 57).</p>
</blockquote>
<p>Perhaps as a result of this under-examination, sometimes there is confusion around the requirements and skills needed in this area of librarianship.
Martin (1988) captured this tension when she wrote the following in 1988, which is still true today:</p>
<blockquote>
<p>Over the years the library world has argued whether systems librarians should be librarians who have learned
information technologies, or computer experts who have learned about libraries (p. 61).</p>
</blockquote>
<p>The argument is partly a matter of jurisdiction.
<a href="https://www.proquest.com/docview/220452054/abstract/A48FC30B10D94886PQ/1?accountid=11836">Abbott (1998)</a>, writing on librarianship in the sociology of professions, illustrated how:</p>
<blockquote>
<p>The future of librarianship thus hinges on what happens to the perpetually changing work of the profession in its
three contexts: the context of larger social and culture forces, the context of other competing occupations, and
the context of competing organizations and commodities.
To these complex contextual forces, any profession responds with varying policies and internal changes (pp. 434-5).</p>
</blockquote>
<p>Essentially, Abbott means that professions, like librarianship, are always changing.
The mechanisms for that change are structural and cultural <a href="https://www.jstor.org/stable/40664150">(Abbott, 2010)</a>,
but a changing profession means that its "link of jurisdiction" (Abbott, 1998, p. 435) changes, too.
It not only changes, but professions constantly compete with each other over to adopt new areas of jurisdiction.
So when we ask, as Martin (1998) did, whether librarians should learn information technologies or whether
computer experts should learn libraries, I find myself thinking the prior is more important for libraries and their patrons.
It means that librarians are expanding their jurisdiction by also becoming computer experts rather than computer experts expanding theirs.</p>
<p>That leads us to the next questions: what does it mean to be a computer expert for a systems librarian?
What does a systems librarians need to do and know?</p>
<p>The answer is that it is a mix.
Some part of the work involves systems administration, but that has broad meanings, and systems librarianship is more specific.
Or, it has a more specific domain: the domain of libraries and librarianship.</p>
<p>A systems librarian might thus be considered a library systems administrator.
Under this view, they need to be someone who knows about libraries, how libraries work, what they do, about their patrons,
what their values are, and then use that knowledge to build and maintain the infrastructure to support that.</p>
<p>Given this, and the technologies involved, such work requires constant learning.
<a href="https://doi.org/10.1108/07378830310494445">Jordan (2003)</a> identified three areas of learning:</p>
<ul>
<li>pre-service education in library schools</li>
<li>on the job training</li>
<li>professional development in the form of workshops,
courses, and conferences (p. 273)</li>
</ul>
<p>Pre-service, formal education is a small part of any professional's career,
regardless if that profession is in medicine, law, or librarianship.
Thus the goal of pre-service education is to prepare people to adapt and grow in their fields.
Jordan (2003) wrote that:</p>
<blockquote>
<p>While formal training is undoubtedly important, the ability to learn new technologies independently lies at
the foundation of systems librarians' professional life, because they often have to use technologies, or make
planning decisions about specific technologies, before they become common enough to be the subject of formal training sessions (p. 273).</p>
</blockquote>
<p>Even though Jordan's article is 20 years old and the technology has changed a lot, the basic duties of the systems librarian
remain the same (<a href="https://digitalcommons.cwu.edu/cgi/viewcontent.cgi?article=1015&amp;context=libraryfac">Fu, 2014</a>; <a href="https://rowman.com/ISBN/9781538107133/Systems-Librarianship-A-Practical-Guide-for-Librarians">Gonzales, 2020</a>).
<a href="https://www.worldcat.org/title/1038159656">Wilson (1998)</a>, as cited in Jordan (2003), refers to a list of the "typical responsibilities of systems librarians."
These responsibilities look different today, because the technology is different, but conceptually, they're the same as they were then.
In fact, this work will focus on a subset of this list that includes:</p>
<ul>
<li>integrated library system management</li>
<li>server management</li>
<li>documentation</li>
<li>technology exploration and evaluation (Jordan, 2003, p. 274)</li>
</ul>
<p>Gonzales (2020) highlights these and more current areas that include:</p>
<ul>
<li>content management systems</li>
<li>electronic resource management systems</li>
<li>website redesign</li>
<li>help and support</li>
</ul>
<p>Other items on Jordan's (2003) list are still relevant, but due to various constraints, this textbook will not cover the following areas:</p>
<ul>
<li>network design and management</li>
<li>desktop computing</li>
<li>application development</li>
<li>planning and budget</li>
<li>specification and purchasing</li>
<li>miscellaneous technology support</li>
<li>technical risk management (p. 274)</li>
</ul>
<p>In short, this work specifically focuses on a few of the bigger technical aspects of systems librarianship.
Other works (or courses) and other sources will provide learning opportunities on the more managerial and administrative functions
of systems librarianship and librarianship, in general.</p>
<blockquote>
<p>If you are interested in learning more about network design and administration, then I encourage you to read my chapters on
<a href="https://cseanburns.github.io/linux_sysadmin/6a-networking-tcpip.html">Networking and TCP/IP</a> and <a href="https://cseanburns.github.io/linux_sysadmin/6b-dns-domain-names.html">DNS and Domain Names</a> in my book on <a href="https://cseanburns.github.io/linux_sysadmin/">Systems Administration with Linux</a>.</p>
<p>If you are interested in learning about application development, then you can pursue courses in a variety
of programming languages, such as R, Python, JavaScript, and PHP, as well as courses on relational databases,
such as MySQL or PostgreSQL, and so forth.</p>
</blockquote>
<p>As Jordan (2003) identified, there is a lack of formalized training in systems librarianship in LIS schools.
This is as true today as it was in 2003.
This course was created to address the lack of that training.
However, as Jordan (2003) noted, pre-service education is only a start.
Technology is constantly changing, and that means we must always embrace learning opportunities,
such as through workshops, conferences, and on the job training.
LIS programs are only two or so years long (if attending full time), but our careers will hopefully span decades.
So all this course can ever be is just a starting point.</p>
<p>It is a big start, though.
This course should lay a strong foundation for self-growth and self-education in the variety of technologies that we will learn and use here.
Although separate areas of librarianship, my work (and course) on <a href="https://cseanburns.github.io/electronic_resource_mgmt/">electronic resource management</a> complement this one in many ways.
For example, this work supports several parts of the technology section in the
<a href="https://www.nasig.org/Competencies-Eresources">NASIG Core Competencies for Electronic Resources Librarians</a>.
It is no coincidence these two areas of librarianship often overlap or are assumed in a single librarian position.</p>
<h2 id="cloud-computing"><a class="header" href="#cloud-computing">Cloud Computing</a></h2>
<p>Lastly, I want to mention cloud computing.
This has become a major area of change in the last decade or so.
It used to be more common for librarians to install their integrated library system software
and store their bibliographic data on their premises.
In the last ten years, there has been more migration to the cloud,
which means that both the integrated library system software and the bibliographic data are stored off-site.
<a href="https://doi.org/10.1108/10650751311294528">Liu &amp; Cai (2013)</a> highlight the beginning of this trend toward cloud computing
that continues to play a large role in systems librarianship <a href="https://doi.org/10.1108/DLP-03-2021-0022">(Naveed et al., 2021)</a>.
As Liu and Cai note:</p>
<blockquote>
<p>Systems librarians used to make their livings by managing hosted library systems.
This situation is silently changing with the library systems moving onto the cloud (p. 26).</p>
</blockquote>
<p>This trend has changed some aspects of systems librarianship.
It means that systems librarians, while still a technical area of librarianship, need to work more closely with vendors,
who themselves are hosting library systems.
This is perhaps why that even though this is a technical area of librarianship,
the ability <a href="https://doi.org/10.3138/jelis-2023-0080">to communicate well is probably the most important requisite</a> for people working as systems librarians.
Specifically, in a recent article, <a href="https://doi.org/10.3138/jelis-2023-0080">Willis, 2025</a> found in an examination of descriptions of job positions
that some of the most in-demand skill sets include:</p>
<ul>
<li>communication</li>
<li>liaison work</li>
<li>vendor relations</li>
</ul>
<p>However, the trend does not erase all locally hosted solutions.
Many libraries and other information agencies continue to support local collections and will either host those locally or
work to get the bibliographic information for those collections ingested into their cloud-based integrated library systems.</p>
<h2 id="conclusion-1"><a class="header" href="#conclusion-1">Conclusion</a></h2>
<p>The remainder of the course will be more technical and will prepare you to work and understand the systems that support the modern library.
We will cover a lot, too!
We will begin with setting up virtual machine instances on Google Cloud.
We will use a distribution of the Linux operating system for these virtual machines.
We will then learn the basics of the Linux command line.
Next, we will learn how to use the version control system called <code>git</code>.
We will use <code>git</code> to document our work flows and push that documentation to <a href="https://github.com">GitHub.com</a>.
On our Linux servers, we will create a web server out of what is called a <a href="https://en.wikipedia.org/wiki/LAMP_(software_bundle)">LAMP stack</a>,
which stands for <a href="https://www.kernel.org/">Linux</a>, <a href="https://www.apache.org/">Apache</a>, <a href="https://www.mysql.com/">MySQL</a>, and <a href="https://www.php.net/">PHP</a>.
We will use the web server to setup a basic website and a bare bones OPAC.
Then we will learn how to install and setup two content management systems: Wordpress and Omeka.
Lastly, we will spend the final two weeks of the semester installing and setting up the open source <a href="https://koha-community.org/">Koha ILS</a>.</p>
<p>Let's get started!</p>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<p>Abbott, A. (1998).
Professionalism and the future of librarianship. Library Trends, 46(3), 430–443.
<a href="https://www.proquest.com/docview/220452054/abstract/A48FC30B10D94886PQ/1?accountid=11836">https://www.proquest.com/docview/220452054/abstract/A48FC30B10D94886PQ/1?accountid=11836</a></p>
<p>Abbott, A. (2010).
Varieties of ignorance.
<em>The American Sociologist, 41</em>(2), 174–189.
<a href="https://www.jstor.org/stable/40664150">https://www.jstor.org/stable/40664150</a></p>
<p>Gonzales, B. M. (2020).
Systems librarianship: A practical guide for librarians.
Rowman &amp; Littlefield Publishers.
<a href="https://rowman.com/ISBN/9781538107133/Systems-Librarianship-A-Practical-Guide-for-Librarians">https://rowman.com/ISBN/9781538107133/Systems-Librarianship-A-Practical-Guide-for-Librarians</a></p>
<p>Fu, P. (2014).
Supporting the next-generation ILS: The changing roles of systems librarians.
<em>Journal of Library Innovation, 5</em>(1), 30–42.
<a href="https://digitalcommons.cwu.edu/cgi/viewcontent.cgi?article=1015&amp;context=libraryfac">https://digitalcommons.cwu.edu/cgi/viewcontent.cgi?article=1015&amp;context=libraryfac</a></p>
<p>Jordan, M. (2003).
The self‐education of systems librarians.
<em>Library Hi Tech, 21</em>(3), 273–279.
<a href="https://doi.org/10.1108/07378830310494445">https://doi.org/10.1108/07378830310494445</a></p>
<p>Liu, W., &amp; Cai, H. (Heather). (2013).
<em>Embracing the shift to cloud computing: Knowledge and skills for systems librarians.</em>
OCLC Systems &amp; Services: International Digital Library Perspectives, 29(1), 22–29.
<a href="https://doi.org/10.1108/10650751311294528">https://doi.org/10.1108/10650751311294528</a></p>
<p>Martin, S. K. (1988).
The role of the systems librarian.
<em>Journal of Library Administration, 9</em>(4), 57–68.
<a href="https://doi.org/10.1300/J111v09n04_06">https://doi.org/10.1300/J111v09n04_06</a></p>
<p>Naveed, M. A., Siddique, N., &amp; Mahmood, K. (2021).
Development and validation of core technology competencies for systems librarian.
<em>Digital Library Perspectives, 38</em>(2), 189–204.
<a href="https://doi.org/10.1108/DLP-03-2021-0022">https://doi.org/10.1108/DLP-03-2021-0022</a></p>
<p>Ratledge, D., &amp; Sproles, C. (2017).
An analysis of the changing role of systems librarians.
<em>Library Hi Tech, 35</em>(2), 303–311.
<a href="https://doi.org/10.1108/LHT-08-2016-0092">https://doi.org/10.1108/LHT-08-2016-0092</a></p>
<p>Willis, S. K. (2025).
Systems librarianship preparedness: A comparative analysis of skills needed and taught.
<em>Journal of Education for Library and Information Science</em>, e20230080.
<a href="https://doi.org/10.3138/jelis-2023-0080">https://doi.org/10.3138/jelis-2023-0080</a></p>
<p>Wilson, T. C. (1998).
<em>Systems librarian: Designing roles, defining skills.</em>
American Library Association.
<a href="https://www.worldcat.org/title/1038159656">https://www.worldcat.org/title/1038159656</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="project-management"><a class="header" href="#project-management">Project Management</a></h1>
<p>This course involves working towards a final project that will lead us to install two content management systems
and an integrated library system.</p>
<p>To accomplish this, we will need to set up Linux servers.
We will use Google Cloud for this purpose.
With Google Cloud, we can create what are called virtual machines that run full-fledged operating systems.
We will work with Linux, and in particular, the Ubuntu distribution of Linux, to complete our project.</p>
<p>We will also want to document our work.
To do that, we will use <a href="https://en.wikipedia.org/wiki/Git">git</a>, which is a version control system,
and <a href="https://github.com">GitHub</a>, an online platform for hosting <code>git</code> repositories.
Using <code>git</code>, we will write and share documentation, code, and more.</p>
<h2 id="using-google-cloud-gcloud"><a class="header" href="#using-google-cloud-gcloud">Using Google Cloud (gcloud)</a></h2>
<p>The first section in this chapter introduces us to <a href="https://cloud.google.com">Google Cloud</a>, which I'll often refer to as <strong>gcloud</strong>.
We will use this platform to create virtual instances of the Ubuntu Server Linux operating system.
Once we create our own Ubuntu virtual machines, we will connect to them via the command line.
I have written some helpful software to help you learn the command line language, specifically, the <a href="https://en.wikipedia.org/wiki/Bash_(Unix_shell)">Bash shell</a>.
Just about everything we'll do this semester will happen via the Bash shell.</p>
<h2 id="git-and-github"><a class="header" href="#git-and-github">Git and GitHub</a></h2>
<p>The last section in this chapter introduces us to <code>git</code> and GitHub.
<code>git</code> and GitHub are primarily used for software management.
Every major software project requires managing the codebase, collaborations, documentation, and more.
Many people may be involved in these projects, and it takes coordination for them to write the many thousands of
lines of software code, which also requires management.</p>
<p>Although <code>git</code> and GitHub are primarily used for this purpose, our goal is to use them to document our work,
much like this book, which has its own <a href="https://github.com/cseanburns/systems_librarianship">GitHub repository or repo</a>.
This documentation covers the processes involved in learning Google Cloud, <code>git</code> and GitHub, Linux, and more.
Therefore, in the next section, we'll learn how to create a new repo on GitHub,
add notes, and write our notes using Markdown, an easy-to-understand and use <em>markup</em> language to format our text.</p>
<p>Providing good documentation is key to being able to build on prior work, make adjustments to our workflows,
recall the details of some process, and, for students, it can help in retention and reflection.
In the remainder of the semester, we will begin to install and configure some complicated pieces of software.
In order to better understand what we will be doing, it will be helpful to document our processes.</p>
<h2 id="attending-to-detail"><a class="header" href="#attending-to-detail">Attending to Detail</a></h2>
<p>As we begin to work on the more technical aspects of this book and course, it will be important to remain <strong>attentive to details</strong>.
Many people who are new to this kind of work often stumble over the details, like a missing period, incorrect capitalization, and more.
To learn how to pay <strong>attention to the details</strong>, work slowly and read any messages, including error messages,
the screen prints out in response to your commands.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="using-gcloud-for-virtual-machines"><a class="header" href="#using-gcloud-for-virtual-machines">Using gcloud for Virtual Machines</a></h1>
<h2 id="virtual-machines"><a class="header" href="#virtual-machines">Virtual Machines</a></h2>
<p>Our goal in this section is to create a <strong>virtual machine (VM)</strong> <em>instance</em>.
A VM is basically a virtualized operating system that runs on a host operating system.
That host operating system may also be Linux, but it could be Windows or macOS.
In short, when we use virtual machines, it means instead of installing an operating system
(like Linux, macOS, Windows, etc) on a physical machine, we use virtual machine software to mimic the process.
The virtual machine, thus, runs on top of our main OS.
It's like an app, where the app is a fully functioning operating system.</p>
<p>In this course, we're going to use gcloud (via Google) to provide us with virtual machines.
There are other cloud service providers available that you can explore on your own.
You can also play with <a href="https://www.virtualbox.org/">VirtualBox</a> (on your own), which I've used in prior classes, to install virtual machines
on your own computers.</p>
<h2 id="google-cloud--gcloud"><a class="header" href="#google-cloud--gcloud">Google Cloud / gcloud</a></h2>
<h3 id="google-account"><a class="header" href="#google-account">Google Account</a></h3>
<p>We need to have a personal Google account to get started with gcloud.
I imagine most of you already have a Google account, but if not, go ahead and create one at <a href="https://www.google.com">https://www.google.com</a>.</p>
<h3 id="google-cloud-gcloud-project"><a class="header" href="#google-cloud-gcloud-project">Google Cloud (gcloud) Project</a></h3>
<p>Next we will need to create a project on the <a href="https://cloud.google.com/?hl=en">Google Cloud website</a>.</p>
<p>Follow <strong>Step 1</strong> at the top of the <strong><a href="https://cloud.google.com/sdk/docs/install-sdk">Install the gcloud CLI</a></strong> page to create a new project.
Also, review the page on <a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects#gcloud">creating and managing projects</a>.</p>
<p>When you create your project, you can name it anything, but try to name it something to do with this course.
E.g., I am using the name <strong>syslib-YEAR</strong> (replace <strong>YEAR</strong> with the actual year).
Avoid using spaces when naming your project.</p>
<p>Then click on the <strong>Create</strong> button, and leave the organization field set to <strong>No Organization</strong>.</p>
<h3 id="google-billing"><a class="header" href="#google-billing">Google Billing</a></h3>
<p>The second thing to do is to set up a billing account for your gcloud project.
This does mean there is a cost associated with this product,
but the good news is that our bills by the end of the semester should only amount to $5 to 10 dollars, at most.
<strong><a href="https://cloud.google.com/sdk/docs/install-sdk">Follow Step 2</a></strong> to enable billing for your new project.
See also the page on how to <strong><a href="https://cloud.google.com/billing/docs/how-to/manage-billing-account">create, modify, or close your self-serve Cloud Billing account</a></strong>.</p>
<p>At the end of the semester, I'll remind you that you may want to delete your virtual machines.
If you don't do this, you will continue to be billed for them.</p>
<h3 id="install-the-gcloud-cli-to-connect-to-our-virtual-machines"><a class="header" href="#install-the-gcloud-cli-to-connect-to-our-virtual-machines">Install the gcloud CLI to connect to our virtual machines</a></h3>
<blockquote>
<p><strong>NOTE:</strong> We install the gcloud CLI to connect to our virtual machines from our personal computers.
I suggest this process because it offers more functionality but it's also a bit advanced.
If you think you'd like to skip this process for now and try later, we can use an alternate method to connect to our virtual machines.
I'll describe the alternate method to connect below in the <strong>Connect to our VM</strong> section.
If you would prefer to use the alternate method, skip to the <strong>gcloud VM Instance section</strong>.</p>
</blockquote>
<p>After you have set up billing, the next step is to install gcloud on your local machines.
The <strong><a href="https://cloud.google.com/sdk/docs/install-sdk">Install the gcloud CLI</a></strong> page provides instructions for different operating systems.</p>
<p>There are installation instructions for macOS, Windows, Chromebooks, and various Linux distributions.
Follow these instructions closely for the operating system that you're using.
Note that for macOS, you have to choose among three different CPU/chip architectures.
If you have an older macOS machine (before November 2020 or so), it's likely that you'll select <strong>macOS 64-bit (x86_64)</strong>.
If you have a newer macOS machine, then it's likely you'll have to select <strong>macOS 64-bit (arm64, Apple M1 silicon).</strong>
It's unlikely that any of you are using a 32-bit macOS operating system.
If you're not sure which macOS system you have, then let me know and I can help you determine the appropriate platform.
Alternatively, follow these instructions to find your processor information:</p>
<ul>
<li>click on the Apple menu</li>
<li>choose <strong>About This Mac</strong></li>
<li>locate the <strong>Processor</strong> or <strong>Chip</strong> information</li>
</ul>
<p>After you have downloaded the gcloud CLI for your particular OS and CPU architecture,
you will need to open a command prompt/terminal on your machines to complete the instructions that describe how to install the gcloud CLI.
macOS uses the Terminal app, which can located using Spotlight.
Windows user can use Command.exe, which can be located by search also.</p>
<p>Windows users will download a regular <strong>.exe</strong> file, but macOS users will download a <strong>.tar.gz</strong> file.
Since macOS is Unix, you can use the <code>mv</code> command to move that file to your <code>$HOME</code> directory.
Then you extract it there using the <code>tar</code> command, and once extracted you can change to the directory that it
creates with the <code>cd</code> command.
For example, if you are downloading the X86_64 version of the gcloud CLI, then you would run the following commands:</p>
<p>For macOS users, this assumes the <strong>.tar.gz</strong> file was downloaded to your default Downloads folder:</p>
<pre><code>cd ~/Downloads/
mv google-cloud-cli-392.0.0-darwin-x86_64.tar.gz ~/
cd ~/
tar -xzf google-cloud-cli-392.0.0-darwin-x86_64.tar.gz
cd google-cloud-sdk
</code></pre>
<p>Modify the above commands, as appropriate, if you're using the M1 or the M2 version of the gcloud CLI.</p>
<h3 id="initializing-the-gcloud-cli"><a class="header" href="#initializing-the-gcloud-cli">Initializing the gcloud CLI</a></h3>
<p><strong>As above, please follow the instructions from the Google Cloud documentation for your operating system.</strong></p>
<p>Once you have downloaded and installed the gcloud CLI program, you need to initialize it on your local machine.
Scroll down on the <a href="https://cloud.google.com/sdk/docs/install-sdk">install page</a> to the section titled <strong>Initializing the gcloud CLI</strong>.
In your terminal/command prompt, run the initialization command, per the instructions at the above page:</p>
<pre><code>gcloud init
</code></pre>
<p>And continue to follow the instructions from the prompt and from the Google Cloud documentation page above.</p>
<h2 id="gcloud-vm-instance"><a class="header" href="#gcloud-vm-instance">gcloud VM Instance</a></h2>
<p>Once you've initialized gcloud, log into <a href="https://console.cloud.google.com/">Google Cloud Console</a>, which should take you to the Dashboard page.</p>
<p>Our first goal is to create a <strong>virtual machine (VM)</strong> <em>instance</em>.
As a reminder, a VM is basically a virtualized operating system.
That means instead of installing an operating system (like Linux, macOS, Windows, etc) on a physical machine,
software is used to mimic the process.</p>
<p>gcloud offers a number of Linux-based operating systems to create VMs.
We're going to use the Ubuntu operating system and specifically the Ubuntu 22.04 LTS version.</p>
<blockquote>
<p>Ubuntu is a Linux distribution.
There are many, many distributions of Linux, and most are probably listed on the <a href="https://distrowatch.com/">DistroWatch</a> site.
A new version of Ubuntu is released every six months.
The 22.04 signifies that this is the April 2022 version.
LTS signifies <strong>Long Term Support</strong>.
LTS versions are released every two years, and Canonical LTD, the owners of Ubuntu,
provide standard support for LTS versions for five years.</p>
<p>LTS versions of Ubuntu are stable.
Non-LTS versions of Ubuntu receive nine months of standard support, and generally apply cutting edge technology,
which is not always desirable for server operating systems.
Each version of Ubuntu has a code name.
24.10 has the code name <strong>Oracular Oriole</strong>.
You can see a list of versions, code names, release dates, and more on Ubuntu's <a href="https://wiki.ubuntu.com/Releases">Releases</a> page.</p>
</blockquote>
<p>We will create our VM using the gcloud console.
To do so, follow these steps from the Project page:</p>
<ul>
<li>Click on the hamburger icon (three vertical bars) in the top left corner.</li>
<li>Click on <strong>Compute Engine</strong> and then <strong>VM instances</strong>.</li>
<li>Enable <strong>Compute Engine API</strong>.</li>
<li>Make sure your project is listed.</li>
<li>Next, click on <strong>Create Instance</strong>.</li>
<li>Provide a name for your <strong>instance</strong>.
<ul>
<li>E.g., I chose <strong>main-ubuntu</strong> (no spaces) but you are free to use any name you prefer</li>
</ul>
</li>
<li>In the <strong>Machine configuration</strong> section, make sure <strong>E2</strong> is selected.</li>
<li>In the <strong>Machine type</strong> section, select <strong>e2-micro (2 vCPU, 1 core, 1 GB memory)</strong>
<ul>
<li>This is the lowest cost virtual machine and perfect for our needs.</li>
</ul>
</li>
<li>In the left navigation section, click on <strong>OS and storage</strong>.
<ul>
<li>Click on <strong>CHANGE</strong>, and a side bar on the right of the screen open up.</li>
<li>In the sidebar, select <strong>Ubuntu</strong> from the <strong>Operating system</strong> drop down box.</li>
<li>Click on <strong>Version</strong> and select <strong>Ubuntu 22.04 LTS x86/64</strong></li>
<li>Leave <strong>Boot disk type</strong> be set to <strong>Balanced persistent disk</strong></li>
<li>Leave disk size to <strong>10 GB</strong>.</li>
<li>Click on the <strong>Select</strong> button.</li>
</ul>
</li>
<li>In the left navigation section, click <strong>Networking</strong>.
<ul>
<li>Check the <strong>Allow HTTP Traffic</strong> button</li>
</ul>
</li>
<li>Finally, click on the <strong>Create</strong> button to create your VM instance.</li>
</ul>
<blockquote>
<p>Later in the semester when we install Koha, we will need to create a virtual machine with more CPUs and memory.
We will be charged more for those machines.
Since we do not yet need the extra resources, we will start off with fairly low powered machines.</p>
</blockquote>
<h2 id="connect-to-our-vm"><a class="header" href="#connect-to-our-vm">Connect to our VM</a></h2>
<p>After the new VM machine has been created, we need to connect to it.</p>
<h3 id="using-gcloud-cli"><a class="header" href="#using-gcloud-cli">Using gcloud CLI</a></h3>
<p>We use a <code>ssh</code> command to connect to our VMs.
The syntax follows this pattern:</p>
<pre><code>gcloud compute ssh --zone "zone-info" "name-info" --project "project-id"
</code></pre>
<p>macOS users will use that command using their Terminal.app.
Windows users can connect to it via their command prompt (CMD.exe or PowerShell).</p>
<p>To get the specific connection command for your virtual instance, click on the <strong>SSH</strong> drop down menu.
Select <strong>View gcloud command</strong>.
Copy and paste that command in your OS terminal.</p>
<h3 id="using-the-web-interface"><a class="header" href="#using-the-web-interface">Using the web interface</a></h3>
<p>If you elected not to install the gcloud CLI, you can connect via the web interface.
Click on the <strong>SSH</strong> drop down menu, and select <strong>Open in browser window</strong>.
This will open a terminal window in your browser.</p>
<h3 id="connection-differences"><a class="header" href="#connection-differences">Connection Differences</a></h3>
<p>If you connect using the gcloud CLI method, your username on your virtual machine will be based on the username on your personal machine.
However, if you connect using the web interface, your username on your virtual machine will be based on your Google account username.
This has important ramifications, especially if you decide to use change connection methods.</p>
<p>The main ramification is that you will have two different home directories on your virtual machine.
For example, if my username for my Google account is <strong>sean_burns</strong>,
then my home directory on the virtual machine will be <strong>/home/sean_burns</strong> if I connect via the web interface.</p>
<p>But if my username on my personal computer is <strong>sean</strong>,
then my home directory on the virtual machine will be <strong>/home/sean</strong> if I connect via the gcloud CLI.</p>
<p>Keep this in mind.</p>
<h3 id="quick-shell-information"><a class="header" href="#quick-shell-information">Quick Shell Information</a></h3>
<p>When you log into your machines, you'll see a command prompt with the following format:</p>
<pre><code>username@machine_name:~$
</code></pre>
<p>This is where we type our commands.
It's not obvious right away, but the command prompt displays our location in the file system.
By default, you will be located in your home directory, which is indicated by the <code>~</code> tilde in your prompt.</p>
<h2 id="update-our-ubuntu-vm"><a class="header" href="#update-our-ubuntu-vm">Update our Ubuntu VM</a></h2>
<p>The VM will include a recently updated version of Ubuntu 22.04, but it may not be completely updated.
Therefore the first thing we need to do is update our machines.
On Ubuntu, we'll use the following commands, which you should run also:</p>
<pre><code>sudo apt update
sudo apt -y upgrade
sudo apt -y autoremove
sudo apt clean
</code></pre>
<p>You should run these commands at least weekly to keep your system updated.</p>
<h2 id="disconnecting"><a class="header" href="#disconnecting">Disconnecting</a></h2>
<p>To exit and disconnect from the remote system, type <code>exit</code>, like so:</p>
<pre><code>exit
</code></pre>
<p>If you are using the gcloud CLI, you should exit back to your personal machine's terminal prompt.
Pay attention to the messages upon logout.
If you get the following message:</p>
<pre><code>Updates are available for some Google Cloud CLI components. To install them, please run:
gcloud components update
</code></pre>
<p>Then run that command in your own terminal app and click <strong>Y</strong> (yes) when prompted:</p>
<pre><code>gcloud components update
</code></pre>
<p>Once updated, you can close your terminal app.</p>
<p>If you are using the web interface, the window should close.</p>
<h2 id="snapshots-optional"><a class="header" href="#snapshots-optional">Snapshots (Optional)</a></h2>
<blockquote>
<p>This is entirely optional at this point.
You may want to consider doing this when we begin to install a LAMP stack and content management systems
because it'll help you recover your system if something gets messed up.
But as of now, if we mess up our system, it's pretty trivial to delete the virtual instance and start a new one.</p>
</blockquote>
<p>Lastly, we have installed a pristine version of Ubuntu, but it's likely that we will mess something up as we work on our systems.
Or it could be that our systems may become compromised at some point.
Therefore, we want to create a snapshot of our newly installed Ubuntu server.
This will allow us to restore our server if something goes wrong later.</p>
<p>To get started:</p>
<ol>
<li>
<p>In the left hand navigation panel, click on <strong>Snapshots</strong>.</p>
</li>
<li>
<p>At the top of the page, click on <strong>Create Snapshot</strong>.</p>
</li>
<li>
<p>Provide a name for your snapshot: e.g., <strong>ubuntu-1</strong>.</p>
</li>
<li>
<p>Provide a description of your snapshot: e.g.,</p>
<p>This is a new install of Ubuntu 22.04.</p>
</li>
<li>
<p>Choose your <strong>Source disk</strong>, which should be the name of your virtual instance (e.g., <strong>main-ubuntu</strong>).</p>
</li>
<li>
<p>Choose a <strong>Location</strong> to store your snapshot.</p>
<ul>
<li>To avoid extra charges, choose <strong>Regional</strong>.</li>
<li>From the drop down box, select the same location (zone-info) your VM has</li>
</ul>
</li>
<li>
<p>Click on <strong>Create</strong></p>
</li>
</ol>
<p><strong><p style="color:red">Please monitor your billing for this to avoid costs that you do not want to incur.</p></strong></p>
<h2 id="conclusion-2"><a class="header" href="#conclusion-2">Conclusion</a></h2>
<p>Congratulations!
You have just completed your first installation of a Linux server.</p>
<p>To summarize, in this section, you learned about and created a VM with gcloud.
This is a lot!
After this course is completed, you will be able to fire up a virtual machine on short notice and deploy websites and more.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="learn-the-command-line-interface-cli"><a class="header" href="#learn-the-command-line-interface-cli">Learn the Command Line Interface (CLI)</a></h1>
<h2 id="introduction-3"><a class="header" href="#introduction-3">Introduction</a></h2>
<p>There are two major interfaces
that we use to interact with our computers.
The most common interface is the
graphical user interface, or GUI.
This interface largely emphasizes
non-textual interaction,
such as the mouse, fingers (touch screens),
remote controls (e.g., smart TVs),
and more recently,
wearable tech such as
VR headsets and like.
All of the above mechanisms for interacting
with our computer systems are worthwhile, but
more importantly, they are all suited to
specific ranges of engagement with our computers.
That is,
<a href="https://doi.org/10.7551/mitpress/7221.001.0001">they <em>afford</em> certain kinds of actions</a>
(Dourish, 2001).</p>
<p>The other major way of interfacing with
our computers is via the
command line interface, or CLI.
The CLI is also suited to
specific ranges of engagement, and
it's the kind of engagement that
allows greater control over
the fundamental uses of our systems.</p>
<p>One reason the CLI provides greater
control over our systems is because
the interaction is text-based.
Text-based interaction requires more
specificity than graphical-based interaction.
By that I mean, it requires us to provide
written instructions to a computer and
to know what instructions to give it
when we want the computer to perform
some specific action.
This means that we have to memorize
some common instructions in order to
use our systems.
This is not necessarily difficult because
many of the most common instructions,
or <em>commands</em>,
are mnemonic, but
it does take some getting used to.</p>
<p>A second reason the CLI provides greater
control over the system is that because
it's text-based,
it can be automated.
We will not cover programming
in this work or course,
but know that all the commands
that we will learn can be put
in a text file,
made into an executable file,
and run like a program.
This makes text-based interaction
rather powerful.</p>
<p>The big gotcha with a text-based
interface with the computer
is that it requires specificity.
We have to be fairly exact
in our commands.
This exactitude requires
an <strong>attention to detail</strong>.
Little things like misplaced punctuation,
missing punctuation,
incorrect capitalization or indentation,
and misspelled words
can cause errors or
prevent the execution of our programs.
It's important to proceed slowly
on the command line and
to <strong>pay attention to the messages</strong>
the screen displays
when we run commands.</p>
<h2 id="basic-commands"><a class="header" href="#basic-commands">Basic Commands</a></h2>
<p>In light of that,
I have developed two programs that
will help you learn and remember
basic Linux shell commands.
The commands that I'll ask you to
learn encompass less than 0.3%
of the commands that are available
on a Linux system, but
they are the most commonly used commands.
Many of the other commands that are
available are for very specific purposes.
I'd estimate
that despite having used the Linux
command line for over 20 years,
I've barely used 20% of them, and
I might be stretching my estimate.</p>
<p>The first set of commands that
I'll ask you to learn and
practice include the following:</p>
<pre><code>list files and directories.................. ls
print name of current/working directory..... pwd
create a new directory...................... mkdir
remove or delete an empty directory......... rmdir
change directory............................ cd
create an empty file........................ touch
print characters to output.................. echo
display contents of a text file............. cat
copy a file or directory.................... cp
move or rename a file or directory.......... mv
remove or delete a file or directory........ rm
</code></pre>
<p>You will practice these commands using
the program that I wrote
called <a href="https://github.com/cseanburns/learn-the-commandline/blob/main/learn-the-cli">learn-the-cli</a>
(I will show you how to install
this and the other programs shortly).</p>
<p>I also developed a <a href="https://github.com/cseanburns/learn-the-commandline/blob/main/flashcards">flashcards</a>
program that will help you learn,
or at least become familiar,
with an additional 45 commands.
(This program is based on one created
by someone else for a different purpose;
see source code link above for credit).
I'll explain these additional commands
as we proceed through the semester.
In the meantime,
I'll ask that you periodically run
the <code>flashcards</code> program to
familiarize yourself with these commands,
which includes the ones in the list above
but also a few additional ones.</p>
<h2 id="the-filesystem"><a class="header" href="#the-filesystem">The Filesystem</a></h2>
<p>In addition to the various commands
that I'll ask you to learn,
you will also have to learn the
structure of the Linux filesystem.
A filesystem has several meanings, but
in this context,
I refer to where the directories
on the Linux system are placed.
I find this to be the most difficult
thing that new Linux users have to learn
for a couple of reasons.
First, modern operating systems tend
to hide (abstract away)
the filesystem from their users.
So even though, for example,
macOS is Unix,
many macOS users that I have taught
are completely unfamiliar with the
layout of directories on their system.
This is because,
per my observations,
macOS Finder does not show
the filesytem by default these days.
Instead it shows its users some common
locations for <strong>folders</strong>.
This might make macOS more usable to
most users, but
it makes learning the system more difficult.</p>
<p>What's common for both macOS and Linux
operating systems is a filesytem based on a
tree-like structure.
These filesystems begin at what's called a
<strong>root</strong> location.
The <strong>root</strong> location is referenced by
a forward slash: <code>/</code>.
All directories <strong>branch</strong> off from root.
The location to any directory is called
a <strong>PATH</strong>.
For example, our home directories on
Linux are located at the following PATH:</p>
<pre><code>/home/USER
</code></pre>
<p>That PATH begins at the root directory <code>/</code>,
proceeds to the directory named <code>home</code>, and
then ends in our <strong>USER</strong> directory,
which will share the same name as our usernames.
As an example,
if my username on a Linux system is <strong>sb</strong>,
then my home directory will be located at:</p>
<pre><code>/home/sb
</code></pre>
<p>It is a little different for Windows users.
Since Windows is not Unix-like,
it uses a different filesystem hierarchy.
Many Windows users might be familiar with
the basics, such as the <strong>C:</strong> drive for the
main storage device or the <strong>D:</strong> drive for
an added USB stick.
As such, the Windows operating system
uses multiple root directories (C:, D:, E:, etc.).
I encourage you to read the following article on
<a href="https://www.redhat.com/sysadmin/linux-filesystem-windows">A quick introduction to the Linux filesystem for Windows users</a>.
The article is published by <em>Red Hat</em>,
which makes its own Linux distribution.</p>
<p>In short, learning the Linux filesystem
requires adopting a new mental model
about how the operating system organizes
its directories and files.
Like learning the basic commands,
it's not too hard,
but it may take time and practice
before it sticks.
To help learn it,
I wrote an additional program that
will let you practice navigating around
the Linux filesystem and making some
changes to it.
The program is called
<a href="https://github.com/cseanburns/learn-the-commandline/blob/main/learn-the-filesystem">learn-the-filesystem</a>.
Before you use this program,
I would like to encourage you to read
another <em>Red Hat</em> article on
<a href="https://www.redhat.com/sysadmin/navigating-filesystem-linux-terminal">Navigating your filesystem in the Linux terminal</a>.
It includes sections that my program will cover
that include:</p>
<ul>
<li>viewing file lists</li>
<li>opening a folder (aka, a directory)</li>
<li>closing a folder</li>
<li>navigating directories</li>
<li>absolute paths</li>
</ul>
<h2 id="bash-the-bourne-again-shell"><a class="header" href="#bash-the-bourne-again-shell">Bash: The Bourne Again Shell</a></h2>
<p>I should point out that the
command line interface that we
are using on our Linux servers
is provided by a <a href="https://en.wikipedia.org/wiki/Unix_shell">shell</a>.
A shell is "both an interactive
command language and a scripting
language" (see link above).
We will use the shell strictly
as a <a href="https://en.wikipedia.org/wiki/Command_language">command language</a>,
but if you're interested someday,
I'd encourage you to explore Bash
as a <a href="https://en.wikipedia.org/wiki/Scripting_language">scripting language</a>
(I personally script in Bash quite a lot, and the
learn-the-cli and flashcard programs
were written in <code>bash</code>).
There are a variety of shells
available for Linux and other Unix-like
operating systems, but
the most popular one and
the one we will be using is called
<a href="https://en.wikipedia.org/wiki/Bash_(Unix_shell)">Bash</a>.</p>
<p>Bash is an acronym for the
<em>Bourne Again Shell</em> because it's
based on the original Unix shell
called the Bourne shell,
written by
<a href="https://en.wikipedia.org/wiki/Stephen_R._Bourne">Stephen Bourne</a>.
Bash itself was written by
<a href="https://en.wikipedia.org/wiki/Brian_Fox_(computer_programmer)">Brian Fox</a>.</p>
<p>I think it's important to know
the history of the technologies
that we use, and
Bash has a super interesting
history that pre-exists Linux.
Therefore, I highly encourage you
listen to the
<a href="https://www.redhat.com/en/command-line-heroes">Command Line Heroes</a> episode titled
<a href="https://www.redhat.com/en/command-line-heroes/season-3/heroes-in-a-bash-shell">Heroes in a Bash Shell</a>,
narrated by
<a href="https://saron.io/">Saron Yitbarek</a>.
The episode recounts Brian Fox's
history with the Bash shell
while he worked for the
<a href="https://en.wikipedia.org/wiki/Free_Software_Foundation">Free Software Foundation</a>
in the 1980s.</p>
<h2 id="conclusion-3"><a class="header" href="#conclusion-3">Conclusion</a></h2>
<p>We will spend the next few weeks
practicing these commands and
learning the filesystem.
We'll do this because knowing
these things is integral to
accomplishing everything else in this work,
including installing and setting up
our content management systems and
the integrated library system.</p>
<p>In the video for this week,
I'll show you how to install the three
programs that I wrote or modified.
We will use <code>git</code> to download them.
The we will move the programs to a
specific directory in our
executable PATH.
This will allow us to run them
simply by typing their names.</p>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<p>To install my practice programs,
login to your Linux virtual instances, and
run the following commands.
You will learn more about these commands shortly.</p>
<p>First, let's take a look at the contents
of your home directory
(the default directory you're in when
you connect to your virtual machine):</p>
<pre><code>ls
</code></pre>
<p>Most likely,
nothing will be listed.</p>
<p>Now let's retrieve the programs
using the <code>git</code> command:</p>
<pre><code>git clone https://github.com/cseanburns/learn-the-commandline.git
</code></pre>
<p>Run the <code>ls</code> command again, and
you'll see a new directory called
<code>learn-the-commandline</code>:</p>
<p><code>ls</code></p>
<p>Next, copy the programs to an executable path:</p>
<pre><code>sudo cp learn-the-commandline/* /usr/local/bin
</code></pre>
<p>Run the first program and
work through it in order to learn
some of the basic commands:</p>
<pre><code>learn-the-cli
</code></pre>
<p>When ready,
run the second program in order
to learn about the Linux filesystem:</p>
<pre><code>learn-the-filesystem
</code></pre>
<p>Finally, periodically run the
<code>flashcards</code> program to refresh your
memory of the basic commands, plus
some other commands that you'll learn
about soon:</p>
<pre><code>flashcards
</code></pre>
<p>After working through the
<code>learn-the-cli</code> program a few times,
you can continue
to practice with the
<code>learn-the-cli-module</code> program.
This is a modified version that
allows you to focus specific
learning modules.</p>
<h3 id="resources"><a class="header" href="#resources">Resources</a></h3>
<p>Here are some additional resources
for learning Bash and Linux shell commands:</p>
<ul>
<li><a href="https://explainshell.com">explainshell.com</a> : helps explain the parts of a shell command</li>
<li><a href="https://www.shellcheck.net/">shellcheck.net</a> : helps debut a shell script</li>
<li><a href="https://github.com/jlevy/the-art-of-command-line">The Art of the Command Line</a> : describes the fundamentals of Bash and the command line</li>
</ul>
<h2 id="references-1"><a class="header" href="#references-1">References</a></h2>
<p>Dourish, P. (2001). <em>Where the Action Is: The Foundations of
Embodied Interaction</em>. MIT Press.
<a href="https://doi.org/10.7551/mitpress/7221.001.0001">https://doi.org/10.7551/mitpress/7221.001.0001</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="text-editors"><a class="header" href="#text-editors">Text editors</a></h1>
<p>As we learn more about how to work on the command line, we will acquire the need to write in plain text or edit configuration files.
Most configuration files for Linux applications exist in the <code>/etc</code> directory and are regular text files.
For example, later in the semester we will install the <a href="https://httpd.apache.org/">Apache Web Server</a>,
and we will need to edit Apache's configuration files in the process.</p>
<p>In order to edit and save text files, like the Apache configuration files, we need a text editor.
Programmers use text editors to write programs, but because programmers often work in graphical user environments,
they may often use graphical text editors or graphical <a href="https://en.wikipedia.org/wiki/Integrated_development_environment">Integrated Development Environments (IDEs)</a>.
It might be that if you work in systems librarianship, that you will often use a graphical text editor,
but knowing something about how to use command line-based editors can be helpful.
For a variety of reasons, a GUI text editor or IDE isn't always available.</p>
<h2 id="what-is-a-plain-text"><a class="header" href="#what-is-a-plain-text">What is a Plain Text?</a></h2>
<p>Plain text is the most basic way to store human-readable textual information.
Whenever we use a word processor program, like Microsoft Office, we are creating a complex series
of files that instruct the Office application how to display the contents of the file as well as how the contents are formatted and arranged.
This can easily be illustrated by using an archive manager to extract the contents of a <strong>.docx</strong> file.
Upon examination, most of the files in a single <strong>.docx</strong> file are plain text that are marked up in XML.
The files are packaged as a <strong>.docx</strong> file and then rendered by an application, commonly Microsoft Word, but any application
that can read <strong>.docx</strong> files will do.</p>
<p>A plain text file only contains <a href="https://www.rapidtables.com/code/text/ascii-table.html">plain text</a>.
Its only arrangement is from top to bottom.
It does not allow for any kind of additional formatting, and it does not include media.
It is the closest thing the digital has to output produced by a typewriter, but
a <a href="https://www.youtube.com/watch?v=jxkygWI-Wfs">typewriter that's connected to the internet</a>.</p>
<p>A lot of content is written in plain text.
For example, HTML is written in plain text and the web browser uses the HTML markup to render how a page will look.</p>
<pre><code>&lt;p&gt;This is using a HTML paragraph tag.
The web browser would render this like the other paragraphs on this page.
However, it's written in a code block,
which allows me to display the HTML as source code.&lt;/p&gt;
</code></pre>
<p>The rendered result is not plain text but HTML, just like the rendered result of all those
XML files in a <strong>.docx</strong> file are not plain text but a <strong>.docx</strong> file.
Software is written in plain text files because programming languages cannot evaluate content that is not just text.
Those of you who have learned how to use the R programming language wrote your R code in plain text likely using the RStudio IDE.
For our purposes, we need plain text files to modify configuration files for the various programs that we will install later.</p>
<h2 id="why-edit-in-plain-text"><a class="header" href="#why-edit-in-plain-text">Why Edit in Plain Text</a></h2>
<p>Most of the time when we configure software, we might use our mouse to find the settings menu
in an application that we are using.
Then we'll make a change in those settings.
For the most part, all we're really doing is making a change in some text file somewhere.
The application's GUI-ness simply obscures that process.</p>
<p>We have to be more direct when we are working on the command line.
That is, the setting configurations we will do require editing plain text files
that modify how the programs will use work.
Often the settings for programs can only be modified by editing their plain text configuration files.</p>
<h2 id="nano"><a class="header" href="#nano"><code>nano</code></a></h2>
<p>The most common text editor on many Linux systems is <code>nano</code>.
The <a href="https://www.nano-editor.org/"><code>nano</code></a> text editor is a user-friendly command line text editor,
but it requires some learning as a new command line user.
The friendliest thing about <code>nano</code> is that it is modeless.
You are already accustomed to using modeless editors.
It means that <code>nano</code> can be used to enter and manipulate text without modes,
like insert mode or command mode.
It is also friendly because, like many graphical text editors and software,
it uses control keys to perform its operations.</p>
<blockquote>
<p>A modal text editor has modes such as insert mode or command mode.
In insert mode, the user types text as anyone would in any kind of editor or word processor.
The user switches to command mode to perform operations on the text,
such as find and replace, saving, and cutting and pasting.
Switching between modes usually involves pressing specific keys.
In Vim and ed(1), my text editors of choice,
users start in command mode and switch to insert mode by pressing the letter <strong>i</strong> or the letter <strong>a</strong>.
The user returns to command mode by pressing the <strong>Esc</strong> key in Vim or by pressing the period in a new line in ed(1).</p>
</blockquote>
<p>The tricky part to learning <code>nano</code> is that the control keys are assigned to different keystroke combinations than what
many graphical editors (or word processors) use by convention today.
For example, instead of Ctrl-c or Cmd-c to copy text, in <code>nano</code> you press the <code>M-6</code> key
(press <code>Alt, Cmd, or Esc key</code> and <code>6</code>) to copy.
To paste, press <code>Ctrl-u</code> instead of the more common <code>Ctrl-v</code>.
Fortunately, <code>nano</code> lists the shortcuts at the bottom of the screen.</p>
<blockquote>
<p><code>nano</code> is a text-editor with old origins.
Specifically, it's a fork of the Unix <code>pico</code> editor.
The keyboard shortcuts used by <code>nano</code> were carried over from the <code>pico</code> editor.
These keyboard shortcuts were designed before the <a href="cua">Common User Access</a> guidelines
helped standardize the common keyboard shortcuts we use today for opening, saving, closing, etc files.</p>
</blockquote>
<p>The shortcuts listed need some explanation.
The carat mark is shorthand for the keyboard's <strong>Control (Ctrl)</strong> key.
Therefore to perform the <strong>Save As</strong> operation on a file,
we <strong>write</strong> out the file by pressing <code>Ctrl-o</code> (although <code>Ctrl-s</code> will work these days, too).
The <strong>M-</strong> key is important.
Depending on your keyboard configuration, it may correspond to your <code>Alt</code>, <code>Cmd</code>, or <code>Esc</code> keys.
To search for text, you press <code>^W</code> or <code>Ctrl</code> and <code>W</code>; that is, <code>Ctrl-W</code> (lowercase <code>w</code> will work).
If your goal is to copy, then press <code>M-6</code> to copy a line.
Move to where you want to paste the text, and press <code>Ctrl-U</code> to paste.</p>
<p>We start <code>nano</code> simply by typing <code>nano</code> on the command line.
This will open a new, unsaved file with no content.
Alternatively, we can start <code>nano</code> by specifying a file name after typing <code>nano</code>.
For example, if I want to open a file called <strong>example.txt</strong>, then I type the following command:</p>
<pre><code>nano example.txt
</code></pre>
<p>If the file doesn't exist, this will create it.
If it does exit, then the command will open it.</p>
<p>One of the other tricky things about <code>nano</code> is that the <em>menu bar</em> (really just a crib sheet, so to speak)
is at the bottom of the screen instead of at the top, which is where we are mostly accustomed to finding it these days.
Also, the <code>nano</code> program does not provide pop up dialog boxes.
Instead, all messages from <code>nano</code>, like what to name a file when we save it, appear at the bottom of the screen.</p>
<p>Lastly, <code>nano</code> also uses distinct terminology for some of its functions.
The most important function to remember is the <strong>Write Out</strong> function, which means to save.</p>
<p>For the purposes of this class, that's all you really need to know about <code>nano</code>.
Use it and get comfortable writing in it. Some quick tips:</p>
<ol>
<li><code>nano file.txt</code> will open and display the file named <strong>file.txt</strong>.</li>
<li><code>nano</code> by itself will open to an empty page.</li>
<li>Save a file by pressing <code>Ctrl-o</code>.</li>
<li>Quit and save by pressing <code>Ctrl-x</code>.</li>
<li>Be sure to follow the prompts at the bottom of the screen.</li>
</ol>
<h2 id="other-editors"><a class="header" href="#other-editors">Other Editors</a></h2>
<p>It's good to be familiar with <code>nano</code> because it's often the default text editor on Linux operating systems nowadays.
However, if you are interested in using a command line text editor with familiar keyboard shortcuts,
then there are others you may want to try.
Specifically, I suggest you investigate the <code>tilde</code> and the <code>micro</code> text editors.
Both of these are really quite nice.</p>
<h3 id="tilde"><a class="header" href="#tilde">tilde</a></h3>
<p>The <a href="tilde"><code>tilde</code></a> text editor is a user friendly text editor that uses conventional keybindings
(like ctrl-s for saving, etc).
<code>tilde</code> also offers a standard menu bar,
which you activate by pressing the <code>Alt</code> key and the letter for the menu option.
For example, to open the File menu, press <code>Alt-F</code>.
Press the <code>Esc</code> key to exit the menu.</p>
<p>You can install <code>tilde</code> via the <code>apt</code> command:</p>
<pre><code>sudo apt install tilde
</code></pre>
<p>You can run <code>tilde</code> either by itself or by invoking a pre-existing or new file:</p>
<pre><code>tilde
</code></pre>
<p>Or:</p>
<pre><code>tilde newfile.md
</code></pre>
<h3 id="micro"><a class="header" href="#micro">micro</a></h3>
<p>The <a href="micro"><code>micro</code></a> text editor is another user friendly editor.
Like <code>tilde</code>, it uses conventional key bindings.
Unlike <code>tilde</code>, there is no menu bar, but you can press <strong>ctrl-g</strong> to open a help menu.
With the help menu open,
use your arrow keys to read through the documentation and learn more about its capabilities and its functions.
One of the nice things about <code>micro</code> is that you can open multiple files in tabs.
Press <strong>ctrl-q</strong> to exit the help menu.</p>
<p>You can install it via the <code>apt</code> command and start the program like you can with the other editors:</p>
<pre><code>sudo apt install micro
</code></pre>
<h2 id="editing-bashrc"><a class="header" href="#editing-bashrc">Editing <code>.bashrc</code></a></h2>
<p>By default, your Bash shell is probably white text on a black background.
We can add some color to this by modifying our Bash shell configuration file.
To do so, open the <code>.bashrc</code> file with <code>nano</code> or your text editor of choice, like <code>tilde</code> or <code>micro</code>:</p>
<pre><code>nano ~/.bashrc
</code></pre>
<p>Scroll to the end of the file and add these two lines:</p>
<pre><code>LS_COLORS='rs=0:di=04;31:fi=00;00:ex=01;93';
export LS_COLORS
</code></pre>
<p>Next, go to the line that starts with the text below, which is probably line 46:</p>
<pre><code># force_color_prompt=yes
</code></pre>
<p>Remove the comment character (i.e., the pound sign, <code>#</code>) at the beginning of the line.
The result should be:</p>
<pre><code>force_color_prompt=yes
</code></pre>
<p>Save the file and exit <code>nano</code>, and then at the shell prompt, type the following command:</p>
<pre><code>source ~/.bashrc
</code></pre>
<h3 id="ls_colors-note"><a class="header" href="#ls_colors-note">LS_COLORS Note</a></h3>
<p>Although the above modification will enable color in our terminals,
we don't have to settle for the defaults.
To change the colors when listing files and directories, we modify the <code>LS_COLORS</code> variable.
The <code>LS_COLORS</code> setting is a bit complicated.
It contains several parameters separated by colons.
Let's break it down:</p>
<pre><code>LS_COLORS='rs=0:di=04;31:fi=00;00:ex=01;93';
</code></pre>
<ul>
<li><code>LS_COLORS</code>: A Bash environmental variable that holds the color values for the <code>ls</code> command.</li>
<li><code>rs=0</code>: This starting parameter resets text formatting to normal (non-bold, default colors, etc). It's placed at the beginning to ensure that we start from basic values.</li>
<li><code>di=04;31</code>: This sets the color of directory names (<code>di</code>) to be underlined (<code>04</code>) and red (<code>31</code>). If you'd rather directory names formatted in bold rather than underlined, you can change <code>04</code> to <code>01</code>. If you'd rather directory names to be green rather than red, you can change <code>31</code> to <code>32</code>. See other formatting properties here: <a href="https://www.bigsoft.co.uk/blog/2008/04/11/configuring-ls_colors">Configuring LS_COLORS</a>.</li>
<li><code>fi=00;00</code>: This sets the color of regular files. Because both values are zero (<code>00;00</code>), the <code>ls</code> command lists these with no special color or style.</li>
<li><code>ex=01;93</code>: This set the color of executable files (i.e., programs) to be bold (<code>01</code>) and bright yellow (<code>93</code>).</li>
</ul>
<p>Feel free to play with the colors of the <code>ls</code> command.
Remember to run <code>source ~/.bashrc</code> to put the changes into effect.</p>
<h2 id="a-note-about-other-text-editors-ed-vivim-emacs"><a class="header" href="#a-note-about-other-text-editors-ed-vivim-emacs">A note about other text editors: ed, Vi/Vim, Emacs</a></h2>
<p>The traditional Unix and Linux text editors are <code>ed</code>, <code>vim</code>, and <code>emacs</code>.
I first started using Linux because I found <code>emacs</code>, but sometime during my early Linux years, I switched to <code>vim</code>.
Vim is a descendant of the <code>vi</code> text editor, which itself is a descendant of the <code>ed</code> editor.
<code>ed</code> was used to write the first versions of the Unix operating system over 50 years ago when <a href="https://en.wikipedia.org/wiki/Teleprinter">teletypes</a>
machines (and not video monitors) were the common input/output devices.
It's still available, and I use it often.
<code>vi</code> was an extension of <code>ed</code> and was written to take advantage of computer monitors.
<code>vim</code> (or Vi Improved) added enhancements to <code>vi</code> and is my main editor.
<code>emacs</code> is highly extensible text editor that can do about anything.
It's so versatile, the saying goes that <code>emacs</code> is an "operating system posing as a text editor."</p>
<p>These editors are extremely powerful once you learn them.
Even though they are quite popular, they are not user-friendly.
(<code>ed</code> probably isn't all that popular, but it has a <a href="https://bsd.network/@ed1conf/">dedicated following</a>.)
If interested, there are plenty of online resources that provide tutorials on getting started with these text editors.
I won't teach you how to use them because it would take too much time,
but they are worth knowing about because all three are important parts of Unix and Linux history.</p>
<h2 id="conclusion-4"><a class="header" href="#conclusion-4">Conclusion</a></h2>
<p>In the prior lesson, we learned how to use the Bash interactive shell.
We will continue to do that, but in the meantime, we begin to learn how to use a command line text editor.
These include <code>nano</code>, <code>tilde</code>, and <code>micro</code>.
We will use a text editor to edit configuration files and publish text to GitHub.
It's your choice what you want to use.</p>
<h2 id="appendix-my-nanorc"><a class="header" href="#appendix-my-nanorc">Appendix: My <code>.nanorc</code></a></h2>
<p>You can configure the look and feel of <code>tilde</code> and <code>micro</code> through their menus or help options.
You can configure <code>nano</code> to look and behave in certain ways, too.
If you decide to use <code>nano</code> and want to mimic the setup I have,
then create a file called <strong>.nanorc</strong> in your home directory:</p>
<pre><code>nano ~/.nanorc
</code></pre>
<p>And add the following to the file:</p>
<pre><code># Syntax:
# set element fgcolor,bgcolor
set titlecolor red,black
set statuscolor blue,white
set errorcolor white,red
set selectedcolor white,black

set numbercolor lightblue
set stripecolor red
set keycolor black,white
set functioncolor white,black 

set speller "aspell -x -c"

## When soft line wrapping is enabled, make it wrap lines at blanks
## (tabs and spaces) instead of always at the edge of the screen.
set atblanks

## Use auto-indentation.
set autoindent

## Back up files to the current filename plus a tilde.
# set backup

## The directory to put unique backup files in.
# set backupdir "~/.backup"

## Use bold text instead of reverse video text.
set boldtext

## Remember the used search/replace strings for the next session.
set historylog

## Display line numbers to the left of the text.
set linenumbers

## Enable vim-style lock-files.  This is just to let a vim user know you
## are editing a file [s]he is trying to edit and vice versa.  There are
## no plans to implement vim-style undo state in these files.
set locking

## Remember the cursor position in each file for the next editing session.
set positionlog

## Do extended regular expression searches by default.
set regexp

## Allow nano to be suspended.
set suspend

## Use this tab size instead of the default; it must be greater than 0.
set tabsize 8

## Convert typed tabs to spaces.
set tabstospaces
</code></pre>
<p>You can read about how to make such settings in the <code>man</code> page for the <code>nano</code> configuration file:</p>
<pre><code>man nanorc
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="documenting-with-git-github-and-markdown"><a class="header" href="#documenting-with-git-github-and-markdown">Documenting with Git, GitHub, and Markdown</a></h1>
<h2 id="introduction-4"><a class="header" href="#introduction-4">Introduction</a></h2>
<p>Documentation is the cornerstone of effective communication and knowledge sharing.
It ensures that processes are understood, tasks are reproducible, and
collaborators can contribute to shared goals.
In this section, we learn how to use Git, GitHub, and Markdown as tools for managing and presenting documentation efficiently.
Specifically, Git with GitHub offer robust version control and collaboration capabilities.
Markdown, a markup language with a simple syntax, facilitates clean, professional documentation compatible with multiple platforms.</p>
<h2 id="create-a-github-account"><a class="header" href="#create-a-github-account">Create a GitHub Account</a></h2>
<p>Let's start by creating an account on GitHub:</p>
<ol>
<li><strong>Visit the GitHub Website</strong>: <a href="https://github.com">GitHub's website</a>.</li>
<li><strong>Sign Up</strong>: Click on the "Sign Up" button usually located at the top right corner of the page.</li>
<li><strong>Enter Your Details</strong>: You will be prompted to enter some basic information:
<ul>
<li><strong>Username</strong>: Choose a unique username that will be your identity on GitHub. Select a name that reflects your personal or professional identity. It will be visible publicly.</li>
<li><strong>Email Address</strong>: Provide a valid, personal email address (not university email address). This will be used for account verification and communication.</li>
<li><strong>Password</strong>: Create a strong password. Use a mix of letters, numbers, and symbols for better security.</li>
</ul>
</li>
<li><strong>Choose a Plan</strong>: GitHub offers various plans. Select the free option, which is fine for most individual users.</li>
</ol>
<h3 id="tips-for-new-users"><a class="header" href="#tips-for-new-users">Tips for New Users</a></h3>
<ul>
<li><strong>Profile Information</strong>: After creating your account, consider adding more details to your profile, like a profile picture and bio, to make it more personable.</li>
<li><strong>Security</strong>: Set up two-factor authentication for added security.</li>
<li><strong>Learning Resources</strong>: GitHub has a <a href="https://docs.github.com/en">wealth of tutorials and guides</a> to help you get started. Utilize these to familiarize yourself with GitHub's features and best practices.</li>
</ul>
<h2 id="create-a-repository-repo-on-github"><a class="header" href="#create-a-repository-repo-on-github">Create a Repository (Repo) on GitHub</a></h2>
<p>Now that you have a GitHub account, your next step is to create a repository (repo) for your documentation project.
I outline the steps below, but see the official documentation:
<a href="https://docs.github.com/en/repositories/creating-and-managing-repositories/creating-a-new-repository">Creating a new repository</a>.
To get started:</p>
<ul>
<li>Click the green <strong>New</strong> button in the upper left corner on your home page.</li>
<li>In the <strong>Owner/Repository name</strong> field, add a name for your repo:
<ul>
<li>Use a descriptive name and avoid spaces and special characters.</li>
</ul>
</li>
<li>Add an optional description:
<ul>
<li>This helps later in case you eventually create lots of repos.</li>
</ul>
</li>
<li>Keep it public.</li>
<li>Click to add a README file:
<ul>
<li>This serves as the main page of your repository on GitHub.</li>
</ul>
</li>
<li>Choose an open source license, if you want.</li>
<li>Click the <strong>Create repository</strong> button.</li>
</ul>
<h3 id="edit-readme"><a class="header" href="#edit-readme">Edit README</a></h3>
<p>You should now see your repository's home page, and you will be viewing the default, empty README.md file.
Let's edit this file on GitHub:</p>
<ul>
<li>Click the pencil icon at the top right of that README.md file.</li>
<li>This opens an editor. Put the cursor after the heading and press Enter.</li>
<li>Add some text that describes the project. You can add a better description later.</li>
<li>Use Markdown code to edit the text you add.</li>
</ul>
<h4 id="markdown-basics"><a class="header" href="#markdown-basics">Markdown Basics</a></h4>
<p>Markdown is a simple markup language for formatting plain text,
which can later be rendered as HTML or even as a PDF, DOCX, etc.
It's a very popular markup language in tech industries, and it's easy to get started.</p>
<p>Here's a quick guide to the most commonly used Markdown syntax:</p>
<h5 id="headings"><a class="header" href="#headings">Headings</a></h5>
<p>Create headings using the pound <code>#</code> symbol before your text.
The number of pound <code>#</code> symbols indicates the level of the heading.
Heading level 1 indicates the main heading and so forth.</p>
<pre><code class="language-markdown"># Heading 1
## Heading 2
### Heading 3
#### Heading 4
##### Heading 5
###### Heading 6
</code></pre>
<h5 id="emphasis"><a class="header" href="#emphasis">Emphasis</a></h5>
<ul>
<li><strong>Bold</strong>: To make text bold, wrap it in double asterisks or double underscores:
<ul>
<li>For example, <code>**bold**</code> or <code>__bold__</code>.</li>
</ul>
</li>
<li><em>Italic</em>: To italicize text, wrap it in single asterisks or single underscores:
<ul>
<li>For example, <code>*italic*</code> or <code>_italic_</code>.</li>
</ul>
</li>
</ul>
<h5 id="lists"><a class="header" href="#lists">Lists</a></h5>
<ul>
<li>
<p><strong>Unordered Lists</strong>: Use asterisks, plus signs, or hyphens to create bullet point lists.</p>
</li>
<li>
<p>Use indentation to create sub-items in a list.</p>
<pre><code class="language-markdown">* Item 1
* Item 2
  * Subitem 1
  * Subitem 2
</code></pre>
</li>
<li>
<p><strong>Ordered Lists</strong>: Use numbers followed by periods for an ordered list.</p>
<pre><code class="language-markdown">1. First item
2. Second item
   1. Subitem 2.1
   2. Subitem 2.2
</code></pre>
</li>
</ul>
<h5 id="links-and-images"><a class="header" href="#links-and-images">Links and Images</a></h5>
<ul>
<li><strong>Links</strong>: To create a named link, wrap the link text in brackets <code>[ ]</code>, and then wrap the URL in parentheses <code>( )</code>.
<ul>
<li>For example, <code>[GitHub](https://github.com)</code> will be <a href="https://github.com">GitHub</a>.</li>
<li>Add a link title: <code>[GitHub](https://github.com "GitHub Code Repo")</code>.</li>
<li>Use reference-style links: <code>[GitHub][github]</code>. Then refer to the full URL elsewhere in the document. For example, I usually add the reference at the end: <code>[github]:https://github.com</code>.</li>
</ul>
</li>
<li><strong>Images</strong>: Similar to links, but start with an exclamation mark, followed by the alt text in brackets, and the URL in parentheses.
<ul>
<li>For example, <code>![Alt text](image-url.jpg)</code>.</li>
<li>In this example, the file <strong>image-url.jpg</strong> must be in the same directory as the Markdown file. It's good practice to organize project files. In this case, I would suggest creating an images directory in the project home and storing images there, with good, descriptive names. Then use a relative path to link to the image: <code>![Alt text](images/image-url.jpg)</code> where <code>images/</code> is the directory name and <code>image-url.jpg</code> is the image file name.</li>
</ul>
</li>
</ul>
<h5 id="code"><a class="header" href="#code">Code</a></h5>
<ul>
<li><strong>Inline Code</strong>: Use a single backtick to wrap your code in text.
<ul>
<li>For example: <code>`inline code`</code>.</li>
</ul>
</li>
<li><strong>Code Blocks</strong>: For larger sections of code, use three backticks or indent
with four spaces.</li>
</ul>
<p>The following code:</p>
<p>```<br />
your code here<br />
```</p>
<p>Will render as:</p>
<pre><code>your code here
</code></pre>
<h5 id="blockquotes"><a class="header" href="#blockquotes">Blockquotes</a></h5>
<p>To create a blockquote, use the greater than symbol <code>&gt;</code> at the beginning of a line.
For nested blockquotes, use multiple <code>&gt;</code> symbols.</p>
<p>This code:</p>
<pre><code>&gt; This is a blockquote.
&gt;&gt; This is a nested blockquote.
</code></pre>
<p>Will render as:</p>
<blockquote>
<p>This is a blockquote.</p>
<blockquote>
<p>This is a nested blockquote.</p>
</blockquote>
</blockquote>
<h5 id="horizontal-rules"><a class="header" href="#horizontal-rules">Horizontal Rules</a></h5>
<p>Create a horizontal line or rule by using three or more asterisks, dashes, or underscores on a new line.</p>
<pre><code>---
</code></pre>
<p>The above will render as:</p>
<hr />
<h5 id="additional-tips"><a class="header" href="#additional-tips">Additional Tips</a></h5>
<ul>
<li><strong>Whitespace and Line Breaks</strong>: In Markdown, paragraphs are automatically created when text is separated by an empty line.
To create a new line without starting a new paragraph, end a line with two or more spaces.</li>
<li><strong>Escaping Markdown</strong>: To display a Markdown character, precede it with a backslash (<code>\</code>). For example, <code>\*demo italicizing\*</code>.</li>
</ul>
<h3 id="preview-and-save"><a class="header" href="#preview-and-save">Preview and Save</a></h3>
<p>As you edit your README.md file, you can click the <strong>Preview</strong> tab to see how it will be rendered.</p>
<p>Once you are finished editing, save with the following steps:</p>
<ul>
<li>Click on the <strong>Commit changes...</strong> button.</li>
<li>A pop-up will appear. Update the <strong>Commit message</strong> or leave as-is:
<ul>
<li>When you make more substantive edits, you will want to leave descriptive commit messages.</li>
<li>This helps with with version control.</li>
</ul>
</li>
<li>Press the <strong>Commit changes</strong> button.</li>
<li>Then click on the repo link to return to your repo's homepage.</li>
</ul>
<h3 id="file-naming-conventions"><a class="header" href="#file-naming-conventions">File Naming Conventions</a></h3>
<p><a href="https://en.wikipedia.org/wiki/README">README</a> files serve as a <em>de facto</em> standard file.
They provide a description of the project, outline its purpose, or provide instructions on using the repository.
As you work on your projects, you can return to edit your README file to add more information about your work.</p>
<p>In the process of working on your project, you will create other files, and you want to name them well.
Good file names help to organize and maintain a clear and efficient documentation system.
They help provide and ensure:</p>
<ol>
<li><strong>Clarity and Accessibility</strong>: To save time and reduce confusion, use well-named files:
<ul>
<li>They make it easier to identify and understand your files at a glance.</li>
</ul>
</li>
<li><strong>Ease of Navigation</strong>: Use consistent naming to aid navigating through files.</li>
<li><strong>System Compatibility</strong>:
<ul>
<li>Avoid spaces in file names. They cause issues in URLs and command-line operations.</li>
<li>Avoid special characters like <code>!</code>, <code>$</code>, <code>#</code>, <code>%</code>, etc. They have specific functions in certain environments or scripts, including shell environments.</li>
<li>Name files with single words or combine words using:
<ul>
<li>camelCase: <code>serverSetupGuide.md</code>,</li>
<li>underscores: <code>server_setup_guide.md</code>, or</li>
<li>hyphens: <code>server-setup-guide.md</code>.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3 id="the-importance-of-md-extension-for-markdown-files"><a class="header" href="#the-importance-of-md-extension-for-markdown-files">The Importance of <code>.md</code> Extension for Markdown Files</a></h3>
<p>File name extensions are not always necessary, especially on Linux and Unix systems.
However, when it comes to Markdown files, add the <code>.md</code> extension (e.g., <code>README.md</code> rather than just <code>README</code>).
This helps in the following ways:</p>
<ol>
<li><strong>GitHub Rendering</strong>: GitHub automatically renders files with a <code>.md</code> extension as formatted Markdown.
This means your documentation will be displayed with the intended formatting (like headers, lists, links, etc.) when viewing it on GitHub.</li>
<li><strong>Editor Support</strong>: Most code editors use file extensions, like <code>.md</code>, and provide appropriate syntax highlighting.</li>
<li><strong>Consistency and Recognition</strong>: Using a file extension, like <code>.md</code>, helps users identify the file type and its intended use.</li>
</ol>
<p>For instance, naming a file <code>installation_guide.md</code> ensures that GitHub renders the file as a Markdown document
and displays all formatting correctly in the browser.
This enhances readability and makes the documentation more user-friendly.
Your text editor will also recognize the file extension and colorize the syntax appropriately.</p>
<h2 id="gitting-started"><a class="header" href="#gitting-started"><em>Gitting</em> Started</a></h2>
<p>Now that we've set up our GitHub repo, it's time to return to our virtual machines.
<code>git</code> is already installed on these machines, but it needs to be configured.</p>
<h3 id="git-configuration"><a class="header" href="#git-configuration">Git Configuration</a></h3>
<p>First, connect to your remote server and run the commands below to begin configuring <code>git</code>.
In the example commands below, note the quotes around the <strong>Your Name</strong> command.
Replace <strong>Your Name</strong> with your name and keep those quotes.
You don't need quotes in the commands for setting your <strong>github_username</strong> and <strong>email address</strong>.
Simply replace your info in the respective places.
Use the same information you used when setting up your GitHub account.
Run these commands separately:</p>
<pre><code>git config --global user.name "Your Name"
git config --global user.email youremail@example.com
</code></pre>
<p>Next, configure <code>git</code> to use the name <strong>main</strong> as your default branch.
The second command instructs <code>git</code> to use <code>nano</code> as your default editor.
Run these two commands as-is, but if you are using a different text editor (like <code>tilde</code> or <code>micro</code>),
be sure to lookup the appropriate command for that editor
(it's just <code>tilde</code> or <code>micro</code>, though).
Keep the quotes around the editor name, which should be the name of the executable (i.e., program name) for your text editor.</p>
<pre><code>git config --global init.defaultBranch main
git config --global core.editor "nano"
</code></pre>
<p>Verify the above settings with the following command:</p>
<pre><code>git config --list
</code></pre>
<p>For additional details, see the Git documentation on getting started:</p>
<ul>
<li><a href="https://git-scm.com/book/en/v2/Getting-Started-First-Time-Git-Setup">Getting Started - First-Time Git Setup</a></li>
</ul>
<p>Next, we need to configure how <code>git</code> and GitHub work together.</p>
<h3 id="generate-ssh-keys"><a class="header" href="#generate-ssh-keys">Generate SSH Keys</a></h3>
<p>We need to secure our <code>git</code> and GitHub connection and repositories.
We do that first by creating an SSH key.</p>
<p>On the server:</p>
<ol>
<li>Generate a new ssh key with the following command:
<ol>
<li><code>ssh-keygen -t ed25519 -C "your_email@example.com"</code></li>
<li>Use the same email that you used when signing up with GitHub.</li>
</ol>
</li>
<li>Copy your SSH public key to your clipboard:
<ol>
<li>View it with this command: <code>cat ~/.ssh/id_ed25519.pub</code>.</li>
<li>Then select it with your mouse and copy it.</li>
<li>Open GitHub and visit Settings.</li>
<li>In the Access section of sidebar, click <strong>SSH and GPG keys</strong></li>
<li>Click <strong>New SSH key</strong> or <strong>Add SSH key</strong></li>
<li>In the <strong>Title</strong> field, add a descriptive label for the new key:
<ol>
<li>For example, the name of the machine you used to generate the key.</li>
</ol>
</li>
<li>Select the key type: authentication.</li>
<li>Paste your SSH public key in the <strong>Key</strong> field.</li>
<li>Click <strong>Add SSH key</strong></li>
<li>See the official documentation here: <a href="https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account">Adding a New SSH Key</a></li>
</ol>
</li>
<li>On your virtual machine, setup the SSH public key as your signing key:
<ol>
<li><code>git config --global gpg.format ssh</code></li>
<li><code>git config --global user.signingkey $HOME/.ssh/id_ed25519.pub</code></li>
<li><code>git config --global commit.gpgsign true</code></li>
<li>See the documentation at: <a href="https://docs.github.com/en/authentication/managing-commit-signature-verification/telling-git-about-your-signing-key">Telling Git About Your Signing Key</a></li>
</ol>
</li>
</ol>
<h3 id="clone-your-repo"><a class="header" href="#clone-your-repo">Clone Your Repo</a></h3>
<p>Now that you have <code>git</code> configured to work with GitHub, clone your repo to your virtual machine.</p>
<ul>
<li>Return to GitHub and your repo's homepage.</li>
<li>Click the green <strong>Code</strong> drop down button.</li>
<li>Make sure the SSH option is selected.</li>
<li>Copy the command, which should have the following syntax:
<ul>
<li><code>git@github.com:repo_user/repo_name.git</code></li>
<li><strong>repo_user</strong> should be your GitHub username.</li>
<li><strong>repo_name.git</strong> should be your repo's name.</li>
</ul>
</li>
<li>Return to your Linux virtual machine, and run the following command to clone your repo:
<ul>
<li><code>git clone git@github.com:repo_user/repo_name.git</code></li>
<li>This command will create a new directory named after your repo.</li>
</ul>
</li>
</ul>
<h3 id="stage-commit-and-push-your-repo"><a class="header" href="#stage-commit-and-push-your-repo">Stage, Commit, and Push Your Repo</a></h3>
<p>Now use your text editor (e.g., <code>nano</code>) to make changes to your repository.
Navigate to your repo's directory on your virtual machine:</p>
<pre><code>cd repo_name
</code></pre>
<p>Create and open a new file.
I'll use <strong>entry_one.md</strong> as an example file name, but feel free to choose a different name:</p>
<pre><code>nano entry_one.md
</code></pre>
<p>Add whatever you'd like here to get started.
When completed, save the file and exit <code>nano</code>.</p>
<p>Now we need to <strong>push</strong> our changes to our GitHub repo.
First, <strong>stage</strong> the changes with the <code>git add</code> command:</p>
<pre><code>git add entry_one.md
</code></pre>
<p>Then <strong>commit</strong> the changes and add a commit message with the <code>-m</code> option:</p>
<pre><code>git commit -m "commit message here"
</code></pre>
<p>Then <strong>push</strong> the commit to our GitHub URL (i.e., <strong>origin</strong>) and <strong>main</strong> branch:</p>
<pre><code>git push origin main
</code></pre>
<p>Visit your repo's homepage on GitHub to see the update.</p>
<p>Whenever we add, edit, or delete a file or directory in our local repo,
we follow the stage (add), commit, and push steps above.
You can monitor the status of your local repository with the following command:</p>
<pre><code>git status
</code></pre>
<h3 id="pull"><a class="header" href="#pull">Pull</a></h3>
<p>Your remote repository is located on GitHub.
Your local repository is located on your virtual instance.
Get used to working on your documentation in your local repository.
However, if you mix it up and make edits to files on your remote repository via the GitHub web interface,
then you need sync your local and remote repositories before switching back to local work.
To do that, you need to run a <code>pull</code> command:</p>
<pre><code>cd repo_name
git pull origin main
</code></pre>
<p>If you often switch between local and remote repo work, the repos will quickly grow apart and it will be hard to merge them later.</p>
<h3 id="git-basics"><a class="header" href="#git-basics">Git Basics</a></h3>
<p>Now that we've covered the basic practice, let's review some concepts.</p>
<h4 id="repos"><a class="header" href="#repos">Repos</a></h4>
<p>The first Git concept to learn is the repository concept.
Git uses two kinds of repositories:</p>
<ul>
<li>local repository (repo)</li>
<li>remote repository (repo)</li>
</ul>
<p>The local repo is a project directory (or folder) on your computer.
I will use the term directory and not folder since the former term is more commonly used in tech fields.
The project directory contains all the project files and any sub-directories for the project.</p>
<p>The remote repo is where we send, retrieve, or sync the files and directories that are contained in the local repo.
We can retrieve projects from other repos that other people or organizations have created, if those repos are public.</p>
<p>With Git and GitHub, we can start a project on the local system (i.e., our computers)
or start a project by creating a remote repo on GitHub and then cloning it to our local system.</p>
<h3 id="branches"><a class="header" href="#branches">Branches</a></h3>
<p>The second Git concept to learn is:</p>
<ul>
<li>branches</li>
</ul>
<p>When you configure a directory on your local system to become a Git project, you create a default branch for your project.
For small projects, we might only work in the default branch.
The default branch will be named <strong>main</strong>.</p>
<p>However, since Git is a version control system,
we can create additional branches to test or work on different components of our projects without messing with the main branch.
For large or complex projects, we would work and switch among different branches.
A large project might be a big website, an software application, or even an operating system.
Working in non-main branches (e.g., a testing branch),
allows us to develop components of our project without interfering with the main branch, which might be at a stable version of our project. And then when we are ready, we can merge a testing branch with our main branch,
or we can delete the testing branch if we don't want to use it.</p>
<p>We will primarily work with the default, main branch with our projects,
but you should read the <a href="https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell">Git documentation on branches</a>.</p>
<p>For future reference, here's a nice cheat sheet of <a href="https://confluence.atlassian.com/bitbucketserver/basic-git-commands-776639767.html">Git commands</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="learning-the-command-line"><a class="header" href="#learning-the-command-line">Learning the Command Line</a></h1>
<p>It's obviously more common for people today
to learn how to use a computer via a
graphical user interface (GUI), but
there are benefits to learning a
command line interface (CLI).
In this section,
we learn some of the basics
of using the <a href="https://en.wikipedia.org/wiki/Bash_(Unix_shell)">Bash shell</a> as our CLI.
Our primary goal is to learn how
to use the CLI as a file manager and
to perform some text editing.
However, if you find this interface appealing,
know that Bash is a
<a href="https://www.redhat.com/sysadmin/learn-bash-scripting">full-fledged programming language</a>, and
I encourage you to explore it as a scripting language.</p>
<p>There are three reasons,
from a systems administration/librarianship
point of view,
to prefer the CLI over the GUI.</p>
<ul>
<li>First, the GUI entails extra software, and the more software we have on a
server, the more resources (memory, CPU, storage, etc) that software
consumes. We would much rather have our machine's resources being used to
provide the services we build them to do than to run irrelevant software.</li>
<li>Second, the extra software a GUI requires means that we expose our systems to
additional security risks. That is, every time we install more software on
our servers, the server becomes more vulnerable because all software is buggy.
This means that we want to be conservative, careful, and protective of our
systems. This is especially true for production systems.</li>
<li>Third, graphical user interfaces do not provide a good platform for
automation, at least not remotely as well as command line interfaces do.
Working on the command line, because it is a text-based environment, in what is
known as a <a href="https://en.wikipedia.org/wiki/Unix_shell">shell</a>, is a reproducible process. That is not as easily
true in a GUI.</li>
</ul>
<p>Fortunately, Linux, and
many other Unix-like operating systems,
have the ability to operate without graphical user interfaces.
This is partly the reason why these operating systems
have done so well in the server market.</p>
<p>In this section,
our focus is learning the command line environment.
We will do this using the Bash shell.
We will learn how to use the shell,
how to navigate around the filesystem,
how to perform basic tasks, and
explore other functions and utilities
the shell has to offer.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="searching-with-grep"><a class="header" href="#searching-with-grep">Searching with grep</a></h1>
<p>As a systems librarian, you might deal with large amounts of text-based data:
logs from library systems, metadata files, MARC records, exported citation data, and configuration files for tools that you manage.
Searching these efficiently is crucial when troubleshooting issues, extracting insights, or automating repetitive tasks.
Graphical interface-based applications exist for some of these tasks, but they can be slow, inflexible, or unavailable when working on a remote server.
Fortunately we have <code>grep</code>, which is a command-line tool that allows for fast and precise searching.
Using <code>grep</code>, we can accomplish all of the above.</p>
<p>There are other powerful utilities and programs to process, manipulate, and analyze text files (e.g., <code>awk</code>, <code>sed</code>, and more).
However, in this section, we will focus on the <code>grep</code> utility, which offers advanced methods for searching the contents of text files.
Specifically, we'll work through an introduction of <code>grep</code> using a small data file that will help us understand how <code>grep</code> works.
Then we will use <code>grep</code> to analyze bibliographic data downloaded as a <code>.bib</code> file from Scopus.
This will demonstrate how <code>grep</code> can help you filter specific information from a structured dataset—an approach that can also be applied to processing
catalog records, debugging system errors, or analyzing usage logs (e.g., see <a href="https://doi.org/10.1080/00987913.2017.1281788">Arneson, 2017</a>).</p>
<h2 id="grep"><a class="header" href="#grep"><code>grep</code></a></h2>
<p>The <code>grep</code> command is one of my most often used commands.
The purpose of <code>grep</code> is to "print lines that match patterns" (see <code>man grep</code>).
In other words, it searches text, and it's super powerful.</p>
<p><code>grep</code> works line by line.
So when we use it to search a file for a <strong>string</strong> of text, it will return the whole line that matches the string.
This <strong>line by line</strong> idea is part of the history of Unix-like operating systems, and it's important to remember that most utilities
and programs that we use on the commandline are line oriented.</p>
<blockquote>
<p>"A string is any series of characters that are interpreted literally by a script. For example, 'hello world' and 'LKJH019283' are both examples of strings"
(<a href="https://www.computerhope.com/jargon/s/string.htm">Computer Hope</a>).
More generally, it's a type of data structure.</p>
</blockquote>
<p>To visualize how <code>grep</code> works, let's consider a file called <strong>operating-systems.csv</strong> with content as seen below.
It's helps to learn something like <code>grep</code> when working with easy, clear examples.</p>
<pre><code>OS, License, Year
Chrome OS, Proprietary, 2009
FreeBSD, BSD, 1993
Linux, GPL, 1991
macOS, Proprietary, 2001
Windows NT, Proprietary, 1993
Android, Apache, 2008
</code></pre>
<p>We can use <code>grep</code> to search for anything in that file.
Let's start with a search for the string <strong>Chrome</strong>.
Notice that even though the string <strong>Chrome</strong> only appears once, and in one part of a line, <code>grep</code> returns the entire line.</p>
<p><strong>Command:</strong></p>
<pre><code>grep "Chrome" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Chrome OS, Proprietary, 2009
</code></pre>
<h3 id="case-matching"><a class="header" href="#case-matching">Case Matching</a></h3>
<p>Be aware that, <em>by default</em>, <code>grep</code> is case-sensitive, which means a search for the string <strong>chrome</strong>, with a lower case <strong>c</strong>, returns no results.
However, many Linux command line utilities can have their functionality extended through command line options.
<code>grep</code> has an <code>-i</code> option that can be used to to ignore the case of the search string.
You can learn about <code>grep</code>'s other command line options in its man page: <code>man grep</code>.
In the following examples, <code>grep</code> returns nothing in the first search since we do not capitalize the string <strong>chrome</strong>.
However, adding the <code>-i</code> option results in success since <code>grep</code> is instructed to ignore case:</p>
<p><strong>Command:</strong></p>
<pre><code>grep "chrome" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<p>None.</p>
<p><strong>Command:</strong></p>
<pre><code>grep -i "chrome" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Chrome OS, Proprietary, 2009
</code></pre>
<h3 id="invert-matching"><a class="header" href="#invert-matching">Invert Matching</a></h3>
<p><code>grep</code> can do inverse searching.
That is, we can search for lines that <strong>do not</strong> match our string using the <code>-v</code> option.
Options can often be combined for additional functionality.
We can combine <code>-v</code> to inverse search with <code>-i</code> to ignore the case.
In the following example, we search for all lines that do not contain the string <strong>chrome</strong>:</p>
<p><strong>Command:</strong></p>
<pre><code>grep -vi "chrome" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>FreeBSD, BSD, 1993
Linux, GPL, 1991
iOS, Proprietary, 2007
macOS, Proprietary, 2001
Windows NT, Proprietary, 1993
Android, Apache, 2008
</code></pre>
<h3 id="regular-expressions"><a class="header" href="#regular-expressions">Regular Expressions</a></h3>
<p>Sometimes data files, like spreadsheets, contain header columns in the first row.
We can use <code>grep</code> to remove the first line of a file by inverting our search and selecting all lines not matching "OS" at the start of a line.
Here the carat key <code>^</code> is a <strong>regex</strong> indicating the start of a line.
Again, this <code>grep</code> command returns all lines that do not match the string <strong>os</strong> at the start of a line, ignoring case:</p>
<p><strong>Command:</strong></p>
<pre><code>grep -vi "^os" operating-systems.csv
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>Chrome OS, Proprietary, 2009
FreeBSD, BSD, 1993
Linux, GPL, 1991
iOS, Proprietary, 2007
macOS, Proprietary, 2001
Windows NT, Proprietary, 1993
Android, Apache, 2008
</code></pre>
<p>Alternatively, since we know that the string <strong>Year</strong> comes at the end of the first line, we can use <code>grep</code> to invert search for that.
Here the dollar sign key <code>$</code> is a <strong>regex</strong> indicating the end of a line.
Like above, this <code>grep</code> command returns all lines that do not match the string <strong>year</strong> at the end of a line, ignoring case.
The result, in this specific instance, is exactly the same as the last command, indicating that there are sometimes many ways to achieve the same outcome with various commands:</p>
<p><strong>Command</strong>:</p>
<pre><code>grep -vi "year$" operating-systems.csv
</code></pre>
<p><strong>Output</strong>:</p>
<pre><code>Chrome OS, Proprietary, 2009
FreeBSD, BSD, 1993
Linux, GPL, 1991
iOS, Proprietary, 2007
macOS, Proprietary, 2001
Windows NT, Proprietary, 1993
Android, Apache, 2008
</code></pre>
<p>The <code>man grep</code> page lists other options, but a couple of other good ones include:</p>
<h3 id="count-matches"><a class="header" href="#count-matches">Count Matches</a></h3>
<p>If we're looking for patterns in a data file, we may also be interested in their frequency.
Fortunately, we can get a count of the matching lines with the <code>-c</code> option.</p>
<p>In the next example, I get a total count of lines that contain the word <em>Proprietary</em>:</p>
<pre><code>grep -ic "proprietary" operating-systems.csv
</code></pre>
<p>More broadly, we can get a total count of rows in our file after excluding the header.
In other words, we can get the total number of data rows or records:</p>
<pre><code>grep -vic "year$" operating-systems.csv
</code></pre>
<h3 id="alternate-matching"><a class="header" href="#alternate-matching">Alternate Matching</a></h3>
<p>We can do a sort of Boolean OR search by using the vertical bar <code>|</code>, also called the <strong>infix operator</strong>.
This is called an alternate expression.
That is, using alternate matching, we can search for at least one string among multiple options.</p>
<p>Here is an example where only one string returns a true value since the file contains <strong>bsd</strong> but not <strong>atari</strong>:</p>
<p><strong>Command:</strong></p>
<pre><code>grep -Ei "(bsd|atari)" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>FreeBSD, BSD, 1993
</code></pre>
<p>Here's an example where both strings evaluate to true:</p>
<p><strong>Command:</strong></p>
<pre><code>grep -Ei "(bsd|gpl)" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>FreeBSD, BSD, 1993
Linux, GPL, 1991
</code></pre>
<p>You can use more than two strings:</p>
<pre><code>grep -Ei "(bsd|gpl|apache)" operating-systems.csv
</code></pre>
<h3 id="whole-word-matching"><a class="header" href="#whole-word-matching">Whole Word Matching</a></h3>
<p>By default, <code>grep</code> will return results where the string appears within a larger word, like <strong>OS</strong> in <strong>macOS</strong>.</p>
<p><strong>Command:</strong></p>
<pre><code>grep -i "os" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>OS, License, Year
Chrome OS, Proprietary, 2009
iOS, Proprietary, 2007
macOS, Proprietary, 2001
</code></pre>
<p>However, we might want to limit results so that we only return results where <strong>OS</strong> is a complete word.
To do that, we can surround the string with special characters:</p>
<p><strong>Command:</strong></p>
<pre><code>grep -i "\&lt;os\&gt;" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>OS, License, Year
Chrome OS, Proprietary, 2009
</code></pre>
<p>Sometimes I find it hard to remember the backslash and angle bracket combinations because they're too much alike HTML syntax but not exactly like HTML syntax.
Fortunately, <code>grep</code> has a <code>-w</code> option to match whole words.
This functions as another way of searching for whole words:</p>
<p><strong>Command:</strong></p>
<pre><code>grep -wi "os" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>OS, License, Year
Chrome OS, Proprietary, 2009
</code></pre>
<h3 id="context-matches"><a class="header" href="#context-matches">Context Matches</a></h3>
<p>Sometimes we want the context for a result; that is, we might want to print lines that surround our matches.
For example, to print the matching line plus the two lines after the matching line using the <code>-A NUM</code> option, where <strong>NUM</strong> equals the number of lines to return after the matching line:</p>
<p><strong>Command:</strong></p>
<pre><code>grep -i "linux" -A2 operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Linux, GPL, 1991
macOS, Proprietary, 2001
Windows NT, Proprietary, 1993
</code></pre>
<p>Or, print the matching line plus the two lines before the matching line using the <code>-B NUM</code> option:</p>
<p><strong>Command</strong></p>
<pre><code>grep -i "linux" -B2 operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Chrome OS, Proprietary, 2009
FreeBSD, BSD, 1993
Linux, GPL, 1991
</code></pre>
<p>We can combine many of the variations.
Here I search for the whole word <strong>BSD</strong>, case insensitive, and print the line before and the line after the match:</p>
<p><strong>Command:</strong></p>
<pre><code>grep -iw -C1 "bsd" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Chrome OS, Proprietary, 2009
FreeBSD, BSD, 1993
Linux, GPL, 1991
</code></pre>
<h3 id="halt-matching"><a class="header" href="#halt-matching">Halt Matching</a></h3>
<p>We can use another option to stop returning results after some number of hits.
Here I use <code>grep</code> to return a search for the string "proprietary" and stop after the first hit:</p>
<p><strong>Command:</strong></p>
<pre><code>grep -i -m1 "proprietary" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Chrome OS, Proprietary, 2009
</code></pre>
<h3 id="returning-line-numbers"><a class="header" href="#returning-line-numbers">Returning Line Numbers</a></h3>
<p>We can add the <code>-n</code> option to instruct <code>grep</code> to tell us the line number for each hit.
Below we see that the string "proprietary" is found on lines 2, 5, and 6.</p>
<p><strong>Command:</strong></p>
<pre><code>grep -in "proprietary" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>2:Chrome OS, Proprietary, 2009
5:macOS, Proprietary, 2001
6:Windows NT, Proprietary, 1993
</code></pre>
<h3 id="character-class-matching"><a class="header" href="#character-class-matching">Character Class Matching</a></h3>
<p>We can use <code>grep</code> to search for patterns in strings instead of literal words.
Here we use what's called <strong>character classes</strong> and <strong>repetition</strong> to search for five letter words that contain any English character <strong>a through z</strong>:</p>
<p><strong>Command:</strong></p>
<pre><code>grep -Eiw "[a-z]{5}" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Linux, GPL, 1991
macOS, Proprietary, 2001
</code></pre>
<p>Or four letter numbers, which highlights the years:</p>
<p><strong>Command:</strong></p>
<pre><code>grep -Eiw "[0-9]{4}" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Chrome OS, Proprietary, 2009
FreeBSD, BSD, 1993
Linux, GPL, 1991
macOS, Proprietary, 2001
Windows NT, Proprietary, 1993
Android, Apache, 2008
</code></pre>
<p><code>grep</code> can also search for words that begin with some letter and end with some letter and with a specified number of letters between.
Here we search for words that start with <strong>m</strong>, end with <strong>s</strong>, and have three letters in the middle:</p>
<p><strong>Command:</strong></p>
<pre><code>grep -Eiw "m.{3}s" operating-systems.csv
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>macOS, Proprietary, 2001
</code></pre>
<h2 id="practice"><a class="header" href="#practice">Practice</a></h2>
<p>Let's use the <code>grep</code> command to investigate bibliographic data.
Our task is to:</p>
<ol>
<li>Search <em>Scopus</em>.</li>
<li>Download a <a href="https://www.bibtex.org/Format/"><em>BibTeX</em></a> file from <em>Scopus</em> as a .bib file.</li>
<li>Use the <code>grep</code> command to search the downloaded <em>BibTeX</em> file, which should be named <strong>scopus.bib</strong>.</li>
</ol>
<h3 id="download-data"><a class="header" href="#download-data">Download Data</a></h3>
<p>I'm using Scopus data in this example, but other bibliographic data can be downloaded from other databases.</p>
<ol>
<li>From your university's website, find Scopus.</li>
<li>In Scopus, perform a search.</li>
<li>Select the documents you want to download.</li>
<li>Click on the <strong>Export</strong> button.</li>
<li>Click on <em>BibTeX</em> under the listed file types.</li>
<li>Select all <strong>Citation Information</strong> and <strong>Bibliographic Information</strong>. Select more in interested.</li>
<li>Click on <strong>Export</strong>.</li>
</ol>
<p>The file should be saved to your Downloads folder and titled <strong>scopus.bib</strong>.
The next step is to upload the file to your virtual instance.
See the steps below.</p>
<h3 id="upload-to-gcloud"><a class="header" href="#upload-to-gcloud">Upload to gcloud</a></h3>
<p>There are several methods for uploading and downloading files to your Google Cloud instance.
The two main ones I cover below depend on how you connect to your virtual instances, but see the full documentation at:
<a href="https://cloud.google.com/compute/docs/instances/transfer-files">Transfer files to Linux VMs</a>.</p>
<h4 id="gcloud-compute-scp"><a class="header" href="#gcloud-compute-scp"><code>gcloud compute scp</code></a></h4>
<p>If you use the <code>gcloud compute</code> command to connect to your virtual instance, you use a similar command to upload and download files.
However, there are some differences between the two commands.
The <code>gcloud</code> copy command uses <code>scp</code> instead of <code>ssh</code> and then specifies the local file to transfer and the remote location.
The following command copies the local file titled <strong>file_name</strong> to the remote server.
Simply replace the file name, server, zone, and project names with those specific to your virtual instances.</p>
<pre><code>gcloud compute scp file_name "server_name":~/ --zone "zone_name" --project "project_name"
</code></pre>
<h4 id="ssh-in-browser"><a class="header" href="#ssh-in-browser">SSH-in-browser</a></h4>
<p>To upload to your virtual instance using the web browser, connect via the <strong>Open in browser window</strong> method from your VM instances console.
Once you've established a connection, then click on the <strong>UPLOAD FILE</strong>.
Select the file and proceed.
(If you get an error, then try again.)</p>
<h3 id="investigate"><a class="header" href="#investigate">Investigate</a></h3>
<p>Now that the file is uploaded, the first task is to to get an understanding of the structure of the data.
<em>BibTeX</em> (.bib) files are structured files that contain bibliographic data.
It's important to understand how files are structured if we want to search them efficiently.</p>
<p>The <strong>scopus.bib</strong> file begins with information about the source (<em>Scopus</em>) of the records and the date the records were exported.
These two lines and the empty line after them can be safely deleted or ignored.</p>
<p>Each bibliographic record in the file begins with an <strong>entry type</strong> (or document type) preceded by an at <strong>@</strong> sign.
Example entry types include: <a href="https://www.bibtex.com/e/entry-types/">article, book, booklet, conference, and more</a>.
There is a opening curly brace after the entry or document type.
These curly braces mark the beginning and ending of each record.</p>
<p>The cite key follows the opening curly brace.
The cite key is an identifier that often refers to the author's name and includes publication date information.
For example, a cite key might look as follows and would stand for the author <strong>Budd</strong> and the date <strong>2020-11-23</strong>.</p>
<pre><code>Budd20201123
</code></pre>
<p>The rows below the entry type contain the metadata for the record.
Each row begins with a tag or field name followed by an equal sign, which is then followed by the values or content for that tag.
For example, there's an author tag, an equal sign, and then a list of authors.
There is a standard list of <em>BibTeX</em> fields.
Example fields include: <a href="https://bibtex.eu/fields/">author, doi, publisher, title, journal, year, and more</a>.
The fields are standardized because some programs use <em>BibTeX</em> records to manage and generate bibliographies, in-text citations, footnotes, etc.</p>
<p>The content of each field is enclosed in additional curly braces.
Each line ends with a comma, except for the last line.
The record ends with a closing curly brace.</p>
<h4 id="document-types"><a class="header" href="#document-types">Document Types</a></h4>
<p>We can use <code>grep</code> to examine the types of documents in the list of records.
In the following command, I use the carat key <code>^</code>, which is a regular expression to signify the start of a line, to search for lines beginning with the at <code>@</code> symbol.
The following <code>grep</code> command therefore means: return all lines that begin with the at <code>@</code> symbol:</p>
<pre><code>grep "^@" scopus.bib
</code></pre>
<p>The results show, for this particular data, that I have BOOK and ARTICLE entry types.
The data I'm using does not contain many records, but if it contained thousands or more, then it would be helpful to filter these results.</p>
<p>Thus, below I use the <code>-E</code> option to extend <code>grep</code>'s regular expression engine.
I use the <code>(A|B)</code> to tell <code>grep</code> to search for letters after the at sign <code>@</code> that start with either A or B, for ARTICLE or BOOK.
Then I use regular expression character class matching with <code>[A-Z]</code> to match any letters after the initial A or B characters.
The <code>-i</code> option turns off case sensitivity, and the <code>-o</code> option returns only matching results from the lines.
I pipe the output of the <code>grep</code> command to the <code>sort</code> command to sort the results alphabetically:</p>
<pre><code>grep -Eio "^@(A|B)[A-Z]*" scopus.bib | sort
</code></pre>
<blockquote>
<p>Tip: Without using the <code>sort</code> command, the <code>grep</code> command returns the results in the order it finds them.
To see this, run the above command with and without piping to the <code>sort</code> command to examine how the output changes.</p>
</blockquote>
<p>Now let's get a frequency of the document types.
Here I <strong>pipe</strong> <code>|</code> the output from the <code>grep</code> command to the <code>sort</code> command, in order to sort the output alphabetically.
Then I <strong>pipe</strong> the output from the <code>sort</code> command to the <code>uniq</code> command.
The <code>uniq</code> command will deduplicate the results, and the <strong>-c</strong> option will count the number of duplicates.
As a result, it will provide an overall count of the document or entry types we searched.</p>
<pre><code>grep -Eio "^@(A|B)[A-Z]*" scopus.bib | sort | uniq -c
</code></pre>
<h4 id="journal-titles"><a class="header" href="#journal-titles">Journal Titles</a></h4>
<p>We can parse the data for other information.
For example, we can get a list of journal titles by querying for the <strong>journal</strong> tag:</p>
<pre><code>grep "journal" scopus.bib
</code></pre>
<p>Even though that works, the data contains the word <strong>Journal</strong> in the name of some journals.
If we were searching thousands or more records, we might want to construct a more unique <code>grep</code> search.</p>
<p>To rectify this, we can modify our <code>grep</code> search in two ways.
First, the rows of data fields begin with a tab character.
The regular expression for the tab character is <code>\t</code>.
Therefore, we can search the file using this expression with the <strong>-P</strong> option:</p>
<pre><code>grep -P "\tjournal" scopus.bib
</code></pre>
<p>Second, we can simply add more unique terms to our <code>grep</code> search.
Since each tag includes a space, an equal sign, followed by another space, we can use that in our <code>grep</code> query:</p>
<pre><code>grep "journal =" scopus.bib
</code></pre>
<p>Using either method above, we can extract the journal title information.
Here I use two new commands, <code>cut</code> and <code>sed</code>.
The <code>cut</code> command takes the results of the <code>grep</code> command, removes the first column based on the comma as the column delimiter.
In the first <code>sed</code> command, I remove the space and opening curly brace and replace it with nothing.
In the second <code>sed</code> command, I remove the closing curly brace and the comma and replace it with nothing.
The result is list of only the journal titles without any extraneous characters.
I then pipe the output to the <code>sort</code> command, which sorts the list alphabetically, to the <code>uniq -c</code> command, which deduplicates and counts the results,
and again to the <code>sort</code> command, which sorts numerically, since the first character is a number:</p>
<pre><code>grep "journal =" scopus.bib | cut -d"=" -f2 | \
    sed 's/ {//' | sed 's/},//' | \
    sort | uniq -c | sort
</code></pre>
<h4 id="total-citations"><a class="header" href="#total-citations">Total Citations</a></h4>
<p>There are other things we can do if we want to learn more powerful technologies.
While I will not cover <code>awk</code>, I do want to introduce it to you.
With the <code>awk</code> command, based on the <em>BibTeX</em> tag that includes citation counts at the time of the download (e.g., <code>note = {Cited by: 2}</code>),
we can extract the number from that field for each record and sum the total citations for the records in the file:</p>
<pre><code> grep -o "Cited by: [0-9]*" scopus.bib | \
    awk -F":" \
    'BEGIN { printf "Total Citations: "} \
    { sum += $2; } \
    END { print sum }'
</code></pre>
<p>In the above command, we use the pipe operator to connect a series of commands to each other:</p>
<ol>
<li>use <code>grep</code> to search for the string "Cited by: " and to include any number of digits</li>
<li>use <code>awk</code> to use the colon as the column or field delimiter</li>
<li>use the <code>awk</code> BEGIN statement to print the words "Total Citations: "</li>
<li>instruct <code>awk</code> to sum the second column, which is the citation numbers</li>
<li>use the <code>awk</code> END statement to print the sum.</li>
</ol>
<blockquote>
<p>If you want to learn more about <code>sed</code> and <code>awk</code>, please see my <a href="https://cseanburns.github.io/linux_sysadmin/3e-text-processing-part-2.html">text processing chapter for my Linux Systems Administration</a>.
There are also many tutorials on the web.</p>
</blockquote>
<h2 id="conclusion-5"><a class="header" href="#conclusion-5">Conclusion</a></h2>
<p><code>grep</code> is very powerful, and there are more options listed in its <code>man</code> page.</p>
<p>The Linux (and other Unix-like OSes) command line offers a lot of utilities to examine data.
It's fun to learn and practice these.
Despite this, you do not have to become an advanced <code>grep</code> user.
For most cases, simple <code>grep</code> searches work well.</p>
<p>There are many <code>grep</code> tutorials on the web if you want to see other examples.</p>
<h2 id="references-2"><a class="header" href="#references-2">References</a></h2>
<p>Arneson, J. (2017).
Determining usage when vendors do not provide data.
<em>Serials Review, 43</em>(1), 46–50.
<a href="https://doi.org/10.1080/00987913.2017.1281788">doi.org/10.1080/00987913.2017.1281788</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="managing-software"><a class="header" href="#managing-software">Managing Software</a></h1>
<h2 id="introduction-5"><a class="header" href="#introduction-5">Introduction</a></h2>
<p>ManyLinux distributions use a <strong>package manager</strong> to handle the installation, upgrades, and uninstalls of the software on a system.
The Ubuntu distribution uses a package manager called <code>dpkg</code> and a front-end called <code>apt</code> (advanced package tool).
We will use <code>apt</code> to install, update, and remove software from our servers.</p>
<h2 id="sudo"><a class="header" href="#sudo"><code>sudo</code></a></h2>
<p>To use the package manager, we will need the <code>sudo</code> command.
The <code>sudo</code> command allows us to <q>execute a command as another user</q> (see <code>man sudo</code>).
By default, the <code>sudo</code> command <q>executes a command as the superuser</q> (see <code>man 8 sudo</code>).</p>
<p>The name of the <strong>superuser</strong> account is <strong>root</strong>.
Th root user can perform administrative tasks that regular users cannot.
For security purposes, regular accounts may not add, remove, or update software on a system,
nor may they modify most files or directories outside their home directories.
Using <code>sudo</code> allows regular users who have administrative privileges to perform maintenance tasks on our systems
by using executing commands as the root user.
Some consider this safer than logging in as the root user.</p>
<p>Not all regular users can use the <code>sudo</code> command.
On regular Ubuntu distributions, users must belong to the <strong>sudo</strong> group in order to run the <code>sudo</code> command.
The <code>groups</code> command will return a list of groups that your account belongs to.
On the Ubuntu version used by the Google Cloud Platform (GCP), your user should belong in the <strong>google-sudoers</strong> group.
The difference between the <strong>sudo</strong> group on regular Ubuntu distributions and
the <strong>google-sudoers</strong> GCP version is that regular users in the <strong>google-sudoers</strong> group are not prompted for their password.</p>
<p>Down the line, we will use the <code>sudo</code> command to modify files, create directories, and perform other maintenance tasks needed to
install and manage software.
In this lesson, we will use <code>sudo</code> along with the <code>apt</code> commands to update our systems and install software.</p>
<h3 id="sudo-syntax"><a class="header" href="#sudo-syntax"><code>sudo</code> syntax</a></h3>
<p>The <code>sudo</code> command is simple to use.
When necessary, we use <code>sudo</code> by pre-pending it to the regular commands that we have already learned.
In our home directories, for example, we don't need to use <code>sudo</code> to create a new directory with the <code>mkdir</code> command.
Instead we type something like <code>mkdir data</code> to create a new directory/folder called <strong>data</strong>.
But our regular user doesn't own the files or directories outside our home directory.
For example, when we downloaded my <code>bash</code> scripts to the <code>/usr/local/bin</code> directory,
we used <code>sudo</code> since don't own that directory.
If I want to create a <strong>data</strong> directory in <strong>/usr/local/bin</strong>, then I have to use sudo at the beginning of my command:</p>
<pre><code>cd /usr/local/bin
sudo mkdir data
</code></pre>
<p>Or, without changing to that directory, I can just specify the full path:</p>
<pre><code>sudo mkdir /usr/local/bin/data
</code></pre>
<p>Or if I want to create a file in some other directory outside my home directory, then I have to use sudo there, too:</p>
<pre><code>cd /srv
sudo touch data.csv
</code></pre>
<p>Or, without changing to that directory, I can specify the full path:</p>
<pre><code>sudo touch /srv/data.csv
</code></pre>
<h2 id="apt"><a class="header" href="#apt">apt</a></h2>
<p>We will use <code>sudo</code> in the above ways soon enough, but for now, we will use <code>sudo</code> to install, update,
and uninstall software on our systems.</p>
<p>Next I'll demonstrate the <code>apt</code> commands that we'll need.</p>
<h3 id="sudo-apt-update"><a class="header" href="#sudo-apt-update"><code>sudo apt update</code></a></h3>
<p>Your system keeps a record of what software is installed on your system and their version numbers.
The <code>sudo apt update</code> command updates that list and compares the update to what's installed.
That is, if you have a piece of software called <strong>acme1.1</strong> on your system, and <strong>acme1.2</strong> is available, then
running <code>sudo apt update</code> will let you know that you can upgrade to <strong>acme1.2</strong>.
It's good practice to run <code>sudo apt update</code> before installing or upgrading your system.
This lets your system upgrade to the most recent version of what you want to install.</p>
<p>In short, the command to download new package information is:</p>
<pre><code>sudo apt update
</code></pre>
<h3 id="sudo-apt-upgrade"><a class="header" href="#sudo-apt-upgrade"><code>sudo apt upgrade</code></a></h3>
<p>Once the list of packages have been updated, you can <strong>upgrade</strong> with the <code>sudo apt upgrade</code> command if there are any upgrades.
When you run this command, and if there are any upgrades, you will be prompted to proceed.
You can press <strong>Y</strong> to proceed, or <strong>N</strong> to cancel.</p>
<p>This command is simply:</p>
<pre><code>sudo apt upgrade
</code></pre>
<h3 id="apt-search"><a class="header" href="#apt-search"><code>apt search</code></a></h3>
<p>If you want to install a piece of software, then you have to install it using its package name.
Sometimes that means we have to search for the name of the package.
This <code>apt</code> command does not require the use of <code>sudo</code>.
<code>sudo</code> is not required because <code>apt search</code> does not modify the system.
It simply helps you search for a package name.</p>
<p>For example, the <code>man</code> pages provide helpful documentation about how to use the commands on our systems, but
the <code>man</code> pages can also be dense and not straightforward.</p>
<p>Fortunately, there's an application called <code>tldr</code>.
This is a community-driven application that provides simple help pages and examples of how to use some of the most commonly used commands.</p>
<p>To search for the <code>tldr</code> package, we execute the following command:</p>
<pre><code>apt search tldr
</code></pre>
<p>This returns a list of results that match the search query.
One of those results is the <code>tldr</code> package, which is simply named <strong>tldr</strong>.
Not all packages are simply named, which is why we need to search for the specific name.</p>
<blockquote>
<p>Note that sometimes when we search for a package, the list of results is quite long.
In those cases, pipe the above command through the <code>less</code> pager to page through the results:
<code>apt search &lt;packagename&gt; | less</code></p>
</blockquote>
<h2 id="apt-show"><a class="header" href="#apt-show"><code>apt show</code></a></h2>
<p>If we want more specific information about a package, we can use the <code>apt show</code> command along with the package name.
Therefore, to get more information about the <code>tldr</code> application, we execute the following command:</p>
<pre><code>apt show tldr
</code></pre>
<p>This will return a fuller description of the package (usually), as well as the URL to the application's website, plus other details.
We do not need to use <code>sudo</code> because we are not modifying the system.
We are only retrieving information.</p>
<h2 id="sudo-apt-install"><a class="header" href="#sudo-apt-install"><code>sudo apt install</code></a></h2>
<p>To install the <code>tldr</code> application, we use the <code>sudo apt install</code> command along with the package name.
We want to make sure that the name of the package is exactly what was returned from the <code>apt search</code> command.
In the <code>tldr</code> case, it's pretty straightforward.
To install:</p>
<pre><code>sudo apt install tldr
</code></pre>
<h2 id="sudo-apt-remove"><a class="header" href="#sudo-apt-remove"><code>sudo apt remove</code></a></h2>
<p>In order to remove a package, we use the <code>sudo apt remove</code> command.
I like to add the <code>--purge</code> option because this also removes system configuration files that I probably do not need.
That is, some applications install configuration files (configs) in the <code>/etc</code> directory.
Adding <code>--purge</code> will remove those configs.</p>
<p>To remove a package and its system configuration files (if any), we run the command with the package name:</p>
<pre><code>sudo apt --purge remove tldr
</code></pre>
<p>Some configs are stored in your home directory.
Generally only end user applications install configs in our home directories.
The <code>--purge</code> option will not remove those configs; instead, we have to remove them manually if we want.</p>
<h3 id="sudo-apt-autoremove"><a class="header" href="#sudo-apt-autoremove"><code>sudo apt autoremove</code></a></h3>
<p>One of the great things about <code>dpkg</code> and <code>apt</code> is that it installs and handles software <strong>dependencies</strong> really well.
Few computer applications are self-contained, and they often require other software to operate.
These other software are called <strong>dependencies</strong>.
When we uninstall (or remove) applications, the package manager does not auto uninstall those dependencies that were installed with it.
We use the <code>autoremove</code> command to uninstall those, which helps keep our systems clean:</p>
<pre><code>sudo apt autoremove
</code></pre>
<h3 id="sudo-apt-clean"><a class="header" href="#sudo-apt-clean"><code>sudo apt clean</code></a></h3>
<p>When we install packages, some files are installed with them.
The <code>sudo apt clean</code> removes those extra files and frees up disk space.
It's a simple command:</p>
<pre><code>sudo apt clean
</code></pre>
<h2 id="the-dpkg-command"><a class="header" href="#the-dpkg-command">The <code>dpkg</code> Command</a></h2>
<p>If you use Windows, then you are likely familiar with downloading and installing <code>.exe</code> files.
On macOS, the equivalent are <code>.dmg</code> files.
The Ubuntu distribution has an equivalent file.
These are <code>.deb</code> files.
These files can be installed using the <code>dpkg</code> command:</p>
<pre><code>sudo dpkg -i &lt;file_name.deb&gt;`
</code></pre>
<p>Like with <code>exe</code> or <code>dmg</code> files, you want to be careful installing <code>deb</code> files you find on the web.
Unlike software managed by the <code>apt</code> system, these files are not monitored and can contain malicious code.</p>
<p>You can generally use <code>apt</code> to remove applications installed with <code>dpkg</code>.
Or, you can uninstall an application installed with <code>dpkg</code> with <code>dpkg</code>:</p>
<pre><code>sudo dpkg --purge &lt;application_name&gt;
</code></pre>
<p>In most cases, stick with <code>apt</code>.</p>
<h2 id="conclusion-6"><a class="header" href="#conclusion-6">Conclusion</a></h2>
<p>The <code>apt</code> command makes it quite easy to manage software on our systems.
We will use this command to install more complicated software later.
Here's a list of commands we covered:</p>
<ul>
<li><code>sudo apt update</code></li>
<li><code>sudo apt upgrade</code></li>
<li><code>apt search</code></li>
<li><code>apt show</code></li>
<li><code>sudo apt install</code></li>
<li><code>sudo apt --purge remove</code></li>
<li><code>sudo apt autoremove</code></li>
<li><code>sudo apt clean</code></li>
</ul>
<h3 id="to-locate-and-install-software"><a class="header" href="#to-locate-and-install-software">To locate and install software</a></h3>
<pre><code>sudo apt update
apt search &lt;package_name&gt;
apt show &lt;package_name&gt;
sudo apt install &lt;package_name&gt;
</code></pre>
<h3 id="to-remove-software-and-purge-related-files"><a class="header" href="#to-remove-software-and-purge-related-files">To remove software and purge related files</a></h3>
<pre><code>sudo apt --purge remove &lt;package_name&gt;
sudo apt autoremove
sudo apt clean
</code></pre>
<h3 id="to-keep-system-up-to-date"><a class="header" href="#to-keep-system-up-to-date">To keep system up to date</a></h3>
<pre><code>sudo apt update
sudo apt upgrade
sudo apt autoremove
sudo apt clean
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="library-search"><a class="header" href="#library-search">Library Search</a></h1>
<p>In this section, we're going to explore the <code>yaz-client</code>.
The <code>yaz-client</code> is an information retrieval client that uses the Z39.50/SRU protocols to query bibliographic databases, like library catalogs and repositories.
For those unfamiliar, Z39.50 is a standard protocol in libraries for sharing, querying, and retrieving bibliographic information between library databases.
Its development in the 1970s pre-dates the web, and its continued use illustrates the evolution of information retrieval systems since the 1970s.
The protocol is maintained by the <a href="https://www.loc.gov/z3950/agency/"><em>Library of Congress</em></a>.</p>
<p>The <code>yaz-client</code> is an SRU client, as well.
SRU (Search/Retrieve via URL) and SRW (Search/Retrieve Web service) are modern internet and web-based successors to Z39.50.
These protocols offer modern flexibility and more simplicity in accessing and sharing bibliographic records than Z39.50.
See OCLC's page on <a href="https://www.oclc.org/research/areas/data-science/srw.html">SRW/U</a> for more information and The Library of Congress's documentation page: <a href="https://www.loc.gov/standards/sru/">SRU/CQL</a>.</p>
<p>The <code>yaz-client</code> allows us to interact with these protocols directly from the command line.
This provides a hands-on opportunity with the underlying mechanics of digital library searches and data retrieval.</p>
<p>However, this exploration is only partly about learning a tool.
More so, it's about understanding the history and ongoing development of information retrieval systems.
This is a crucial (and fun!) part of library and information science.</p>
<p>In order for us to use the <code>yaz-client</code>, we need to connect to a library database.
Fortunately, LSPs (library service platforms) can function as <strong>SRU</strong> targets for applications like <code>yaz-client</code>.
For example, see the <a href="https://developers.exlibrisgroup.com/alma/integrations/SRU/">ExLibris tutorial</a> on enabling and using SRU in Alma, its LSP product.
We will connect to an Alma database in the following tutorial.</p>
<h2 id="installing-yaz"><a class="header" href="#installing-yaz">Installing <code>yaz</code></a></h2>
<p>First, let's get started by installing the <code>yaz-client</code>.
Use the <code>apt</code> instructions from the prior lesson to locate the name of the <code>yaz</code> client.</p>
<p>First search for the name of the software:</p>
<pre><code>apt search yaz
</code></pre>
<p>The package name happens to be <code>yaz</code>, but you never know!
To get information about the program, we use the <code>apt show</code> command:</p>
<pre><code>apt show yaz
</code></pre>
<p>The details help confirm that this is the program we want to install.
Note that the output also returns a URL to the program's homepage on the web.
Visit that link to read more about the software and its documentation.</p>
<p>To install <code>yaz</code>, run the following command:</p>
<pre><code>sudo apt install yaz
</code></pre>
<h2 id="documentation"><a class="header" href="#documentation">Documentation</a></h2>
<p>The documentation for the <code>yaz-client</code> can be accessed via its manual page or on the web.
To access the man page, see:</p>
<pre><code>man yaz-client
</code></pre>
<p><code>yaz</code> is able to search quite a few bibliographic attributes, including many <strong>metadata</strong> fields.
To see which attributes are available to <code>yaz</code>, see:</p>
<pre><code>man bib1-attr
</code></pre>
<p>The Library of Congress also provides an overview of the <strong>bib1-attr</strong> documentation,
but it's less comprehensive:
<a href="https://www.loc.gov/z3950/agency/defns/bib1.html">Bib-1 Attribute Set</a></p>
<p>Complete documentation for the <code>yaz-client</code> can be found on its homepage:
<a href="https://www.indexdata.com/resources/software/yaz/">YAZ</a></p>
<h2 id="using-yaz"><a class="header" href="#using-yaz">Using <code>yaz</code></a></h2>
<p>The start the <code>yaz</code> program, run the <code>yaz-client</code> command.</p>
<pre><code>yaz-client
</code></pre>
<p>This creates a new command line interface with a new prompt:</p>
<pre><code>Z&gt;
</code></pre>
<p>In this new interface, we can connect to a library's OPAC or discovery service.
To do so, we use the <code>open</code> command followed by the server address.
The following <code>open</code> command establishes a connection to the University of Kentucky's library catalog:</p>
<pre><code>Z&gt; open saalck-uky.alma.exlibrisgroup.com:1921/01SAA_UKY
</code></pre>
<h2 id="queries"><a class="header" href="#queries">Queries</a></h2>
<p>Queries are constructed using Prefix Query Notation (PQN).
In the context of PQN, this is a way of structuring queries where the operator (e.g., AND, NOT, OR)
precedes the operands (e.g., search terms, attributes, fields).</p>
<p>Each query begins with a <em>command</em> followed by a search syntax articulated by its PQN.
The list of commands are described in <code>man yaz-client</code> in the COMMANDS section.
The main command we'll use is the <code>find</code> command, which may be abbreviated as <code>f</code>.
Let's see some examples:</p>
<h3 id="example-1"><a class="header" href="#example-1">Example 1</a></h3>
<p>To search for the term <strong>information</strong> in the <em>title</em> field
and the term <strong>library science</strong> in the <em>Library of Congress Subject Heading</em> (LCSH) field,
we use the following query:</p>
<pre><code>Z&gt; find @and @attr 1=4 "information" @attr 1=21 "library science"
</code></pre>
<p>Let's break that down:</p>
<ul>
<li><code>find</code> is the command that sends a search request</li>
<li><code>@and</code> is the operator signifying a Boolean AND search of the next two attributes</li>
<li><code>@attr 1=4</code> instructs the query to search for the term in the Title field</li>
<li><code>"information"</code> is the term for the Title search</li>
<li><code>@attr 1=21</code> instructs the query to search for the term in the subject heading field</li>
<li><code>"library science"</code> is the second search term for the subject heading search</li>
</ul>
<p>The search does not reveal the results.
To peruse the results, we use the <code>show</code> command.
To show the first record:</p>
<pre><code>show 1
</code></pre>
<p>To show the second record:</p>
<pre><code>show 2
</code></pre>
<p>And so forth.</p>
<h3 id="example-2"><a class="header" href="#example-2">Example 2</a></h3>
<p>Search for works with subject headings <em>library science</em> and <em>philosophy</em>.
In this example, I abbreviate the <code>find</code> command as <code>f</code>:</p>
<pre><code>Z&gt; f @and @attr 1=21 "library science" @attr 1=21 "philosophy"
</code></pre>
<ul>
<li><code>@attr 1=21</code> instructs the query to search for the term <em>library science</em> in the subject heading field</li>
<li><code>@attr 1=21</code> instructs the query to search for the term <em>philosophy</em> in the subject heading field</li>
</ul>
<h3 id="example-3"><a class="header" href="#example-3">Example 3</a></h3>
<p>Find where personal name is "mcmurtry, larry".</p>
<pre><code>Z&gt; f @attr 1=1 "mcmurtry, larry"
</code></pre>
<ul>
<li><code>@attr 1=1</code> instructs the query to search for the term <em>mcmurtry, larry</em> in the personal name field.</li>
</ul>
<h3 id="example-4"><a class="header" href="#example-4">Example 4</a></h3>
<p>Find where the term "c programming language" appears in the <strong>Any</strong> field.</p>
<pre><code>Z&gt; f @attr 1=1016 "c programming language"
</code></pre>
<ul>
<li><code>@attr 1=1016</code> instructs the query to search for the term in <strong>Any</strong> field.</li>
</ul>
<p>Finally, we can exit the <code>yaz</code> client with the <code>quit</code> command:</p>
<pre><code>Z&gt; quit
</code></pre>
<h2 id="advanced-usage"><a class="header" href="#advanced-usage">Advanced Usage</a></h2>
<p>Let's open the <code>yaz-client</code> again but with the <code>-m</code> option.
According to the <code>yaz-client</code> man page, the <code>-m</code> option
option instructs the client to append bibliographic records to a file.
In the example below, I arbitrarily name the file <code>records.marc</code>.</p>
<pre><code>$ yaz-client -m records.marc
</code></pre>
<p>Again, we use the <code>open</code> command to connect to the library's catalog.
Then use the <code>find</code> command to search the catalog.
Use the <code>show</code> command to examine some of the retrieved records.
Then use the <code>quit</code> command to exit the <code>yaz-client</code>.</p>
<pre><code>Z&gt; open saalck-uky.alma.exlibrisgroup.com:1921/01SAA_UKY
Z&gt; find @and @attr 1=4 "information" @attr 1=21 "library science"
Z&gt; show 1
Z&gt; show 2
Z&gt; show 3
Z&gt; quit
</code></pre>
<p>However, this time when we exit the <code>yaz-client</code>, we can examine all the records we retrieved.
The default file type isn't human friendly.
We can take a look at the first few lines of the file first:</p>
<pre><code>head records.marc
</code></pre>
<p>Then we can use the <code>file</code> command to determine its file type:</p>
<pre><code>file records.marc
records.marc: MARC21 Bibliographic
</code></pre>
<p>Fortunately, we can convert the MARC file to friendlier formats.
For example, using the <code>yaz-marcdump</code> command, we can convert the file to JSON,
which is a <q>standard text-based format for representing structured data</q>
(<a href="https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/Scripting/JSON">JSON</a>).</p>
<pre><code>yaz-marcdump -o json records.marc &gt; records.json
</code></pre>
<p>We then use the <code>jq</code> command, a JSON processor, to format the JSON for better readability:</p>
<pre><code>jq . records.json &gt; records-formatted.json
</code></pre>
<p>With the records formatted, we can use the <code>less</code> command to scan the file, but
the <code>jq</code> command is quite powerful and we can use it to query and examine specific fields in the JSON-formatted MARC records.</p>
<blockquote>
<p>Note: learning <code>jq</code> and MARC is beyond the scope of this work.
However, if you are new to MARC or need a reminder, see:
<a href="https://www.loc.gov/marc/bibliographic/">MARC 21 Format for Bibliographic Data</a>.
The <code>jq</code> homepage also provides a nice tutorial: <a href="https://jqlang.github.io/jq/tutorial/">jq Tutorial</a>.</p>
</blockquote>
<p>But as an example,
the following command extracts the <strong>650 Subject</strong> field with the <strong>a</strong> (Topical term) subfields for our entries:</p>
<pre><code>jq '.fields[] | select(has("650")) | .["650"].subfields[] | select(has("a")) | .a' records-formatted.json
</code></pre>
<p>Or we can examine general subdivisions (<code>$x</code> subfield) of the 650 subfields and tabulate the data
by piping through <code>sort</code>, <code>uniq -c</code>, and <code>sort</code>:</p>
<pre><code>jq '.fields[] | select(has("650")) | .["650"].subfields[] | select(has("x")) | .x' records-formatted.json | sort | uniq -c | sort
</code></pre>
<p>For other fields to examine, see the <a href="https://www.loc.gov/marc/umb/um07to10.html">MARC 21 Reference Materials sheet</a>.</p>
<h3 id="jq-breakdown"><a class="header" href="#jq-breakdown"><code>jq</code> breakdown</a></h3>
<p>Selects all fields:</p>
<pre><code>jq '.fields[]' records-formatted.json
</code></pre>
<p>Selects all <strong>650</strong> fields:</p>
<pre><code>jq '.fields[] | select(has("650"))' records-formatted.json
</code></pre>
<p>Selects only the subfields from the <strong>650</strong> fields:</p>
<pre><code>jq '.fields[] | select(has("650")) | .["650"].subfields[]' records-formatted.json
</code></pre>
<p>Selects only the <strong>x</strong> subfields from the <strong>650</strong> fields:</p>
<pre><code>jq '.fields[] | select(has("650")) | .["650"].subfields[] | select(has("x")) | .x' records-formatted.json
</code></pre>
<p>Selects only the <strong>z</strong> subfields (Geographic subdivision) from the <strong>650</strong> fields:</p>
<pre><code>jq '.fields[] | select(has("650")) | .["650"].subfields[] | select(has("z")) | .z' records-formatted.json
</code></pre>
<h3 id="other-formats"><a class="header" href="#other-formats">Other formats</a></h3>
<p>We can the original MARC data to XML:</p>
<pre><code>yaz-marcdump -o marcxml records.marc &gt; records.xml
</code></pre>
<p>We can query the XML data with <code>xmlstarlet</code> command, which is similar to <code>jq</code> but for XML structured data.</p>
<h3 id="downloading-all-results"><a class="header" href="#downloading-all-results">Downloading All Results</a></h3>
<p>The process above saved only records we examined with the <code>show</code> command.
The following <code>find</code> query locates 120 records and then the <code>show</code> command below allows us to save to file all 120 records.</p>
<pre><code>$ yaz-client
&gt; set_marcdump records.new
&gt; open saalck-uky.alma.exlibrisgroup.com:1921/01SAA_UKY
&gt; find @and @attr 1=4 "technology" @attr 1=21 "library science"
&gt; show 1 +120
&gt; quit
</code></pre>
<p>Then we can follow the steps above to convert to JSON and examine the file with <code>jq</code>.
One thing we learn with bigger data sets is that data gets messy.
In the 120 records, I found differences in capitalization, usage of punctuation, and other variations that are mistakes.
The following command helps to clean some of that up.
The command is technically a one-liner, but I've broken it up on multiple lines by including a backslash at the end <code>\</code>.</p>
<pre><code>jq '.fields[] | select(has("650")) | .["650"].subfields[] | select(has("a")) | .a' records-formatted.json |\
sort | \
sed 's/\.//g' | \
awk '{ print tolower($0) }' | \
sort | \
uniq -c | \
sort -n
</code></pre>
<p>In the following, I add a final <code>sed</code> and <code>awk</code> command on the last two lines.
The final <code>sed</code> command deletes the most common subject heading, which is <strong>library science</strong>.
Since these are all <strong>library science</strong> records, including it in the results is meaningless.
The final <code>awk</code> command sums the number of records from the tabulated count.</p>
<pre><code>jq '.fields[] | select(has("650")) | .["650"].subfields[] | select(has("a")) | .a' records-formatted.json |\
sort | \
sed 's/\.//g' | \
awk '{ print tolower($0) }' | \
sort | \
uniq -c | \
sort -n | \
sed '$d' | \
awk '{ sum+=$1 } END{print sum}'
</code></pre>
<p>Because these commands are query agnostic, they can be used to examine subject headings in the catalog from other queries.
We can even select for other subfields, like the <code>z</code> <strong>650</strong> subfield to get the geographical divisions and
use that to map out the geographies reported in the subject headings in a catalog.</p>
<h2 id="conclusion-7"><a class="header" href="#conclusion-7">Conclusion</a></h2>
<p>Z39.50 is often presented as an abstract information retrieval concept even though it has played a central
part of searching online catalogs and database for nearly 50 years.
The protocol, using tools like <code>yaz</code>, can be used to build search interfaces to bibliographic data.
For example, see:</p>
<ul>
<li><a href="https://reintech.io/blog/guide-to-php-yaz-library-information-retrieval">A Guide to the PHP YAZ Library for Information Retrieval</a></li>
<li><a href="https://sites.nd.edu/emorgan/2013/11/fun/">Fun with bibliographic indexes, bibliographic data management software, and Z39.50</a>
If you are interested in establishing a connection to the Library of Congress's catalog,
use the following server address:</li>
</ul>
<pre><code>Z&gt; open z3950.loc.gov:7090/voyager
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="creating-a-lamp-server"><a class="header" href="#creating-a-lamp-server">Creating a LAMP Server</a></h1>
<p>In this section,
we learn how to set up a LAMP
(Linux, Apache, MySQL, PHP) stack.
This stack enables us to create a web server
that provides extra funtionality via PHP and MySQL,
both of which are required to run
content management systems and
integrated library systems.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installing-the-apache-web-server"><a class="header" href="#installing-the-apache-web-server">Installing the Apache Web Server</a></h1>
<h2 id="introduction-6"><a class="header" href="#introduction-6">Introduction</a></h2>
<p>In this section, we focus on a fundamental component of the internet's infrastructure:
the web server, or alternatively called the HTTP server.</p>
<p>The web server is the software that makes websites available in your browsers.
The basic function is to make files on the web server accessible to others via their web browsers.
At a basic level, the web is essentially a world wide file system, and
the web browser retrieves and displays files from web servers, much like a file explore does for local files.
At a more advanced level, HTTP can also add more complexity beyond simple file access,
such as providing dynamic content, APIs, and more.</p>
<p>Knowing how a web server functions is crucial for anyone wanting to manage or deploy web services.
There are many <a href="https://en.wikipedia.org/wiki/Comparison_of_web_server_software">web servers available</a>, but in this session,
I will guide you through installing and using the <a href="https://httpd.apache.org/">Apache</a> web server.
Apache is one of the most popular web server applications.
We will learn how to install it, configure it, and conduct basic checks to ensure its operation.
We will end this lesson by creating your first web page on your first web server.</p>
<p>It's important to understand the basics of an HTTP server,
and therefore I ask you to read Apache's <a href="https://httpd.apache.org/docs/2.4/getting-started.html">Getting Started</a>
page before proceeding with the rest of this section.
Each of the main sections on that page describe the important elements
that make up and serve a website.
These elements include:</p>
<ul>
<li>clients, servers, and URLs</li>
<li>hostnames and DNS</li>
<li>configuration files and directives</li>
<li>web site content</li>
<li>log files and troubleshooting</li>
</ul>
<h2 id="installation-1"><a class="header" href="#installation-1">Installation</a></h2>
<p>Before we install Apache, we need to update our systems first.
This ensures we will be downloading and installing the most recent, secure version of the package.</p>
<pre><code>sudo apt update
sudo apt -y upgrade
</code></pre>
<p>Once the machine is updated, we can install Apache using <code>apt</code>.
First we'll use <code>apt search</code> to identify the specific package name.
I know that a lot of results will be returned, so I will <strong>pipe</strong> <code>|</code> the output from <code>apt search</code> command
through the <code>head</code> command to look at the initial results:</p>
<pre><code>apt search apache2 | head
</code></pre>
<p>The package that we're interested in happens to be named <code>apache2</code> on Ubuntu.
The name of this package is not a given, though.
On other distributions, like Fedora, the Apache package is called <code>httpd</code>.
This is why it's important to learn and use <code>apt search &lt;package_name&gt;</code> and <code>apt show &lt;package_name&gt;</code> commands
to locate desired packages before installing.</p>
<pre><code>apt show apache2
</code></pre>
<p>Once we've confirmed that <code>apache2</code> is the package that we want,
install it with the <code>apt install &lt;package_name&gt;</code> command.
Press <strong>Y</strong> to agree to continue after running the command below:</p>
<pre><code>sudo apt install apache2
</code></pre>
<h2 id="basic-checks"><a class="header" href="#basic-checks">Basic checks</a></h2>
<p>Once it is installed, we need to make sure the server is up and running,
configure some basic things, and create a basic web site.</p>
<p>To start, I use the <code>systemctl</code> command to acquire status info about <code>apache2</code> and
make sure it is <em>enabled</em> and <em>running</em>:</p>
<pre><code>systemctl status apache2
</code></pre>
<p>The output may be overwhelming at first glance, so I advise you to read each line slowly.
In particular, look for key lines that show its <strong>Active</strong> and <strong>Loaded</strong> status.
For example, the output shows that <code>apache2</code> is <strong>enabled</strong>, which is the default for this software.
The term <strong>enabled</strong> means that the software starts automatically on reboot.
The output should also state that the software is <strong>active</strong>.
This means that the <code>apache2</code> is running and live.</p>
<h2 id="creating-a-web-page"><a class="header" href="#creating-a-web-page">Creating a web page</a></h2>
<p>Since <code>apache2</code> is active, let's look at the default web page.</p>
<p>There are (at least) two ways we can look at the default web page.
We can use a command line web browser or a graphical web browser, like Firefox, Chrome, etc.</p>
<h3 id="text-based-web-browser"><a class="header" href="#text-based-web-browser">Text Based Web Browser</a></h3>
<p>We have lots of command line browsers to use.
I like <code>w3m</code> because it defaults to Vim keybindings, but many like <code>elinks</code>.</p>
<p>To check the site with <code>w3m</code>, we have to install it first:</p>
<pre><code>sudo apt install w3m
</code></pre>
<p>Or if you want to try <code>elinks</code>, run:</p>
<pre><code>sudo apt install elinks
</code></pre>
<p>Once the text based browser is installed, we can visit our default site using its loopback IP address.
The loopback address is named <code>localhost</code> and always points to the local machine.
It is useful for testing services, connections, and more locally.
From the command line on our server, we can run either of these two commands to view localhost:</p>
<pre><code>w3m 127.0.0.1
</code></pre>
<p>Or:</p>
<pre><code>w3m localhost
</code></pre>
<blockquote>
<p>If you elected to use <code>elinks</code>, just replace <code>w3m</code> with it.</p>
</blockquote>
<p>We can also acquire the system's <a href="https://en.wikipedia.org/wiki/Private_network">private IP address</a> using the <code>ip a</code> command.
There are different address ranges for the private networks.
On your home network, your private IP address for a your laptop or phone might begin with <code>192.168.x.x</code>.
On our virtual instances, the address will begin with the number <strong>10</strong> and look like <code>10.128.0.99</code>.
The difference deals with the size of the private networks.
In any case, to use the private IP address with <code>w3m</code> from the virtual machine's command line, we run,
assuming private IP address for my local machine is <code>10.128.0.99</code>:</p>
<pre><code>w3m 10.128.0.99
</code></pre>
<p>If <code>apache2</code> installed and started correctly, you should see the following text:</p>
<p><strong>Apache2 Ubuntu Default Page</strong><br />
<strong>It works!</strong></p>
<p>To exit <code>w3m</code> or <code>elinks</code>, press <strong>q</strong> and then <strong>y</strong> to confirm exit.</p>
<h3 id="graphical-browser"><a class="header" href="#graphical-browser">Graphical Browser</a></h3>
<p>To view the default web page using a regular web browser,
like Firefox, Chrome, Safari, Edge, or etc., you need to get your server's
<a href="https://en.wikipedia.org/wiki/IP_address">public IP address</a>.
To do that, log into the <a href="https://console.cloud.google.com/">Google Cloud Console</a>
click on the <strong>Compute Engine</strong> link, and then click on <strong>VM instances</strong>.
You should see your <strong>External IP</strong> address in the table on that page.
You can copy that external IP address or simply click on it to open it in a new browser tab.
If successful, you should see the graphical version of the <strong>Apache2 Ubuntu Default Page</strong>.</p>
<blockquote>
<p>Note that most browsers nowadays try to force a secure HTTPS mode.
If your web page is not loading, make sure your URL is <strong>http://IP-ADDRESS</strong>
and not <strong>https://IP-ADDRESS</strong>.</p>
</blockquote>
<p>Please take a moment to read through the text on the default page.
It provides important information about where Ubuntu stores configuration files,
what those files do, and the document root, which is where website files are stored.</p>
<h2 id="create-a-web-page"><a class="header" href="#create-a-web-page">Create a Web Page</a></h2>
<p>Let's create your first web page on your first web server.
The default page described above provides the location of the <strong>document root</strong> at <code>/var/www/html</code>.
The <strong>document root</strong> may reside at a different location on a different Linux operating system,
so it's important to verify that location.
Remember that the web is, at its simplest, a filesystem that has been made available to the wide world.
The web server is what provides access to part of the filesystem.
That point of access is called the <strong>document root</strong>.</p>
<p>When we navigate to the document root on the command line,
we'll see that there is an <code>index.html</code> file located in that directory.
This is the <strong>Apache2 Ubuntu Default Page</strong> that we visited above in our browsers.
Most web servers look for a file specifically named <code>index.html</code> and serve that as the default.
Let's rename that <code>index.html</code> file, in order to back it up, and create a new one:</p>
<pre><code>cd /var/www/html/
sudo mv index.html index.html.original
sudo nano index.html
</code></pre>
<blockquote>
<p>Note: we use <code>sudo</code> in this directory
because we are working on files and directories outside our home directories.
Thus, be careful here about the commands you run.
Any mistake may result in deleting necessary files or directories.</p>
</blockquote>
<p>If you know HTML, then feel free to write some basic HTML code to get started.
Otherwise, you can re-type the content below in <code>nano</code> or like, and then save and exit out.</p>
<pre><code>&lt;html&gt;
&lt;head&gt;
&lt;title&gt;My first web page using Apache&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;

&lt;h1&gt;Welcome&lt;/h1&gt;

&lt;p&gt;Welcome to my web site.
I created this site using the Apache HTTP server.&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>If you have your site open in your web browser, reload the page, and you should see the new text.</p>
<p>You can still view the original default page by specifying its name in the URL.
Remember that web browsers are, at their most basic, simply file viewers.
So it makes sense that you simply have to specify the name of the file you want to view.
For example, if your <strong>public IP address</strong> is <code>55.222.55.222</code>, then you'd specify it like so:</p>
<pre><code>http://55.222.55.222/index.html.original
</code></pre>
<h2 id="conclusion-8"><a class="header" href="#conclusion-8">Conclusion</a></h2>
<p>In this section, we learned about the Apache HTTP server.
We learned how to install it on Ubuntu, how to use a <code>systemctl</code> command to check its status,
how to create a basic web page in <code>/var/www/html</code>,
how to view that web page using the <code>w3m</code> command line browser and in our graphical browser,</p>
<p>In the next section, we will install PHP,
which will provide the language needed to connect to the relational database MySQL.
This will enable more data driven web sites and begin to transform out sites
from basic file viewers to full fledged applications.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installing-and-configuring-php"><a class="header" href="#installing-and-configuring-php">Installing and Configuring PHP</a></h1>
<h2 id="introduction-7"><a class="header" href="#introduction-7">Introduction</a></h2>
<p>Client-side programming languages, like JavaScript, are handled entirely by the browser.
To do this, browsers like Firefox, Chrome, Safari, Edge, and others include <a href="https://en.wikipedia.org/wiki/JavaScript_engine">JavaScript engines</a> that use
just-in-time compilers to execute JavaScript code (see <a href="https://blog.mozilla.org/javascript/">JavaScript Engine, Mozilla</a>).</p>
<p>From an end user's perspective, you basically install JavaScript when you install a web browser.</p>
<p><a href="https://www.php.net/">PHP</a>, on the other hand, is a server-side programming language.
This means it must be installed on the server.
Unlike with JavaScript, the browser does not execute PHP directly;
instead, the web server processes the PHP and sends the resulting HTML or other content to the browser.</p>
<p>From a system or web administrator's perspective,
this means that PHP has be installed and configured to work with the web/HTTP server.
In our case, we have to install and configure PHP on our virtual instances to work with the Apache web server software.</p>
<p>One of the primary uses of PHP is to interact with databases, like MySQL, MariaDB, PostgreSQL, etc.,
in order to create data-based page content.
To begin to set this up, we have to:</p>
<ol>
<li>Install PHP and relevant Apache modules</li>
<li>Configure PHP and relevant modules to work with Apache</li>
<li>Configure PHP and relevant modules to work with MySQL</li>
</ol>
<h2 id="install-php"><a class="header" href="#install-php">Install PHP</a></h2>
<p>As usual, we will use <code>apt install</code> to install PHP and relevant modules.
Then we will restart Apache using the <code>systemctl</code> command.
Use <code>apt show [package_name]</code> to read more about each package we will install.
The first command below installs the <strong>php</strong> and the <strong>libapache2-mod-php</strong> packages.
The latter package is used to create a connection between PHP and the Apache web server.</p>
<pre><code>sudo apt install php libapache2-mod-php
sudo systemctl restart apache2
</code></pre>
<p>Once installed, you want to confirm the installed version with the following command.
This is because other software (e.g., WordPress, etc.) might require a specific version before that other software to work.</p>
<pre><code>php -v
</code></pre>
<p>After we restart Apache, we need to check its status and see if there are any errors in the log output:</p>
<pre><code>systemctl status apache2
</code></pre>
<h2 id="check-install"><a class="header" href="#check-install">Check Install</a></h2>
<p>Next we check that PHP has been installed and that it's working with Apache.
We can create a small PHP file in our web document root.
To do that, we <code>cd</code> to the document root, <code>/var/www/html/</code>, and create a file called <strong>info.php</strong>:</p>
<pre><code>cd /var/www/html/
sudo nano info.php
</code></pre>
<p>In that file, add the following text, then save and close the file:</p>
<pre><code>&lt;?php
phpinfo();
?&gt;
</code></pre>
<p>Now visit that file using the public IP address for your server.
If the public IP address for my virtual machine is <code>55.333.55.33</code>, then in Firefox, Chrome, etc, I would open:</p>
<pre><code>http://55.333.55.333/info.php
</code></pre>
<blockquote>
<p>Again, be sure to replace the IP below with the IP address of your server and
be sure to use <strong>http</strong> and not <strong>https</strong>.</p>
</blockquote>
<p>You should see a page that provides system information about PHP, Apache, and the server.
The top of the page should look like Figure 1 below:</p>
<figure>
<img src="images/4b-phpinstall.png"
alt="PHP install page"
title="PHP install page">
<figcaption>
Fig. 1. A screenshot of the title of the PHP install page.
</figcaption>
</figure>
<p>Once you've confirmed that PHP is installed and functioning,
you should delete it since it exposes detailed systems information:</p>
<pre><code>sudo rm /var/www/html/info.php
</code></pre>
<h2 id="basic-configurations"><a class="header" href="#basic-configurations">Basic Configurations</a></h2>
<p>By default, when Apache serves a web page, it looks for a <a href="https://httpd.apache.org/docs/current/mod/mod_dir.html">file titled <code>index.html</code></a> and serves that,
even if it does not display that file in the URL bar.
Thus, <code>http://example.com/</code> actually resolves to <code>http://example.com/index.html</code> in such cases.
However, if our plan is to provide for PHP,
we want Apache to default to a file titled <code>index.php</code> instead of <code>index.html</code> file.
In these cases, <code>http://example.com/</code> would actually resolves to <code>http://example.com/index.php</code>.</p>
<p>To configure that, we need to edit the <code>dir.conf</code> file in the <code>/etc/apache2/mods-enabled/</code> directory.
In that file there is a line that starts with <code>DirectoryIndex</code> followed by a list of files.
The first file listed in that line is <code>index.html</code>,
and then there are a series of other files that Apache looks for in the order listed.
Apache checks that list list and prioritizes these in order of appearance on the list.
If any of those files exist in the document root, then Apache serves those before proceeding to the next.
We want Apache to prioritize the <code>index.php</code> file first and <code>index.html</code> second.
Before modifying this file, it's good practice to create a backup of the original.
So we will use the <code>cp</code> command to create a copy with a new name, and then we will use <code>nano</code> to edit the file.</p>
<pre><code>cd /etc/apache2/mods-enabled/
sudo cp dir.conf dir.conf.bak
sudo nano dir.conf
</code></pre>
<p>Next we change the line to this, so that <code>index.php</code> is first in line:</p>
<pre><code>DirectoryIndex index.php index.html index.cgi index.pl index.xhtml index.htm
</code></pre>
<p>Whenever we make a configuration change, we should use the <code>apachectl</code> command to check our configuration:</p>
<pre><code>apachectl configtest
</code></pre>
<p>If we get a <code>Syntax Ok</code> message, we can reload the Apache configuration, restart the service, and check its status:</p>
<pre><code>sudo systemctl reload apache2
sudo systemctl restart apache2
systemctl status apache2
</code></pre>
<h2 id="create-an-indexphp-file"><a class="header" href="#create-an-indexphp-file">Create an index.php File</a></h2>
<p>Now create a basic PHP page.
<code>cd</code> back to the Apache document root directory and use <code>nano</code> to create and open an <code>index.php</code> file:</p>
<pre><code>cd /var/www/html/
sudo nano index.php
</code></pre>
<p>Let's add some HTML and PHP to it.
We will add PHP that functions as a simple <a href="https://stackoverflow.com/questions/8754080/how-to-get-exact-browser-name-and-version">browser detector</a>.
Add the following code:</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
&lt;head&gt;
    &lt;meta charset="UTF-8"&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
    &lt;title&gt;Browser Detector&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;h1&gt;Browser &amp; OS Detection&lt;/h1&gt;
    &lt;p&gt;You are using the following browser to view this site:&lt;/p&gt;

    &lt;?php
    $user_agent = $_SERVER['HTTP_USER_AGENT'];

    // Browser Detection
    if (stripos($user_agent, 'Edge') !== false) {
        $browser = 'Microsoft Edge';
    } elseif (stripos($user_agent, 'Firefox') !== false) {
        $browser = 'Mozilla Firefox';
    } elseif (stripos($user_agent, 'Chrome') !== false &amp;&amp; stripos($user_agent, 'Chromium') === false) {
        $browser = 'Google Chrome';
    } elseif (stripos($user_agent, 'Chromium') !== false) {
        $browser = 'Chromium';
    } elseif (stripos($user_agent, 'Opera Mini') !== false) {
        $browser = 'Opera Mini';
    } elseif (stripos($user_agent, 'Opera') !== false || stripos($user_agent, 'OPR') !== false) {
        $browser = 'Opera';
    } elseif (stripos($user_agent, 'Safari') !== false &amp;&amp; stripos($user_agent, 'Chrome') === false) {
        $browser = 'Safari';
    } else {
        $browser = 'Unknown Browser';
    }

    // OS Detection
    if (stripos($user_agent, 'Windows') !== false) {
        $os = 'Windows';
    } elseif (stripos($user_agent, 'Mac') !== false || stripos($user_agent, 'Macintosh') !== false) {
        $os = 'Mac';
    } elseif (stripos($user_agent, 'Linux') !== false) {
        $os = 'Linux';
    } elseif (stripos($user_agent, 'iOS') !== false || stripos($user_agent, 'iPhone') !== false || stripos($user_agent, 'iPad') !== false) {
        $os = 'iOS';
    } elseif (stripos($user_agent, 'Android') !== false) {
        $os = 'Android';
    } else {
        $os = 'Unknown OS';
    }

    // Output Result
    echo "&lt;p&gt;Your browser is &lt;strong&gt;$browser&lt;/strong&gt; and your operating system is &lt;strong&gt;$os&lt;/strong&gt;.&lt;/p&gt;";
    ?&gt;

&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>Next, save the file and exit <code>nano</code>.
In your browser, visit your site at its public IP address (again, replace your server's IP address):</p>
<pre><code>http://55.333.55.333/
</code></pre>
<p>Although your <code>index.html</code> file still exists in your document root, Apache now returns the <code>index.php</code> file instead.
If for some reason PHP fails, then the <code>index.html</code> file would be served next
since that's what is listed next in the <code>dir.conf</code> file on the <code>DirectoryIndex</code> line.</p>
<h2 id="conclusion-9"><a class="header" href="#conclusion-9">Conclusion</a></h2>
<p>In this section, we installed PHP and configured it to work with Apache.
We also created a simple PHP test page that reported our browser user agent information on our website.</p>
<p>In the next section, we'll learn how to complete the LAMP stack by adding the MySQL relational database to our setup.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installing-and-configuring-mysql"><a class="header" href="#installing-and-configuring-mysql">Installing and Configuring MySQL</a></h1>
<h2 id="introduction-8"><a class="header" href="#introduction-8">Introduction</a></h2>
<p>We started our LAMP stack when we installed Apache on Linux.
We added extra functionality when we installed and configured PHP to work with Apache.
In this section, our objective is to complete the LAMP stack and install and configure <a href="https://en.wikipedia.org/wiki/MySQL">MySQL</a>.</p>
<p>If you need a refresher on relational databases, see:
<a href="https://mariadb.com/kb/en/introduction-to-relational-databases/">Introduction to Relational Databases</a>.
However in the next section, we will explore the database basics from the command line.</p>
<h2 id="install-and-set-up-mysql"><a class="header" href="#install-and-set-up-mysql">Install and Set Up MySQL</a></h2>
<p>In this section, we'll learn how to install, setup, secure, and
configure the MySQL relational database so that it works with the Apache web server and the PHP programming language.</p>
<p>First, as normal, we make run the following commands to ensure our machines are fully updated:</p>
<pre><code>sudo apt update
sudo apt upgrade
sudo apt autoremove
sudo apt clean
</code></pre>
<blockquote>
<p>Note that sometimes you will have to reboot your machine, mostly when you get a Linux kernel update.
To do so, run <code>sudo reboot now</code>.
This command will disconnect you from your machine.
Wait a minute or two, and then reconnect.</p>
</blockquote>
<p>Then we install the MySQL Community Server package.
The MySQL Community Server package is a <strong>metapackage</strong> that installs the latest, most secure version of MySQL,
regardless of the software's version number, as well as its dependencies.</p>
<pre><code>sudo apt install mysql-server
</code></pre>
<p>The above install should be fine for us, but note that in some cases you first may want to specifically
confirm which versions are available via the <code>apt</code> command since some software (e.g., WordPress)
may require specific versions or above.
You can check that with the following:</p>
<pre><code>apt policy mysql-server
</code></pre>
<p>After installing, you can confirm the version number with the following command
to ensure you know which version you are using:</p>
<pre><code>mysql --version
</code></pre>
<p>The install process should start and enable the database server,
but we can check if it's running and enabled using the <code>systemctl</code> command.
Note that you are looking for the lines beginning with <code>Loaded</code> and <code>Active</code>.</p>
<pre><code>systemctl status mysql 
</code></pre>
<p>Next we need to run a post installation script called <code>mysql_secure_installation</code>.
The script performs some security checks and creates a secure, baseline configuration of MySQL.
To do that, run the following command:</p>
<pre><code>sudo mysql_secure_installation
</code></pre>
<p>When you run the above script, you'll get a series of prompts to respond to.
For most responses, you will want to respond with a <strong>Y</strong> for yes.
We will respond with a <strong>Y</strong> to the first question on validating passwords, but to keep things simple,
select <strong>LOW</strong> when prompted for the password validation policy.
This will enforce a weak password policy for testing, but note that in real-world scenarios,
we might want to select a more secure policy.
In the output below, I show how to respond to each question:</p>
<pre><code>Validate passwords: Y
Password validation policy: 0 (zero) for LOW
Remove anonymous users: Y
Disallow root login remotely: Y
Remove test database and access to it: Y
Reload privilege tables now: Y
</code></pre>
<p>We can login to the database now.
In order to do so, we use the following command:</p>
<pre><code>sudo mysql -u root
</code></pre>
<blockquote>
<p><strong>NOTE:</strong> we need to distinguish between the regular user prompt of our Linux accounts and the MySQL prompt below.
The default prompt for our user accounts has the following syntax: <code>user_name@computer_name:path$ </code>.
In the following, I will indicate we are at the MySQL prompt with the following text: <code>mysql&gt;</code>.
Do not type that prompt when you are using MySQL.</p>
</blockquote>
<p>When I connect to a MySQL server, I like to list the available databases.
To do this, we use the <code>show databases;</code> command.
Note that MySQL commands end with a semicolon:</p>
<pre><code>mysql&gt; show databases;
</code></pre>
<p>The following databases should be returned:</p>
<pre><code>+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
| sys                |
+--------------------+
4 rows in set (0.01 sec)
</code></pre>
<p>To exit the MySQL server prompt and return to the Bash shell, we use the following command:</p>
<pre><code>mysql&gt; \q
</code></pre>
<h2 id="create-and-set-up-a-regular-user-account"><a class="header" href="#create-and-set-up-a-regular-user-account">Create and Set Up a Regular User Account</a></h2>
<p>We need to reserve the <strong>root MySQL user</strong> for special administrative cases
and create a <strong>regular MySQL user</strong> account for regular use cases.</p>
<p>To create a regular MySQL user, we use the MySQL <code>create</code> command.
In the command below, I create a new user called <code>opacuser</code> with a complex password.
The single quotes are quoting the password and are not the password itself.
Thus in the example below, the <strong>Xs</strong> indicate my password.</p>
<p>First, log back into the MySQL server:</p>
<pre><code>sudo mysql -u root
</code></pre>
<p>At the MySQL prompt, create the new user:</p>
<pre><code>mysql&gt; create user 'opacuser'@'localhost' identified by 'XXXXXXXXX';
</code></pre>
<p>If the prompt returns a <strong>Query OK</strong> message,
then the new user should have been created without any issues.</p>
<h2 id="create-a-practice-database"><a class="header" href="#create-a-practice-database">Create a Practice Database</a></h2>
<p>As the root database user, we create a new database for the user account we just created.
We'll call this database <code>opacdb</code> and set the character encoding to UTF-8 to support international characters.
Then we run the MySQL <code>show</code> command to view the new database.
Next we grant <code>all privileges</code> on the database to the user account <code>opacuser</code>.</p>
<pre><code>mysql&gt; create database opacdb default character set utf8mb4 collate utf8mb4_unicode_ci;
mysql&gt; show databases;
mysql&gt; grant all privileges on opacdb.* to 'opacuser'@'localhost' with grant option;
</code></pre>
<p>Other than granting <strong>all privileges</strong>, we could limit the user to specific privileges, including:</p>
<ul>
<li>CREATE</li>
<li>DROP</li>
<li>DELETE</li>
<li>INSERT</li>
<li>SELECT</li>
<li>UPDATE</li>
<li>GRANT OPTION</li>
</ul>
<p>Such privileges may be called operations or functions.
They allow MySQL users to use and modify the databases, where appropriate.
For example, we may want to limit the <strong>opacuser</strong> user account to only be able to use <strong>SELECT</strong> commands.
These decisions depend on the purpose of the database and our security risks.</p>
<p>Exit out of the MySQL database as the <strong>root MySQL user</strong>, and then exit out of the <strong>root Linux user account</strong>.
You should be back to your normal Linux user account:</p>
<pre><code>mysql&gt; \q
</code></pre>
<blockquote>
<p>Note: relational database keywords are often written in all capital letters: <code>CREATE</code>, <code>DROP</code>, <code>SELECT</code>, etc.
As far as I know, this is simply a convention to make the code more readable.
However, in most cases I'll write the keywords in lower case letters.
This is simply because, by convention, <em>I'm lazy</em>.</p>
</blockquote>
<h2 id="logging-in-as-regular-user-and-creating-tables"><a class="header" href="#logging-in-as-regular-user-and-creating-tables">Logging in as Regular User and Creating Tables</a></h2>
<p>We can now start doing MySQL work.
Note that when we login to the MySQL server, we leave the <code>bash</code> shell and enter the MySQL command line client.
By default, the prompt for the client is bare-bones.
We can make it more informative, though.
To do so, open your <code>.bashrc</code> file:</p>
<pre><code>nano .bashrc
</code></pre>
<p>Scroll to the bottom of the file and add the following:</p>
<pre><code>export MYSQL_PS1="[\d]&gt; "
</code></pre>
<p>Then save and exit and source the file:</p>
<pre><code>source ~/.bashrc
</code></pre>
<p>As a reminder, we've created a new MySQL user named <code>opacuser</code> and a new database for <code>opacuser</code> that is called <code>opacdb</code>.
When we run the <code>show databases</code> command as the <code>opacuser</code> user, we should see the <code>opacdb</code> database.
Note below that I use the <code>-p</code> option when logging back into MySQL as the <code>opacuser</code>.
This instructs MySQL to request the password for this user, it is required.</p>
<pre><code>mysql -u opacuser -p
</code></pre>
<p>From the MySQL prompt, list the available databases and use the <code>use</code> command to switch to the new <code>opacdb</code> database:</p>
<pre><code>mysql&gt; show databases;
mysql&gt; use opacdb;
</code></pre>
<p>A database is not worth much without data.
In the following code, I create and define a new table for our <code>opacdb</code> database.
The table will be called <code>books</code> that will contain data describing, er, some books.
We will keep this table very simple and use only four fields: <code>id</code>, <code>author</code>, <code>title</code>, and <code>copyright</code>.
The <code>id</code> field will function as a primary key (second to last line in the command below).
This key is used as a unique identifier for a record in the field.
When we create this key as a field called <code>id</code>, we state that it should be an integer <code>id</code> (or whole number),
that it should only be a positive number <code>unsigned</code>,
that it should not be empty <code>not null</code>, and
that with each record, it should increment by a single integer <code>auto_increment</code>.
When we create the <code>author</code> and <code>title</code> fields,
we say that these fields can have a maximum length of 150 characters and should not be empty.
When we create the <code>copyright</code> field, we limit it to the <code>year</code> data type,
which means it has to adhere to a specific syntax <code>YYYY</code>, and should not be empty.</p>
<pre><code>mysql&gt; create table books (
        id int unsigned not null auto_increment,
        author varchar(150) not null,
        title varchar(150) not null,
        copyright year(4) not null,
        primary key (ID)
);
</code></pre>
<blockquote>
<p>Note: A relational database contains tables.
If you are unfamiliar with this, you can think of a database as an overall Excel spreadsheet file and
tables as specific sheets in the Excel file.
There is quite a bit that goes into creating proper tables in database because the composition dictates how well
data is described and how tables connect and interact with (or <strong>relate to</strong>) each other.
However, we are going to keep things rather simple in this exercise.</p>
</blockquote>
<p>You can confirm that the table was created by running the following two commands.
The MySQL <code>show</code> command lists the tables in a database and the <code>describe</code> command describes a table's structure.</p>
<pre><code>mysql&gt; show tables;
mysql&gt; describe books;
</code></pre>
<p>Congratulations! Now create some records for that table.</p>
<h3 id="adding-records-into-the-table"><a class="header" href="#adding-records-into-the-table">Adding records into the table</a></h3>
<p>We can populate our <strong>opacdb</strong> database with some data.
(I simply picked the first book listed from the NYTimes best lists of books for the years 2019-2022.)
We'll use the MySQL <code>insert</code> command to add our records into our <code>books</code> table.
We need to specify three fields when entering data: <code>author</code>, <code>title</code>, and <code>copyright</code>.
The <code>copyright</code> field is a date field, and it should conform to the <code>YYYY</code> syntax.
We do not need to specify data for the <code>id</code> field because that will be created and will increment automatically.</p>
<pre><code>mysql&gt; insert into books (author, title, copyright) values
('Jennifer Egan', 'The Candy House', '2022'),
('Imbolo Mbue', 'How Beautiful We Were', '2021'),
('Lydia Millet', 'A Children\'s Bible', '2020'),
('Julia Phillips', 'Disappearing Earth', '2019');
</code></pre>
<p>Now we can view all the records that we just created with the MySQL <code>select</code> command:</p>
<pre><code>mysql&gt; select * from books;
</code></pre>
<p>Success! Now let's test our table.</p>
<h3 id="testing-commands"><a class="header" href="#testing-commands">Testing Commands</a></h3>
<p>We will complete the following tasks to refresh our MySQL knowledge or begin to learn how it works.
We will:</p>
<ul>
<li>retrieve records or parts of records,</li>
<li>delete a record,</li>
<li>alter the table structure so that it will hold more data, and</li>
<li>add a record</li>
</ul>
<blockquote>
<p>Note: each MySQL command ends with a semi-colon.
Some of the following MySQL commands are single-line, but others are multi-line.
Regardless if a MySQL command is one-line or multi-line, it doesn't end until it ends with a semi-colon.</p>
</blockquote>
<p>Please run the following commands, one at a time:</p>
<pre><code>mysql&gt; select author from books;
mysql&gt; select copyright from books;
mysql&gt; select author, title from books;
mysql&gt; select author from books where author like '%millet%';
mysql&gt; select title from books where author like '%mbue%';
mysql&gt; select author, title from books where title not like '%e';
mysql&gt; select * from books;
mysql&gt; alter table books add publisher varchar(75) after title;
mysql&gt; describe books;
mysql&gt; update books set publisher='Simon \&amp; Schuster' where id='1';
mysql&gt; update books set publisher='Penguin Random House' where id='2';
mysql&gt; update books set publisher='W. W. Norton \&amp; Company' where id='3';
mysql&gt; update books set publisher='Knopf' where id='4';
mysql&gt; select * from books;
mysql&gt; delete from books where author='Julia Phillips';
mysql&gt; insert into books
       (author, title, publisher, copyright) values
       ('Emma Donoghue', 'Room', 'Little, Brown \&amp; Company', '2010'),
       ('Zadie Smith', 'White Teeth', 'Hamish Hamilton', '2000');
mysql&gt; select * from books;
mysql&gt; select author, publisher from books where copyright &lt; '2011';
mysql&gt; select author from books order by copyright;
mysql&gt; \q
</code></pre>
<h2 id="install-php-and-mysql-support"><a class="header" href="#install-php-and-mysql-support">Install PHP and MySQL Support</a></h2>
<p>The next goal is to complete the connection between PHP and MySQL so that we can use both for our websites.</p>
<p>First install PHP support for MySQL.
We're installing some modules alongside the basic support.
These may or may not be needed, but I'm installing them to demonstrate some basics.</p>
<pre><code>sudo apt install php-mysql php-mysqli
</code></pre>
<p>And then restart Apache and MySQL:</p>
<pre><code>sudo systemctl restart apache2
sudo systemctl restart mysql
</code></pre>
<h3 id="create-php-scripts"><a class="header" href="#create-php-scripts">Create PHP Scripts</a></h3>
<p>In order for PHP to connect to MySQL, it needs to authenticate itself.
To do that, we will create a <code>login.php</code> file in in our document root's parent directory: <code>/var/www</code>.
We also need to change the group ownership of the file and its permissions (see note below).
This will allow the file to be read by the Apache web server but not by the world.
This prevents the password information from being accessible to web users.</p>
<pre><code>cd /var/www
sudo touch login.php
sudo chmod 640 login.php
sudo chown :www-data login.php
ls -l login.php
sudo nano login.php
</code></pre>
<blockquote>
<p><strong>A Short Aside on File Ownership and Permissions</strong></p>
<p>We haven't covered in detail the concept of file ownership and permissions.
In short, files and directories are owned by users and by user groups.
When we run a command like <code>ls -l</code>, the output will show the file owner and the group owner:
In the output below, the user <code>root</code> is the file owner and the group <code>www-data</code> is the group owner of the file <code>login.php</code>.</p>
<pre><code>-rw-r----- 1 root www-data    0 Feb 14 02:44 login.php
</code></pre>
<p>We change the file permissions with the <code>chmod</code> command.
We change the file ownership with the <code>chown</code> command.
In the above code snippet, the <code>chmod 640 login.php</code> command changes the file permissions to:</p>
<ul>
<li>user owner: read, write</li>
<li>group owner: read only</li>
<li>other/world: no permissions</li>
</ul>
<p>For an in-depth tutorial, see <a href="https://cseanburns.github.io/linux_sysadmin/3c-file-perms-owns.html">Linux Systems Administrations, Chapter 3.3 File Permissions and Ownership</a></p>
<p>When we created the file using the <code>sudo touch login.php</code> command, the use of <code>sudo</code> creates the file with <code>root</code> as the owner.
The <code>chown :www-data login.php</code> command changes the group ownership to <code>www-data</code>.
The <code>www:data</code> user is the Apache user.</p>
<p>Many services, like Apache, have corresponding users on the system.
Files placed in our document root <code>/var/www/html</code> will be served on the web.
Therefore, those files must have read access for <strong>other/world</strong>.
But we don't want the <code>login.php</code> file to be accessible to the world since it contains login information for our MySQL user.
Thus, in addition to modifying file ownership and permissions, we also place it in the parent directory <code>/var/www</code>.</p>
</blockquote>
<p>In the file, add the following credentials.
If for some reason you used a different database name than <code>opacdb</code> and a different username than <code>opacuser</code>,
then you need to substitute your names below.
You need to use your own password where I have the <code>Xs</code>:</p>
<pre><code>&lt;?php // login.php
$db_hostname = "localhost";
$db_database = "opacdb";
$db_username = "opacuser";
$db_password = "XXXXXXXXX";
?&gt;
</code></pre>
<p>Next we create a new PHP file for our website.
This file will display HTML but will primarily be PHP interacting with our <code>opacdb</code> database.</p>
<p>Create a file titled <code>opac.php</code> in <code>/var/www/html</code>.</p>
<pre><code>cd /var/www/html
sudo nano opac.php
</code></pre>
<p>Then copy over the following text.
I suggest you transcribe it, especially if you're interested in learning a bit of PHP.</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
&lt;head&gt;
    &lt;meta charset="UTF-8"&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
    &lt;title&gt;MySQL Server Example&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;

    &lt;h1&gt;A Basic OPAC&lt;/h1&gt;
    &lt;p&gt;We can retrieve all the data from our database and book table using a couple of different queries.&lt;/p&gt;

    &lt;?php
    // Load MySQL credentials securely
    require_once '/var/www/login.php';

    // Enable detailed MySQL error reporting
    mysqli_report(MYSQLI_REPORT_ERROR | MYSQLI_REPORT_STRICT);

    // Establish database connection
    $conn = new mysqli($db_hostname, $db_username, $db_password, $db_database);

    if ($conn-&gt;connect_error) {
        die("Connection failed: " . $conn-&gt;connect_error);
    }

    echo "&lt;h2&gt;Query 1: Retrieving Publisher and Author Data&lt;/h2&gt;";

    // Query using prepared statement
    $stmt = $conn-&gt;prepare("SELECT publisher, author FROM books");
    $stmt-&gt;execute();
    $result = $stmt-&gt;get_result();

    while ($row = $result-&gt;fetch_assoc()) {
        echo "&lt;p&gt;Publisher " . htmlspecialchars($row["publisher"]) .
             " published a book by " . htmlspecialchars($row["author"]) . ".&lt;/p&gt;";
    }

    $stmt-&gt;close();

    echo "&lt;h2&gt;Query 2: Retrieving Author, Title, and Date Published Data&lt;/h2&gt;";

    $stmt2 = $conn-&gt;prepare("SELECT author, title, copyright FROM books");
    $stmt2-&gt;execute();
    $result2 = $stmt2-&gt;get_result();

    while ($row = $result2-&gt;fetch_assoc()) {
        echo "&lt;p&gt;A book by " . htmlspecialchars($row["author"]) .
             " titled &lt;em&gt;" . htmlspecialchars($row["title"]) .
             "&lt;/em&gt; was released in " . htmlspecialchars($row["copyright"]) . ".&lt;/p&gt;";
    }

    $stmt2-&gt;close();
    $conn-&gt;close();
    ?&gt;

&lt;/body&gt;
&lt;/html&gt;

</code></pre>
<p>Save the file and exit out of <code>nano</code>.</p>
<h3 id="test-syntax"><a class="header" href="#test-syntax">Test Syntax</a></h3>
<p>After you save the file and exit the text editor, we need to test the PHP syntax.
If there are any errors in our PHP, these commands will show the line numbers causing errors or leading up to errors.
Nothing will output if all is well with the first command.
If all is well with the second command, HTML should be outputted:</p>
<pre><code>sudo php -f /var/www/login.php
sudo php -f /var/www/html/opac.php
</code></pre>
<p>Now view the site by opening the public IP address for your server in your browser.
If all goes well, you should see the data in your <code>opacdb</code> database and <code>books</code> table rendered in your webpage.</p>
<h2 id="conclusion-10"><a class="header" href="#conclusion-10">Conclusion</a></h2>
<p>Congratulations! If you've reached this far, you have successfully created a LAMP stack.
In the process, you have learned how to install and set up MySQL, how to create MySQL root and regular user accounts,
how to create a test database with play data for practicing, and how to connect this with PHP for display on a webpage.</p>
<p>In regular applications of these technologies, there's a lot more involved,
but completing the above process is a great start to learning more.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="diy-integrated-library-systems"><a class="header" href="#diy-integrated-library-systems">DIY Integrated Library Systems</a></h1>
<p>Libraries use integrated library systems (ILS) to manage and add to their collections and
to provide online public access catalogs or discovery systems to their users to search those collections.
In this chapter, based on the LAMP stack we created in the prior chapter, we will build a basic ILS.
Specifically, we will create a bare bones OPAC and a bare bones cataloging module, two cornerstone modules in all ILSs.</p>
<p>We start with an additional lesson in relational database usage.
We then proceed to build the OPAC and cataloging modules.
By the end of this chapter, you will have a deeper understanding of the fundamentals of these library technologies
and how they are used in everyday practice.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="introduction-to-relational-databases"><a class="header" href="#introduction-to-relational-databases">Introduction to Relational Databases</a></h2>
<p>In the last section, we installed, configured, and setup a Linux, Apache, MySQL, and PHP (LAMP) stack.
While setting up MySQL, we created a basic <code>opacdb</code> database containing a <code>books</code> table.
Then we learned a few queries to get a feel for how querying a relational database, like MySQL, works.</p>
<p>In this section, we are going to spend a bit more time with MySQL simply to acquire a greater understanding of relational databases.
The real power of relational databases lies in their ability to manage and retrieve data efficiently.
This is accomplished by spreading data across multiple tables in order to limit data duplication and increase efficiency.</p>
<p>We'll create a new database for our <code>opacuser</code>.
Unlike the <code>opacdb</code> database, our new database, which we'll call <code>DinnerDB</code>, will contain two tables.
This will reduce the amount of data we need to add to our database.</p>
<h2 id="create-database"><a class="header" href="#create-database">Create Database</a></h2>
<p>First, we will create a new database.
Our <code>opacuser</code> does not have the privileges to create a new database, so we must login as the root MySQL user:</p>
<pre><code>sudo mysql -u root
</code></pre>
<p>Once logged in, we create a database called <code>DinnerDB</code>.
We could create another user, but for our purposes, we can simply grant <code>opacuser</code> privileges on the new database.</p>
<pre><code>mysql&gt; create database DinnerDB;
mysql&gt; grant all privileges on DinnerDB.* to 'opacuser'@'localhost';
</code></pre>
<p>We can now exit the root MySQL user account:</p>
<pre><code>mysql&gt; \q
</code></pre>
<h2 id="create-tables"><a class="header" href="#create-tables">Create Tables</a></h2>
<p>Next we login as <code>opacuser</code>:</p>
<pre><code>mysql -u opacuser -p
</code></pre>
<p>First, let's check if we can see if the new <code>DinnerDB</code> database is visible to <code>opacuser</code>.
If so, we begin using it:</p>
<pre><code>[(none)]&gt; show databases;
[(none)]&gt; use DinnerDB;
</code></pre>
<h3 id="create-meals-table"><a class="header" href="#create-meals-table">Create <code>Meals</code> Table</a></h3>
<p>We are going to create two tables in <code>DinnerDB</code>.
We will call the first table <code>Meals</code> and the second table <code>Ingredients</code>.
The second table will list the ingredients and quantities needed to make the meals named in the <code>Meals</code> table.</p>
<p>The following command creates a table called <code>Meals</code>.
The table has five values:</p>
<ul>
<li><code>meal_id</code> is an integer that serves as the <a href="https://dev.mysql.com/doc/refman/8.4/en/primary-key-optimization.html">primary key</a>.</li>
<li><code>meal_name</code> contains a <a href="https://dev.mysql.com/doc/refman/8.4/en/char.html">variable-length string</a> up to 100 characters.</li>
<li><code>cuisine</code> contains a variable-length string up to 50 characters.</li>
<li><code>cooking_time</code> is an integer that uses a <a href="https://dev.mysql.com/doc/refman/8.4/en/create-table-check-constraints.html">CHECK constraint</a> so that if a user enters a zero or a negative value, MySQL will reject it, or if a user enters no value, it will default to one.</li>
<li><code>vegetarian</code> contains a BOOLEAN value, which means it must be TRUE or FALSE:
<ul>
<li>technically, BOOLEAN is synonymous with a data type called <code>TINYINT(1)</code>, but BOOLEAN better conveys what we mean.</li>
</ul>
</li>
</ul>
<pre><code>create table Meals (
    meal_id int auto_increment primary key,
    meal_name varchar(100) not null,
    cuisine varchar(50),
    cooking_time int not null default 1 check (cooking_time &gt; 0),
    vegetarian boolean
);
</code></pre>
<h3 id="create-ingredients-table"><a class="header" href="#create-ingredients-table">Create <code>Ingredients</code> table</a></h3>
<p>The following command creates the <code>Ingredients</code> table.
The table has four values:</p>
<ul>
<li><code>ingredient_id</code> is the primary key.</li>
<li><code>meal_id</code> is an integer:
<ul>
<li>the last line in the command declares the <code>meal_id</code> value to be a foreign key that references the <code>meal_id</code> primary key in the <code>Meals</code> table</li>
<li>foreign keys allow for cross-referencing to primary keys.</li>
<li>since we cross-reference <code>meal_id</code> in the <code>Ingredients</code> table to <code>meal_id</code> in the <code>Meals</code> table, we make the <code>Ingredients</code> table a child of the <code>Meals</code> table, which by entailment functions as the parent table to the <code>Ingredients</code> table.</li>
<li>the <code>on delete cascade</code> clause instructs MySQL to delete associated ingredients when deleting a meal in the <code>Meals</code> table.</li>
</ul>
</li>
<li><code>ingredient_name</code> contains a variable-length string up to 100 characters.</li>
<li><code>quantity</code> contains a variable-length string up to 50 characters.</li>
</ul>
<pre><code>create table Ingredients (
    ingredient_id int auto_increment primary key,
    meal_id int,
    ingredient_name varchar(100) not null,
    quantity varchar(50),
    foreign key (meal_id) references Meals(meal_id) on delete cascade
);
</code></pre>
<h2 id="insert-data"><a class="header" href="#insert-data">Insert data</a></h2>
<p>Now that we have created the structure of our two tables, we can begin adding data to them.
The first command adds four records to the <code>Meals</code> table:</p>
<pre><code>insert into Meals (meal_name, cuisine, cooking_time, vegetarian) values
    ('Spaghetti Bolognese', 'Italian', 45, FALSE),
    ('Vegetable Stir Fry', 'Chinese', 20, TRUE),
    ('Chicken Curry', 'Indian', 50, FALSE),
    ('Mushroom Risotto', 'Italian', 35, TRUE);
</code></pre>
<p>And the second command adds the list of ingredients for the meals we added to the <code>Meals</code> table.
The integers we use for <code>meal_id</code> match the values produced in the <code>Meals</code> table, which we can see with the <code>select * from Meals;</code> command.
Therefore, <code>1</code> refers to <strong>Spaghetti Bolognese</strong>, <code>2</code> refers to <strong>Vegetable Stir Fry</strong>, and so on.</p>
<pre><code>insert into Ingredients (meal_id, ingredient_name, quantity) values
    (1, 'Spaghetti', '200g'),
    (1, 'Ground Beef', '250g'),
    (1, 'Tomato Sauce', '1 cup'),
    (2, 'Broccoli', '100g'),
    (2, 'Carrots', '50g'),
    (2, 'Soy Sauce', '2T'),
    (3, 'Chicken Breast', '300g'),
    (3, 'Curry Powder', '2T'),
    (3, 'Coconut Milk', '1 cup'),
    (4, 'Arborio Rice', '1 cup'),
    (4, 'Mushrooms', '1 cup'),
    (4, 'Parmesan Cheese', '1/2 cup');
</code></pre>
<pre><code>In practice, we might want to create an additional column that would contain units for the quantities (e.g., cups, grams, etc).
This would result in better [database normalization][db_normalization_wiki].
</code></pre>
<h2 id="querying-data"><a class="header" href="#querying-data">Querying Data</a></h2>
<p>Now that we have created our tables and added records to them, we can begin to query them.
The next command is a simple <code>SELECT</code> statement that returns the entire contents of the <code>Meals</code> table:</p>
<pre><code>select * from Meals;
</code></pre>
<p>We can filter results with the <code>WHERE</code> clause.
In this example, we filter results by whether a meal is vegetarian:</p>
<pre><code>select * from Meals where vegetarian = TRUE;
</code></pre>
<p>We can sort the results by descending or ascending order.
This works for both alphabetic and numeric characters.
The following commands sort the Meals by length of cooking time, which is an integer:</p>
<pre><code>select * from Meals order by cooking_time desc; 
select * from Meals order by cooking_time asc; 
</code></pre>
<p>In the following command, we select three values:</p>
<ul>
<li><code>meal_name</code> from the <code>Meals</code> table and rename the resulting column <code>Meals</code>.</li>
<li><code>ingredient_name</code> from the <code>Ingredients</code> table and rename the resulting column <code>Ingredients</code>.</li>
<li><code>quantity</code> from the <code>Ingredients</code> table and rename the resulting column <code>Quantity</code>.</li>
</ul>
<p>We also use the <code>join</code> action to cross-reference the tables based on the shared <code>meal_id</code> value.
Note that I use the <code>table_name.column_name</code> syntax in the query.
For example, <code>Meals.meal_name</code> refers to the column <code>meal_name</code> in the <code>Meals</code> table, and so forth.
The <code>as</code> in <code>as Meals</code>, <code>as Ingredients</code>, and <code>as Quantity</code> instructs MySQL to rename the columns.
This is useful for the presentation of the data:</p>
<pre><code>select Meals.meal_name as Meals,
    Ingredients.ingredient_name as Ingredients,
    Ingredients.quantity as Quantity
    from Meals
    join Ingredients on Meals.meal_id = Ingredients.meal_id;
</code></pre>
<p>In the following example, we list the ingredients and their quantities based on the name of a meal.
In this case, we are looking to list the ingredients for the Chicken Curry dish:</p>
<pre><code>select ingredient_name as Ingredients,
    quantity as Quantity
    from Ingredients 
    where meal_id = (select meal_id from Meals where meal_name = 'Chicken Curry');
</code></pre>
<p>In the following example, we instruct MySQL to provide a count of the Meals by cuisine:</p>
<pre><code>select cuisine, count(*) as meal_count 
    from Meals
    group by cuisine;
</code></pre>
<p>And finally, it's been a tough day, and we want to identify Meals that don't take long to cook.
The following command returns all Meals where the cooking time is less than or equal to 45 minutes:</p>
<pre><code>select meal_name, cooking_time 
    from Meals 
    where cooking_time &lt;= 45
    order by cooking_time asc;
</code></pre>
<p>Once done querying our database, we can logout:</p>
<pre><code>\q
</code></pre>
<h2 id="database-management"><a class="header" href="#database-management">Database Management</a></h2>
<p>When we started this lesson, we logged into MySQL and created a database called <code>DinnerDB</code>.
Then we granted <code>opacuser</code> all privileges to this database.
There might be times when we want to revoke those privileges.
To do so, we first log back in as the root MySQL user:</p>
<pre><code>sudo mysql -u root
</code></pre>
<p>Now we can re-review the privileges for <code>opacuser</code>:</p>
<pre><code>mysql&gt; show grants for 'opacuser'@'localhost';
</code></pre>
<p>We can take away those privileges with the <code>REVOKE</code> command:</p>
<pre><code>mysql&gt; revoke all privileges on DinnerDB.* from 'opacuser'@'localhost';
</code></pre>
<p>To confirm, we can re-run the <code>show grants</code> command.</p>
<p>If we want to track other users in with accounts in the MySQL server,
the following command queries the <code>user</code> table in the <code>mysql</code> database and will return all user accounts:</p>
<pre><code>select user, host from mysql.user;
</code></pre>
<p>We can also delete the database using the <code>DROP</code> command:</p>
<pre><code>mysql&gt; drop database DinnerDB;
</code></pre>
<p>If desired, we can delete user accounts (but <strong>don't do this</strong> with <code>opacuser</code>).
For example, if had a user named <code>sean</code>, then we could use the following command to remove their account:</p>
<pre><code>mysql&gt; drop user 'sean'@'localhost';
</code></pre>
<h2 id="conclusion-11"><a class="header" href="#conclusion-11">Conclusion</a></h2>
<p>In this introduction, we explored the basics of relational databases using MySQL.
We created a structured database (<code>DinnerDB</code>), defined two tables (<code>Meals</code> and <code>Ingredients</code>),
inserted data, and performed various queries to retrieve information efficiently.</p>
<p>The key takeaways from this exercise include:</p>
<ul>
<li><strong>Normalization</strong>: Breaking data into multiple tables reduces redundancy and improves consistency.</li>
<li><strong>Relationships</strong>: Using foreign keys, we linked meals with their ingredients. This allows for more meaningful data retrieval.</li>
<li><strong>Querying</strong>: We practiced <code>SELECT</code>, <code>JOIN</code>, <code>WHERE</code>, <code>ORDER_BY</code>, and <code>GROUP_BY</code> to manipulate and filter data results.</li>
<li><strong>Management</strong>: We reviewed how to create a database, grant privileges to the database to a specific user, remove those privileges, and delete the database.</li>
</ul>
<p>This tutorial is only a start.
You should experiment with modifying the data, adding constraints, separating quantities from units, or
creating additional tables to further your understanding.</p>
<p>Databases are powerful tools for organizing and retrieving structured information.
If you understand SQL and its logic, it will help you in other domains, too!
So go play!</p>
<p>And if interested, I encourage you to learn other SQL implementation, like <a href="https://www.sqlite.org/">SQLite</a>,
which you can download and install on your personal machines fairly easily.
SQLite is probably the most popular SQL application, but it uses <a href="https://www.sqlite.org/cli.html">a slightly different command syntax</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="creating-a-bare-bones-opac"><a class="header" href="#creating-a-bare-bones-opac">Creating a Bare Bones OPAC</a></h1>
<p>In this section, we're going to create a bare bones, very basic OPAC.
The idea is simple: to acquire an intuition and understanding of how data
from a relational database is retrieved and entered using LAMP technologies.</p>
<p>A real integrated library system (ILS) is much more complex than what we are doing here, but
the fundamental ideas are the same:
we enter data into a database, and we retrieve data from a database.
In practice, a whole slew of other technologies are added to present the data in a user-friendly way: HTML, CSS, and JavaScript.</p>
<p>ILSes provide modules for patron management, acquisitions, circulation, cataloging,
serials management, authorities, reporting, and sometimes more (see <a href="https://koha-community.org/about/">Koha: About</a>).
Those modules rely on some kind of underlying relational database, like MySQL (which is what Koha uses).
And this results in a complex, interconnected set of tables.
We are working with only one table in our database, the <strong>books</strong> table.
In reality, an ILS will rely on dozens of tables.</p>
<p>In the prior section, we created a MySQL database called <strong>opacdb</strong>, which has one table, called <strong>books</strong>.
We also created a PHP file to retrieve the data from the <strong>books</strong> table and present it on a web page.</p>
<p>In this section, we are going to use different PHP code that will allow us to search the <strong>books</strong> table.
In this way, we more closely mimic an OPAC.</p>
<h2 id="creating-the-html-page-and-a-php-search-page"><a class="header" href="#creating-the-html-page-and-a-php-search-page">Creating the HTML Page and a PHP Search Page</a></h2>
<p>The first thing we do is create a basic HTML page that contains a form for entering queries.
We'll call this HTML page with the form <strong>mylibrary.html</strong>.
When a user clicks on the submit button in the form, the form will activate a PHP script called <strong>search.php</strong>.
That <strong>search.php</strong> will establish a connection to the OPAC database that we already have created.
Our PHP script will contain a special MySQL query that will allow us to search all the fields in our <strong>books</strong> table.
Then it will iterate through each row of the <strong>books</strong> table and return results that match our query.
We also add two date fields to our form to limit results by publication dates,
which we labeled as <strong>copyright</strong> in our MySQL <strong>books</strong> table.</p>
<h3 id="html-form"><a class="header" href="#html-form">HTML Form</a></h3>
<p>Here is the HTML for our search page, titled <strong>mylibrary.html</strong>:</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta charset="UTF-8"&gt;
		&lt;title&gt;MySQL Server Example&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;

	&lt;h1&gt;A Basic OPAC&lt;/h1&gt;

	&lt;p&gt;In the form below, &lt;b&gt;optionally&lt;/b&gt; enter text in the search field.
	Your search query will search by author, title, or publisher.
	Capitalization is not necessary.
	It's okay to enter partial information, like part of an author's, title's, or publisher's name.&lt;/p&gt;

	&lt;p&gt;You can leave the search field empty and only enter dates.
    Regardless, both start and end dates are required for all searches.
	You can use the date fields to limit results, too.
	I added some extra records, which you can view to know what you can query:&lt;/p&gt;

	&lt;p&gt;&lt;a href="opac.php"&gt;OPAC&lt;/a&gt;&lt;/p&gt;

	&lt;p&gt;This is very much a toy, stripped down
	&lt;a href="https://en.wikipedia.org/wiki/Online_public_access_catalog"&gt;OPAC&lt;/a&gt;.
	The records are basic.
	Not only do they not conform to &lt;a href="https://www.loc.gov/marc/"&gt;MARC&lt;/a&gt;,
	they don't even conform to something as simple as &lt;a href="https://www.dublincore.org/"&gt;Dublin Core&lt;/a&gt;.

	&lt;p&gt;I also don't provide options to select different fields, like author, title, or publisher fields.
	Instead the search field below searches all the fields (author, title, publisher) in our &lt;b&gt;books&lt;/b&gt; table.&lt;/p&gt;

	&lt;p&gt;The key idea is to get a sense of how an OPAC works, though.&lt;/p&gt;

	&lt;h2&gt;My Basic Library OPAC&lt;/h2&gt;

	&lt;form method="post" action="search.php"&gt;
		&lt;label for="search"&gt;Search Terms (optional):&lt;/label&gt;
		&lt;input type="text" name="search" id="search"&gt;
        
		&lt;br&gt;
        
		&lt;label for="start_date"&gt;Start Date:&lt;/label&gt;
		&lt;input type="date" name="start_date" id="start_date" required&gt;
        
		&lt;br&gt;
        
		&lt;label for="end_date"&gt;End Date:&lt;/label&gt;
		&lt;input type="date" name="end_date" id="end_date" required&gt;
        
		&lt;br&gt;
        
		&lt;input type="submit" value="Search"&gt;
	&lt;/form&gt;

&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<h3 id="php-search-script"><a class="header" href="#php-search-script">PHP Search Script</a></h3>
<p>Here is the PHP for our search script, which should be named <strong>search.php</strong>:</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
&lt;head&gt;
    &lt;meta charset="UTF-8"&gt;
    &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
    &lt;title&gt;Search Results&lt;/title&gt;
&lt;style&gt;
    table {
        border-collapse: collapse;
        width: 100%;
    }
    th, td {
        border: 1px solid black;
        padding: 8px;
        text-align: left;
    }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;

    &lt;h1&gt;Search Results&lt;/h1&gt;

    &lt;?php
    // Load MySQL credentials
    require_once '/var/www/login.php';

    // Enable MySQL error reporting
    mysqli_report(MYSQLI_REPORT_ERROR | MYSQLI_REPORT_STRICT);

    // Establish connection
    $conn = new mysqli($db_hostname, $db_username, $db_password, $db_database);
    if ($conn-&gt;connect_error) {
        die("Connection failed: " . $conn-&gt;connect_error);
    }

    if ($_SERVER["REQUEST_METHOD"] == "POST") {
        $search = trim($_POST['search']);
        $start_date = $_POST['start_date'];
        $end_date = $_POST['end_date'];

        // Prepared statement to prevent SQL injection
        $stmt = $conn-&gt;prepare("SELECT * FROM books 
                                WHERE (author LIKE ? OR title LIKE ? OR publisher LIKE ?) 
                                AND copyright BETWEEN ? AND ?");

        // Use wildcard search
        $search_param = "%$search%";
        $stmt-&gt;bind_param("sssss", $search_param, $search_param, $search_param, $start_date, $end_date);
        $stmt-&gt;execute();
        $result = $stmt-&gt;get_result();

        if ($result-&gt;num_rows &gt; 0) {
            echo "&lt;table&gt;";
            echo "&lt;tr&gt;&lt;th&gt;ID&lt;/th&gt;&lt;th&gt;Author&lt;/th&gt;&lt;th&gt;Title&lt;/th&gt;&lt;th&gt;Publisher&lt;/th&gt;&lt;th&gt;Copyright&lt;/th&gt;&lt;/tr&gt;";

            while ($row = $result-&gt;fetch_assoc()) {
                echo "&lt;tr&gt;";
                echo "&lt;td&gt;" . htmlspecialchars($row["id"]) . "&lt;/td&gt;";
                echo "&lt;td&gt;" . htmlspecialchars($row["author"]) . "&lt;/td&gt;";
                echo "&lt;td&gt;" . htmlspecialchars($row["title"]) . "&lt;/td&gt;";
                echo "&lt;td&gt;" . htmlspecialchars($row["publisher"]) . "&lt;/td&gt;";
                echo "&lt;td&gt;" . htmlspecialchars($row["copyright"]) . "&lt;/td&gt;";
                echo "&lt;/tr&gt;";
            }

            echo "&lt;/table&gt;";
        } else {
            echo "&lt;p&gt;No results found.&lt;/p&gt;";
        }

        $stmt-&gt;close();
    }

    $conn-&gt;close();
    ?&gt;

    &lt;p&gt;&lt;a href="mylibrary.html"&gt;Return to search page&lt;/a&gt;&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<h2 id="modifications"><a class="header" href="#modifications">Modifications</a></h2>
<p>Add more records, using MySQL, to your <strong>books</strong> table, and test your queries.
To add records to your <strong>books</strong> table, recall that we used the <strong>insert into</strong> MySQL statements.
Here's the example from the prior lesson.
Use it to add titles that are of interest to you.</p>
<p>First connect to the MySQL server:</p>
<pre><code>mysql -u opacuser -p
</code></pre>
<p>Then run the <code>insert</code> command with the data for the new records:</p>
<pre><code>insert into books
(author, title, publisher, copyright) values
('Emma Donoghue', 'Room', 'Little, Brown \&amp; Company', '2010'),
('Zadie Smith', 'White Teeth', 'Hamish Hamilton', '2000');
</code></pre>
<h2 id="conclusion-12"><a class="header" href="#conclusion-12">Conclusion</a></h2>
<p>In this lesson, we created a very bare bones OPAC simply to express the fundamental idea
of how data is stored and retrieved on the web.
In reality, what separates an OPAC, or a discovery service in a modern integrated library system
or library service platform, from other databases on the web is the structure of the records
that are stored in the relational database.
Such records are structured using MARC.
Our records are very simply structured, but still, I hope this helps in creating an intuition
about how OPACs and like function.
In the next section, we will learn how to enter data into our catalog,
thereby mimicking the cataloging module of an integrated library system.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="creating-a-bare-bones-cataloging-module"><a class="header" href="#creating-a-bare-bones-cataloging-module">Creating a Bare Bones Cataloging Module</a></h1>
<p>If you have worked with an integrated library system (ILS) or a more modern library service platform (LSP),
then you know that an OPAC or discovery system, respectively,
is simply one <a href="https://cseanburns.github.io/electronic_resource_mgmt/04-erm-ils.html#administration">module out of several that makeup an ILS or LSP</a>.
Other modules include acquisitions, authority files, circulation, course reserves, patron management, and more.
In the prior section, we created one of those modules: a bare bones OPAC.
In this section, we are going to create a bare bones cataloging module in the same kind of way.</p>
<p>Up until this point, you have added records to your OPAC using the MySQL command interface.
But unless you are a full time database administrator or programmer,
it's unlikely that you would add data to your system via that interface.
Instead you would use an application via a fancy graphical user interface, i.e., integrated library system.
The reason we started off with MySQL is not because you would necessarily use this interface on a daily basis.
Rather, it's because I want you to understand the foundations of these technologies and
the how they get translated for users when they become web applications.</p>
<h2 id="creating-the-html-page-and-a-php-cataloging-page"><a class="header" href="#creating-the-html-page-and-a-php-cataloging-page">Creating the HTML Page and a PHP Cataloging Page</a></h2>
<p>Like in the last exercise, the first thing we do is create a basic HTML page that
contains a form for entering our bibliographic data.
Again, our cataloging <em>module</em> will not be real world like.
The goal here is to build an intuition about how these technologies work and
to provide some grounding if you do want to pursue a more technical path.</p>
<p>The form that we will create needs to mirror the data structure in the <strong>books</strong> table that we created in our prior lesson.
That means it will only contain four fields:</p>
<ul>
<li>author</li>
<li>title</li>
<li>publisher</li>
<li>copyright</li>
</ul>
<p>I'll call this page <strong>index.html</strong>.
I'll create a new directory for this module:</p>
<pre><code>cd /var/www/html
sudo mkdir cataloging
</code></pre>
<p>Then I'll use a text editor to create the index.html file and add the content:</p>
<pre><code>cd cataloging
sudo nano index.html
</code></pre>
<p>In <strong>index.html</strong>, we add the following content:</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
	&lt;title&gt;Enter Records&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
	&lt;h1&gt;OPAC Library Administration&lt;/h1&gt;

	&lt;p&gt;This is the library administration page for entering records into the OPAC.&lt;/p&gt;
	&lt;p&gt;Please do not use this page unless you are an authorized cataloger.&lt;/p&gt;

	&lt;form action="insert.php" method="post"&gt;
		&lt;label for="author"&gt;Author:&lt;/label&gt;
		&lt;input type="text" name="author" id="author" required&gt;&lt;br&gt;&lt;br&gt;

		&lt;label for="title"&gt;Book Title:&lt;/label&gt;
		&lt;input type="text" name="title" id="title" required&gt;&lt;br&gt;&lt;br&gt;

		&lt;label for="publisher"&gt;Publisher:&lt;/label&gt;
		&lt;input type="text" name="publisher" id="publisher" required&gt;&lt;br&gt;&lt;br&gt;

		&lt;label for="copyright"&gt;Copyright:&lt;/label&gt;
		&lt;input type="number" name="copyright" id="copyright" min="1000" max="2300" required&gt;

		&lt;input type="submit" value="Submit"&gt;
	&lt;/form&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<h2 id="php-insert-script"><a class="header" href="#php-insert-script">PHP Insert Script</a></h2>
<p>The <strong>index.html</strong> page will provide a user interface, that is, a form, for entering our bibliographic data.
However, the PHP script is needed to communicate and add the data from our form into our MySQL database and <strong>books</strong> table.</p>
<p>Also, just as the HTML form has to match the data structure of the <strong>books</strong> table,
the PHP script also needs to match the form from the HTML page and the data structure in the <strong>books</strong> table.</p>
<p>Here is the PHP script, which I call <strong>insert.php</strong>, which you'll notice was referenced in the HTML code above:</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
	&lt;meta charset="UTF-8"&gt;
	&lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;
	&lt;title&gt;Cataloging: Data Entry&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;

&lt;h1&gt;Cataloging: Data Entry&lt;/h1&gt;

&lt;?php

// Load MySQL credentials
require_once '/var/www/login.php';

// Enable MySQL error reporting
mysqli_report(MYSQLI_REPORT_ERROR | MYSQLI_REPORT_STRICT);

// Establish connection
$conn = new mysqli($db_hostname, $db_username, $db_password, $db_database);
if ($conn-&gt;connect_error) {
    die("Connection failed: " . $conn-&gt;connect_error);
}

// Prepare and bind SQL statement
$stmt = $conn-&gt;prepare("INSERT INTO books (author, title, publisher, copyright) VALUES (?, ?, ?, ?)");
$stmt-&gt;bind_param("ssss", $author, $title, $publisher, $copyright);

// Set parameters and execute statement
$author = $_POST["author"];
$title = $_POST["title"];
$publisher = $_POST["publisher"];
$copyright = $_POST["copyright"];

if ($stmt-&gt;execute() === TRUE) {
    echo "New record created successfully";
} else {
    echo "Error: " . $stmt-&gt;error;
}

// Close statement and connection
$stmt-&gt;close();
$conn-&gt;close();
?&gt;

&lt;p&gt;&lt;a href='index.html'&gt;Return to Cataloging Page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href='../mylibrary.html'&gt;Return to Library Home Page&lt;/a&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<h2 id="security"><a class="header" href="#security">Security</a></h2>
<p>Since our HTML and PHP files allow us to enter data into our MySQL database from a simple web interface,
we need to limit access to the module.
In a real-world situation, modules like these would have a variety of security measures in place to prevent wrongful data entry.
In our case, we will rely on a simple authorization mechanism provided by the Apache2 server called <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-password-authentication-with-apache-on-ubuntu-18-04">htpasswd</a>.</p>
<p>First, we create an authentication file in our <strong>/etc/apache2</strong> directory,
which is where the <strong>Apache2</strong> web server stores its configuration files.
The file will contain a <strong>hashed</strong> password and a username we give it.
In the following command to set the password, I set the username to <strong>libcat</strong>, but it could be anything:</p>
<pre><code>sudo htpasswd -c /etc/apache2/.htpasswd libcat
</code></pre>
<p>Next we need to tell the Apache2 web server that we will use the <code>htpasswd</code> to control access to our cataloging module.
To do that, we use a text editor to open the <strong>apache2.conf</strong> file.</p>
<pre><code>sudo nano /etc/apache2/apache2.conf
</code></pre>
<p>In the <strong>apache2.conf</strong> file, look for the code block / stanza below.
We are interested in the third line in the stanza, which is line 172 for me, and probably is for you, too.</p>
<pre><code>&lt;Directory /var/www/&gt;
  Options Indexes FollowSymLinks
  AllowOverride None
  Require all granted
&lt;/Directory&gt;
</code></pre>
<p><strong>Carefully</strong>, we need to change the word <strong>None</strong> to the word <strong>All</strong>:</p>
<pre><code>&lt;Directory /var/www/&gt;
  Options Indexes FollowSymLinks
  AllowOverride All
  Require all granted
&lt;/Directory&gt;
</code></pre>
<p>Next, change to the <strong>cataloging</strong> directory and use our text editor to create a file called <strong>.htaccess</strong>
(note the leading period in the file name):</p>
<pre><code>cd /var/www/html/cataloging
sudo nano .htaccess
</code></pre>
<p>Add the following content to <strong>.htaccess</strong>:</p>
<pre><code>AuthType Basic
AuthName "Authorization Required"
AuthUserFile /etc/apache2/.htpasswd
Require valid-user
</code></pre>
<p>Check that the configuration file is okay:</p>
<pre><code>apachectl configtest
</code></pre>
<p>If you get a <strong>Syntax OK</strong> message, then restart Apache2 and check its status:</p>
<pre><code>sudo systemctl restart apache2
systemctl status apache2
</code></pre>
<h3 id="permissions-and-ownership"><a class="header" href="#permissions-and-ownership">Permissions and Ownership</a></h3>
<p>The Apache2 web server has a user account on your Linux server.
The account name is <strong>www-data</strong>, and it's account details are stored in the <strong>/etc/passwd</strong> file:</p>
<pre><code>grep "www-data" /etc/passwd
www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin
</code></pre>
<p>From the output, we can see that the <strong>www-apache</strong> user's home directory is <strong>/var/www</strong> and
its default shell is <strong>/usr/sbin/nologin</strong>.
See <code>man nologin</code> for details, but in short, the <code>nologin</code> prevents the <strong>www-data</strong> account to be able to login to a shell.</p>
<blockquote>
<p>You can compare the output of the above <code>grep</code> command with your account
information that is stored in <strong>/etc/passwd</strong>. Use the following command:
<code>grep $USER /etc/passwd</code> to do so. You'll see, for example, that your home
directory is listed there as well as your default shell, which is <code>bash</code>.</p>
</blockquote>
<p>The benefit with having Apache2 a user is that we can limit file permissions and ownership to this user.</p>
<p>The general guidelines for this are as follows:</p>
<ul>
<li>Static files (like HTML, CSS, JS) might not need to be writable by the Apache
server, so they could be owned by a different user (like your own user
account) but be readable by <strong>www-data</strong>.</li>
<li>Directories where Apache needs to write data (like upload directories) or
applications that need write access should be owned by <strong>www-data</strong>.</li>
<li>Configuration files (incl. files like <strong>login.php</strong>) should be readable by
<strong>www-data</strong> but not writable, to prevent unauthorized modifications.</li>
</ul>
<p>We can initiate this guidelines with the <code>chown</code> and <code>chmod</code> commands:</p>
<ol>
<li>
<p>Change the group ownership of <strong>/var/www/html</strong> to <strong>www-data</strong>:</p>
<pre><code> sudo chown :www-data /var/www/html
</code></pre>
</li>
<li>
<p>Set the <strong>setgid bit</strong> on <strong>/var/www/html</strong>. This command makes it so that
any new files and directories created within <strong>/var/www/html</strong> will inherit
the group ownership of the parent directory (<strong>www-data</strong>, in this case).
While this ensures that group ownership is inherited, the user ownership of
new files will still be the user that creates the files. In our case, since
we use <code>sudo</code> to work in this directory, that means that the user owner for
subsequent files and directories will be the Linux <strong>root</strong> user.</p>
<pre><code> sudo chmod -R g+s /var/www/html
</code></pre>
</li>
</ol>
<h2 id="get-cataloging"><a class="header" href="#get-cataloging">Get Cataloging!</a></h2>
<p>Now visit your cataloging module.
You should be required to enter the username and password that you created with <code>htpasswd</code>.</p>
<h2 id="conclusion-13"><a class="header" href="#conclusion-13">Conclusion</a></h2>
<p>In the last lesson, we created a very bare bones OPAC that would allow patrons to search our catalog.
In this lesson, we learned how to create a bare bones cataloging module that would allow librarians
to add bibliographic data and records to the OPAC.</p>
<p>Now try this:</p>
<ol>
<li>Add some records using the above form, and then return to your OPAC and
conduct some queries to confirm that the new records have been added.</li>
<li>Use the MySQL command line interface to view the new records, just
like we did a couple of lessons ago.</li>
</ol>
<p>In a production level environment, we would add quite a bit more functionality and security.
Our MySQL database would contain many more tables that allow storing data related to the modules listed above.
We would also like to make our modules graphically attractive and provide more content.
That would mean we would add <a href="https://en.wikipedia.org/wiki/CSS">Cascading Style Sheets (CSS)</a> and <a href="https://en.wikipedia.org/wiki/JavaScript">JavaScript</a>
to create an attractive and usable interface.
But that would be a whole other book.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installing-content-management-systems"><a class="header" href="#installing-content-management-systems">Installing Content Management Systems</a></h1>
<p>Many library websites are compositions of interconnected resources.
For example, a library may have a front-facing website that provides
basic information about the library, its physical locations, and its services.
That front-facing website may be connected to an integrated library system (ILS) or
library service platform (LSP) that is itself a different website.
The front-facing website may also be connected to other sites that provide access to all sorts of databases.
In the end, this means that a library website is not just one place.
It is much more like a series of interconnected buildings, each of which has its own entry points.</p>
<p>In this section, we will learn how to build these interconnected resources.
First, we learn how to use <a href="https://wordpress.org/">WordPress</a> to setup a library's front-facing web presence.
The basic process is similar to the process we used when building our bare bones OPAC.
We will then use the instructions for the WordPress install to install and configure <a href="https://omeka.org/">Omeka</a>,
which we might imagine is used to build a digital library for our library.</p>
<p>At the end of this section, we will have begun creating a library web presence that is more than a basic front-facing web presence.
Instead, it will start the infrastructure for an ecosystem that provides access to all sorts of library resources.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="install-wordpress"><a class="header" href="#install-wordpress">Install WordPress</a></h1>
<h2 id="introduction-9"><a class="header" href="#introduction-9">Introduction</a></h2>
<p><a href="https://en.wikipedia.org/wiki/WordPress">WordPress</a> is a free and open source content management system (CMS).
Originally, its focus was on providing a platform for blogging, but it has become a general purpose CMS that can serve as a website builder.</p>
<p>Two main sites exist to provide access to WordPress: <a href="https://wordpress.com">WordPress.com</a> and <a href="https://wordpress.org">WordPress.org</a>.
WordPress.com is a hosting solution, which means that customers can sign up and create a free WordPress site.
Since its hosted, customers are only responsible for their content and not for managing the core WordPress installation and its updates.
Various paid plans can extend the functionality offered to WordPress.com customers.</p>
<p>WordPress.org is maintained by the <a href="https://wordpressfoundation.org/">WordPress Foundation</a>, which
oversees the development of and provides access to the software.
When we download the WordPress software, we download it from WordPress.org.
Unlike the hosted solution, when we install and setup WordPress on our own servers,
we become responsible for administrating its installation and for keeping the software updated.</p>
<p>WordPress is widely used software, and because of that, it's often the focus of attack.
Take a moment to read about the developer's efforts to protect WordPress: <a href="https://wordpress.org/about/security/">Security</a>.
We will not need to update our WordPress installs during the course of this course, but
you should be familiar with the update process in case you decide to maintain your install or an install at a future date:
<a href="https://wordpress.org/documentation/article/updating-wordpress/">Updating WordPress</a>.</p>
<h2 id="libraries-and-wordpress"><a class="header" href="#libraries-and-wordpress">Libraries and WordPress</a></h2>
<p>Many libraries use WordPress as as their main website and a quick web search will reveal them.
For example, I quickly found an example of a (beautiful) WordPress library site for the
<a href="https://readingpl.org/">Reading Public Library (RPL)</a> in Massachusetts.
These library websites coordinate with additional solutions that provide integrated library systems and other electronic resource services.
RPL, for instance, connects their WordPress installation, which serves as their main website page,
with the open source <a href="https://evergreen-ils.org/">Evergreen ILS</a>, which serves their OPAC.
Check this by clicking on RPL's <strong>Library Catalog</strong> link, and you will see that it takes you to a different URL.</p>
<blockquote>
<p>Aside: it is this need to coordinate so many services across all these websites that in part drives the need to
develop standards for data exchange and work flow processes.
This topic is covered in my <a href="https://cseanburns.github.io/electronic_resource_mgmt/">electronic resource management</a> textbook.</p>
</blockquote>
<p>Many library websites are partitioned like this.
Thus, when we install WordPress soon, it is as if we are only installing the <em>front entrance</em> to the library.
Libraries are generally like this.
They have one main website (like https://libraries.uky.edu) but then connect to other sites that provide access
to OPACS, discovery systems, eBook vendors, bibliographic databases, and more.
This is part of the confusion around how libraries provide electronic resources.
There are efforts to make all these components connect more seamlessly (e.g., through <a href="https://en.wikipedia.org/wiki/Discovery_system_(bibliographic_search)">discovery systems</a>),
but if we were to model this to the walking around world, it would be like having a library that has multiple buildings,
where each building provides one thing:</p>
<ul>
<li>one building for books,</li>
<li>one building for journals,</li>
<li>another building for other journals,</li>
<li>another building for another set of journals,</li>
<li>another building for looking up where to find journals,</li>
<li>another building for special collections, and so on.</li>
</ul>
<p>I digress.</p>
<p>You can read the announcement about RPL's WordPress launch at:
<a href="https://web.archive.org/web/20230927082116/https://www.bartlettinteractive.com/blog/libraries-using-wordpress">Reading Public Library Launches New WordPress Site</a>.
The announcement describes how various plugins were used to offer patrons additional functionality and
describes other basic changes that come with the new site.
The plugins they added display business hours and help manage events and event attendees.</p>
<p>Plugins are often used with WordPress sites to offer all sorts of additional capabilities.
Currently, there are nearly <a href="https://wordpress.org/plugins/">60 thousand plugins</a> available for WordPress, but
some are of higher quality and utility than others.
In addition to the thousands of available plugins, there are nearly <a href="https://wordpress.org/themes/">12 thousand free themes</a> for WordPress sites.
Plus, many businesses offer paid themes or can create customized themes based on customer needs.
These themes can drastically alter the appearance and usability of a WordPress site or cater a site for a specific clientele,
such as a library.</p>
<h2 id="installation-2"><a class="header" href="#installation-2">Installation</a></h2>
<p>So far I have shown you how to install software using two methods:</p>
<ul>
<li>using the <code>apt</code> command</li>
<li>downloading from GitHub</li>
</ul>
<p>In this lesson, we are going to install WordPress by downloading the most recent version from WordPress.org and installing it manually.
The WordPress application is available via the <code>apt</code> command, but the <code>apt</code> process makes it a bit more confusing than it should be, oddly.</p>
<p>We are going to <em>kind of</em> follow the documentation provided by WordPress.org.
You should read through the documentation <em><strong>before</strong></em> following my instructions, but
then follow the process I outline here instead because the documentation uses some different tools than we'll use.</p>
<p>Another reason we do this manually is because it builds on what we have learned by building our bare bones ILS.
That is, the two processes are similar.
In both cases, we create a specific database for our platform, we create a specific user for that database,
and we provide login credentials in a specific file.</p>
<p>First, <em>read through</em> <strong>but don't follow</strong> the following instructions:</p>
<p><a href="https://wordpress.org/documentation/article/how-to-install-wordpress/">How to install WordPress</a></p>
<h2 id="customized-installation-process"><a class="header" href="#customized-installation-process">Customized Installation Process</a></h2>
<p>After you have read through the WordPress.org documentation, follow the steps below to complete the manual install:</p>
<h3 id="step-1-requirements"><a class="header" href="#step-1-requirements">Step 1: Requirements</a></h3>
<p>All major software has dependencies.
For example, our bare bones OPAC depends on MySQL and PHP to provide the database (MySQL) and
the glue (PHP) between our HTML and the database.
The same is true for WordPress.
However, since WordPress is much more complicated software than our bare bones OPAC, its dependencies are stricter.
This means that when we plan to download software outside of the <code>apt</code> ecosystem,
we need to make sure that our systems meet the requirements for our installation.
The <a href="https://wordpress.org/about/requirements/">WordPress.org Requirements</a> page states that the WordPress installation requires
at least PHP version 7.4 and MySQL version 8.0 or greater.
We can check that our systems meet these requirements with the following commands.
To check our installed version of PHP:</p>
<pre><code>php --version
</code></pre>
<p>To check our installed version of MySQL:</p>
<pre><code>mysql --version
</code></pre>
<p>The output from <code>php --version</code> shows that my systems have PHP 8.1.2, which is greater than PHP 7.4.
The output from <code>mysql --version</code> show that our systems have MySQL 8.0.41, which is greater than MySQL 8.0.
This should be the same for you if you're running the Ubuntu 22.04.5 LTS Linux distribution.
You can check that with the following command:</p>
<pre><code>cat /etc/issue.net
</code></pre>
<p>Since the system meets the PHP and MySQL requirements, it means I can proceed.</p>
<blockquote>
<p>As always, be sure to update your system with the <code>apt</code> commands: <code>sudo apt update</code> etc.</p>
</blockquote>
<p>Next, we need to add some additional PHP modules to our system to let WordPress operate at full functionality.
We can install these using the <code>apt</code> command:</p>
<pre><code>sudo apt install php-curl php-xml php-imagick php-mbstring php-zip php-intl
</code></pre>
<p>Then restart Apache2 and MySQL:</p>
<pre><code>sudo systemctl restart apache2
sudo systemctl restart mysql
</code></pre>
<h3 id="step-2-download-and-extract"><a class="header" href="#step-2-download-and-extract">Step 2: Download and Extract</a></h3>
<p>The next step is to download and extract the WordPress software, which is downloaded as a <code>zip</code> file.
Although we only download one file, when we extract it with the <code>unzip</code> command,
the extraction will result in a new directory that contains multiple files and subdirectories.
The general instructions include:</p>
<ol>
<li>Change to the <strong>/var/www/html</strong> directory.</li>
<li>Download the latest version of WordPress using the <code>wget</code> program.</li>
<li>Extract the package using the <code>unzip</code> program.</li>
</ol>
<p>Specifically, we do the following on the command line:</p>
<pre><code>cd /var/www/html
sudo wget https://wordpress.org/latest.zip
sudo unzip latest.zip
</code></pre>
<p>As noted in the WordPress documentation, this will create a directory called <code>wordpress</code> in the same directory.
Therefore the full path of your installation will located at <code>/var/www/html/wordpress</code>.</p>
<h3 id="step-3-create-the-database-and-a-user"><a class="header" href="#step-3-create-the-database-and-a-user">Step 3: Create the Database and a User</a></h3>
<p>The WordPress documentation describes how to use <code>phpMyAdmin</code> to create the database and a user for WordPress.
<code>phpMyAdmin</code> is a graphical front end to the MySQL relational database that you would access through the browser.
We are not going to install that because I like to minimize the software that we install
on servers to reduce the server's security exposure.
Though you can use <code>phpMyAdmin</code> from a different machine and connect to the server, this is a command line class.
Therefore, we are going to create the WordPress database and a database user using the same process we used
to create a database and user for our bare bones ILS.
You already know this, but the general instructions are:</p>
<ol>
<li>Switch to the root Linux user</li>
<li>Login as the MySQL root user</li>
</ol>
<p>Specifically, we do the following on the command line:</p>
<pre><code>sudo su
mysql -u root
</code></pre>
<p>The <code>mysql -u root</code> command places us in the MySQL command prompt.
The next general instructions are to:</p>
<ol>
<li>Create a new user for the WordPress database</li>
<li>Be sure to replace the <code>X</code>s with a strong password</li>
<li>Create a new database for WordPress</li>
<li>Grant all privileges to the new user for the new database</li>
<li>Examine the output</li>
<li>Exit the MySQL prompt</li>
</ol>
<p>Specifically, this means the following (be sure to replaces the <code>X</code>s with a unique and strong password of your own):</p>
<pre><code>create user 'wordpress'@'localhost' identified by 'XXXXXXXXX';
create database wordpress;
grant all privileges on wordpress.* to 'wordpress'@'localhost';
show databases;
\q
</code></pre>
<h3 id="step-4-set-up-wp-configphp"><a class="header" href="#step-4-set-up-wp-configphp">Step 4: Set up <code>wp-config.php</code></a></h3>
<p>When we created our bare bones ILS, we created a file called <code>login.php</code> that contained the name of the database (e.g., <code>opacdb</code>),
the name of the database user (e.g., <code>opacuser</code>), and the user's password.
WordPress follows a similar process, but instead of <code>login.php</code>, it uses a file called <code>wp-config.php</code>.</p>
<p>Follow these general steps:</p>
<ol>
<li>Change to the <code>wordpress</code> directory, if you haven't already.</li>
<li>Copy and rename the <code>wp-config-sample.php</code> file to <code>wp-config.php</code>.</li>
<li>Edit the file and add your WordPress database name, user name, and password in the fields for:
<ul>
<li><code>DB_NAME</code>,</li>
<li><code>DB_USER</code>, and</li>
<li><code>DB_PASSWORD</code>.</li>
</ul>
</li>
</ol>
<p>This means that we specifically do the following:</p>
<pre><code>cd /var/www/html/wordpress
sudo cp wp-config-sample.php wp-config.php
sudo nano wp-config.php
</code></pre>
<p>Using <code>nano</code>, add your database name, user, and password in the appropriate fields,
just like we did with our <code>login.php</code> file for our bare bones OPAC.</p>
<p>Additionally, we want to disable FTP uploads to the site for security reasons.
To do that, navigate to the end of the file and add the following line:</p>
<pre><code>define('FS_METHOD','direct');
</code></pre>
<h3 id="step-5-optional"><a class="header" href="#step-5-optional">Step 5: <strong>Optional</strong></a></h3>
<p>The WordPress files are now installed at <code>/var/www/html/wordpress</code>.
This means that your site would be located at a URL like:</p>
<pre><code>http://11.111.111.11/wordpress
</code></pre>
<p>If you want to, you can rename your <code>wordpress</code> directory to something else.
The WordPress documentation uses <code>blog</code> as an example.
But it could be something else, like the name of a fictional library that you might be using WordPress for to build a site.
If you decide to change it, be sure to keep the name lowercase and one word (no spaces and only alphabetic characters).
For example, if I want to change mine to <code>library</code>, then:</p>
<pre><code>sudo mv /var/www/html/wordpress /var/www/html/library
</code></pre>
<h3 id="step-6-change-file-ownership"><a class="header" href="#step-6-change-file-ownership">Step 6: Change File Ownership</a></h3>
<p>WordPress will need to write to files in the base directory.
Assuming you are still in your base directory, run the following command, which assumes that my directory is still named
<code>/var/www/html/wordpress</code>:</p>
<pre><code>sudo chown -R www-data:www-data /var/www/html/wordpress
</code></pre>
<h3 id="step-7-run-the-install-script"><a class="header" href="#step-7-run-the-install-script">Step 7: Run the Install Script</a></h3>
<p>The next part of the process takes place in the browser.
The location (URL) that you visit in the browser depends on your specific IP address and also
includes the name of the directory in <code>/var/www/html</code> that we extracted WordPress to or that you renamed if you followed <strong>Step 5</strong>.
Thus, if my IP address is 11.111.111.11 and I renamed my directory to <strong>library</strong>, then I need to visit the following URL:</p>
<pre><code>http://11.111.111.11/library/
</code></pre>
<p>Or if that doesn't work (though it should), try:</p>
<pre><code>http://11.111.111.11/library/wp-admin/install.php
</code></pre>
<p><strong>IF</strong> I kept the directory named <code>wordpress</code>, then this is the URL that I use:</p>
<pre><code>http://11.111.111.11/wordpress/
</code></pre>
<p>If you changed the name of your <code>wordpress</code> directory, be sure to substitute that name for <code>wordpress</code> in the URL.</p>
<h3 id="finishing-installation"><a class="header" href="#finishing-installation">Finishing Installation</a></h3>
<p>From this point forward, the steps to complete the installation are exactly the steps you follow using WordPress's documentation.</p>
<p>Most importantly, you should see a <strong>Welcome</strong> screen where you enter your site's information.
The site <strong>Username</strong> and <strong>Password</strong> <em>should not</em> be the same as the username and password you used
to create your WordPress database in MySQL.
Rather, the username and password you enter here are for WordPress website users; i.e.,
those who will add content and manage the website.</p>
<p><strong>Two things to note:</strong></p>
<p>We have not setup <strong>Email</strong> on our servers.
It's quite complicated to setup an email server correctly and securely, but
it wouldn't work well without having a domain name setup anyway.
So know that you probably should enter an email when setting up the user account, but it won't work.</p>
<p>Second, when visiting your site, your browser may throw an error.
Make sure that the URL is set to <code>http</code> and that it's not trying to access <code>https</code>.
Setting up an <code>https</code> site also generally requires a domain name, but we are not doing that here.
So if there are any problems accessing your site in the browser, be sure to check that the URL starts off with <code>http</code>.</p>
<h2 id="conclusion-14"><a class="header" href="#conclusion-14">Conclusion</a></h2>
<p>Congrats on setting up your WordPress library site.
It's now time to explore and build a website.
Use free themes and free plugins to alter the look of the site, its usability, and its functionality.
Try to create a nice looking website.
Generally, your goal for the next week is to create an attractive, yet fictional, <em>front entrance</em> for a library website.
It's also a break from the command line!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="install-omeka"><a class="header" href="#install-omeka">Install Omeka</a></h1>
<p><a href="https://omeka.org/">Omeka</a> is an <q>Open-source web publishing platforms for sharing digital collections and creating media-rich online exhibits.</q>
Most if not all of you have already used Omeka in a prior course.
Here our task is not to practice information/knowledge organization, but to learn how to administer the Omeka digital library platform.</p>
<h2 id="the-task"><a class="header" href="#the-task">The Task</a></h2>
<p>So far we have created a:</p>
<ul>
<li>bare bones OPAC/ILS, and</li>
<li>downloaded, installed, and configured WordPress on our servers.</li>
</ul>
<p>We will use the same basic process to download, install, and configure Omeka.</p>
<p>Instead of providing comprehensive instructions, your goal is to take what you learned from the
bare bones OPAC/ILS and WordPress assignments, and apply them to the Omeka installation and setup.
Below are some additional <strong>prerequisites</strong> that you should complete first.
After you've completed them, move on to the <strong>General Steps</strong> section to
remind yourself of the overall process.</p>
<p>You can do it!</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p>When we installed WordPress, we installed most of the prerequisites that Omeka needs, but
there are a couple of additional things we need to do.</p>
<p>Some prerequisites:</p>
<ul>
<li>Make sure your system is fully updated first: <code>sudo apt update</code> etc.</li>
<li>Check that you installed versions of PHP and MySQL meet <a href="https://omeka.org/classic/docs/Installation/System_Requirements/">Omeka's system requirements</a>.</li>
<li>Install <a href="https://imagemagick.org/index.php">ImageMagick</a>: this is a suite of utilities to work with photo files.
Omeka uses ImageMagick to create thumbnail images of photos uploaded to the digital library.
Visit the ImageMagick link above for more information.</li>
</ul>
<pre><code>sudo apt install imagemagick
</code></pre>
<ul>
<li>Enable Apache <code>mod_rewrite</code>.
This is an Apache module used to rewrite URLs.
Omeka uses this to create user friendly URLs for items and collections in its digital libraries.</li>
</ul>
<pre><code>sudo a2enmod rewrite
</code></pre>
<p>You should be instructed to restart Apache after enabling <code>mod_rewrite</code>:</p>
<pre><code>sudo systemctl restart apache2
</code></pre>
<h2 id="general-steps"><a class="header" href="#general-steps">General Steps</a></h2>
<p>Below is a list of the general steps you need to use to install Omeka.
Generally, you have already completed these steps when you created a bare bones ILS and installed WordPress.
Your task is to apply what you've learned when doing those prior assignments by completing an Omeka installation on your own.</p>
<p><strong>Note</strong>: let me emphasize that the process is very similar to what
we have already done with our bare bones ILS and our WordPress installations.
Use this handbook to remind you of the specific commands.</p>
<p>In short, you are going to complete the following steps:</p>
<ul>
<li>Create a new user and a new database in MySQL for the Omeka installation
(do not re-use the WordPress database, user, etc credentials or names of databases or tables).</li>
<li>Use <code>wget</code> from your server to download Omeka Classic as a Zip file and extract it in <code>/var/www/html</code>:
<ul>
<li>https://github.com/omeka/Omeka/releases/download/v3.1.2/omeka-3.1.2.zip</li>
<li>Unzip it with the <code>unzip</code> command, which you might have to install with the <code>apt</code> command.</li>
<li>The extracted directory will be named <code>omeka-3.1.2</code>.</li>
<li>You want to <strong>rename</strong> it simply <code>omeka</code> or something else of your choosing (like <code>digital_library</code>).
Remember that names of files and directories should not have spaces in them.</li>
</ul>
</li>
<li>In the extracted directory, find the <code>db.ini</code> file and add your new database credentials.
Replace all values containing <code>XXXXXX</code> with the appropriate information.
This is the same thing we did with the <code>login.php</code> file for our bare bones ILS and the <code>wp-config.php</code> file for WordPress.</li>
<li>Use the <code>chown</code> command like we did with WordPress on the <code>files</code> directory in the <code>omeka</code> directory.
However, the <strong>user AND owner</strong> should be owned by <code>www-data</code>. <strong>NOTE: This is necessary!!!</strong></li>
<li>Restart Apache2 and MySQL.</li>
<li>In your web browser, go to <code>http://your-ip-address/omeka/</code> and complete the setup via the web form, just like you did with WordPress.</li>
</ul>
<h2 id="helpful-links"><a class="header" href="#helpful-links">Helpful Links</a></h2>
<p><strong>Note</strong>: The user manual below is helpful, but it does not provide explicit instructions.</p>
<p>Be sure to download <strong>Omeka Classic</strong> and not <strong>Omeka S</strong>.</p>
<ul>
<li>Omeka: <a href="https://omeka.org/">https://omeka.org/</a></li>
<li>Omeka Classic: <a href="https://omeka.org/classic/">https://omeka.org/classic/</a></li>
<li>Omeka Classic User Manual: <a href="https://omeka.org/classic/docs/">https://omeka.org/classic/docs/</a></li>
</ul>
<h2 id="conclusion-15"><a class="header" href="#conclusion-15">Conclusion</a></h2>
<p>The purpose of this exercise is to apply what you've learned in
creating, installing, and setting up both the bare bones ILS and WordPress on your systems.
The same basic logic is used in all these processes, even if the specifics vary.
But all in all:
Have fun.
Go slow.
Read the documentation.
Pay attention to the details.
Use this textbook to search for how we did things in prior installations.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="install-the-koha-ils"><a class="header" href="#install-the-koha-ils">Install the Koha ILS</a></h1>
<h2 id="introduction-10"><a class="header" href="#introduction-10">Introduction</a></h2>
<p>In the prior sections, we built a WordPress site that functions as our library's front-facing presence.
Then we built an Omeka site that could serve as our library's digital library.
In this section, we complete our library web infrastructure by installing the <a href="https://koha-community.org/">Koha ILS</a>.</p>
<p>Koha is a free and open source library system that provides modules
for patron accounts, circulation, cataloging, serials, an OPAC, and more.
The process of installing and using Koha is more complicated
than the processes we used to install and use WordPress and Omeka.
This is because Koha, like other ILS software,
is a complex project that must provide a lot of different functionality for a library and its patrons.
Fortunately, the documentation makes the process pretty straightforward.
We will rely on that documentation and other resources to install Koha and complete our library's interconnected web presence.</p>
<h2 id="koha-ils"><a class="header" href="#koha-ils">Koha ILS</a></h2>
<p>Koha is an open source <q>library management system</q>,
otherwise called an <a href="https://en.wikipedia.org/wiki/Integrated_library_system">integrated library system (ILS)</a>.
These systems provide modules that perform specific ranges of functionality.
Koha's modules include:</p>
<ul>
<li>Administration</li>
<li>Patron management</li>
<li>Cash management</li>
<li>Circulation</li>
<li>Cataloging</li>
<li>Course reserves</li>
<li>Serials</li>
<li>Acquisitions</li>
<li>Reports</li>
<li>OPAC</li>
</ul>
<p>According to <a href="https://librarytechnology.org/product/koha">Library Technology Guides</a> (April 2025),
<q>Koha has been installed in 4,484 libraries [around the world], spanning 6,273 facilities or branches.</q>
Most installations are in medium sized or small libraries.
Koha is well represented in academic libraries, but the majority of installations are in public libraries.</p>
<p>Although Koha is an open source ILS and free to download, install, and administer without external support,
librarians can hire companies that support open source library management solutions,
like <a href="https://bywatersolutions.com/">ByWater Solutions</a> or the <a href="https://www.equinoxoli.org/">Equinox Open Library Initiative</a>
These companies support ILS migration, hosting, training, and more.
They also provide support for other library software services,
such as open source discovery systems and electronic resource management systems.</p>
<p>In addition to Koha, <a href="https://evergreen-ils.org/">Evergreen</a> is an open source integrated library system.
According to <a href="https://librarytechnology.org/product/evergreen-equinox">Library Technology Guides</a>, Evergreen is primarily installed at small and medium
size public libraries, and most installations are in the U.S. and Canada.</p>
<p>There is currently a migration to what has been called <em>library service platforms (LSP)</em> in recent years.
The LSP is a next generation ILS designed from the start to integrate electronic resources.
For example, the ILS has an OPAC that was designed to search a library's print collections.
Modern OPACs have been adapted for electronic resources,
but they are still limited because of the older design model.
LSPs use a <em>discovery service</em> instead of an OPAC.
Discovery services are designed to search a library's entire collection, including the content in third party databases and journals.
Example LSPs include Ex Libris Primo (used by UK Libraries),
OCLC's WorldCat Discovery Service, and open source solutions
like <a href="https://bywatersolutions.com/products/aspen-discovery">Aspen Discovery</a> and <a href="https://vufind.org/vufind/">VuFind</a>.</p>
<p>The integration of library systems like the ILS and the LSP is a major aspect of library services.
When we visit a library's website,
we first interact with a normal website that might be built on
WordPress, <a href="https://www.drupal.org/">Drupal</a>, or some other content management system.
These websites will link to the public facing components of an ILS or LSP, as well as other services,
such as bibliographic databases, journal publishers, ebook services, and more.
It may therefore be the systems librarians job to help build and connect these services.
In this demo, we will continue that work by installing, configuring, and setting up the Koha ILS.</p>
<h2 id="google-cloud-setup"><a class="header" href="#google-cloud-setup">Google Cloud Setup</a></h2>
<p>Before we begin to install Koha, we need to create a new virtual machine instance
and configure the Google firewall to allow HTTP traffic to our Koha install.</p>
<h3 id="new-virtual-instance"><a class="header" href="#new-virtual-instance">New Virtual Instance</a></h3>
<p>The virtual instances we have been using do not meet the memory (RAM) needs
required by the Koha integrated library system.
We therefore need to create a new virtual instance that has more RAM.
I will also use a bigger disk, to be sure, since Koha takes up more disk space.
Check <a href="https://koha-community.org/download-koha/">Koha System Requirements</a> for details.</p>
<p>As a refresher for creating a VM,
see the section titled <strong>gcloud VM Instance</strong> at <a href="2a-using-gcloud-virtual-machines.html">Using gcloud Virtual Machines</a>.
In this lesson,
we will use for the <strong>Series</strong> an <strong>E2</strong> and set
the <strong>Machine Type</strong> to <strong>2 vCPU, 4 GB memory</strong>, and up the disk size to 20GB.
Under <strong>Networking</strong>, click on <strong>Allow HTTP traffic</strong>.
In the <strong>Network tags</strong> box, add the following tag name: <code>koha-8080</code>.</p>
<p>All else, including the operating system (Ubuntu 22.04), should remain the same.
Note that this is a more expensive setup.
Therefore, feel free to delete this instance at the end of the semester to avoid incurring extra costs.</p>
<h3 id="google-cloud-firewall"><a class="header" href="#google-cloud-firewall">Google Cloud firewall</a></h3>
<p>Later, after we install Koha, we will need to access the staff interface on a special port for HTTP data.
All internet traffic to a server contains metadata that identifies itself by port numbers.
The default port for HTTP is 80, and the default port for HTTPS (encrypted) is 443.
Since we do not have encryption enabled, this means we will only use port 80, but
the staff interface will be identified by port 8080.</p>
<blockquote>
<p>How does a server know where to send internet traffic?
Internet data is <em>packaged</em> in many forms.
One of the most common forms are TCP packets.
These packets contain <em>header</em> information that names the source IP, destination IP, source port, and destination port.
When TCP packets arrive at a destination server,the operating system inspects the packet header for the port number.
The OS looks up the port number in a table that contains a mapping of ports to applications.
When the OS makes the match, it sends the TCP packets to the application.
In its default setup, the Apache2 web server handles traffic on port 80.</p>
</blockquote>
<p>Firewalls are used to control incoming and outgoing traffic via ports.
We selected <strong>Allow HTTP traffic</strong> when we created our virtual instance on Google Cloud,
and we instructed the Google Console firewall to allow traffic through port 80.
We need to add a firewall rule to allow web traffic through port 8080.
We will use port 8080 to access the Koha staff interface.</p>
<blockquote>
<p>Please take a moment to read more about ports: <a href="https://www.cloudflare.com/learning/network-layer/what-is-a-computer-port/">What is a computer port? | Ports in networking</a>.</p>
</blockquote>
<p>To create a firewall rule to allow traffic to port 8080, go to the Google Cloud Console:</p>
<ul>
<li>Click on the <em>hamburger</em> icon.</li>
<li>Click on <strong>VPN Network</strong>.</li>
<li>Click on <strong>Firewall</strong>.</li>
<li>At the top of the page, choose <strong>Create a firewall rule</strong> (Do not choose <strong>Create a firewall policy</strong>):
<ul>
<li>Add name: <code>koha-opac</code></li>
<li>Add description: <strong>Open port 8080 for the OPAC</strong></li>
</ul>
</li>
<li>Next to <strong>Targets</strong>, click on <strong>Specified target tags</strong>.</li>
<li>In <strong>Target tags</strong>, add our tag name: <code>koha-8080</code>.</li>
<li>In the <strong>Source IPv4 ranges</strong>, add <strong>0.0.0.0/0</strong></li>
<li>Click on <strong>Specified protocols and ports</strong>
<ul>
<li>Click on TCP</li>
<li>Add <strong>8080</strong> in the <strong>Ports</strong> box</li>
</ul>
</li>
<li>Click on <strong>Create</strong></li>
</ul>
<h2 id="install-koha-repo"><a class="header" href="#install-koha-repo">Install Koha Repo</a></h2>
<h3 id="server-setup"><a class="header" href="#server-setup">Server setup</a></h3>
<p>Now let's log onto our new server and prepare it for the Koha installation.</p>
<p>First we need to update our local repositories:</p>
<pre><code>sudo apt update
</code></pre>
<p>And then upgrade our servers:</p>
<pre><code>sudo apt upgrade
</code></pre>
<p>The next two commands help save disk space.
As a reminder, the <code>apt autoremove</code> command
<q>is used to remove packages that were automatically installed to satisfy dependencies for other packages and
are now no longer needed as dependencies changed or the package(s) needing them were removed in the meantime</q> (see <code>man apt</code>).
The <code>apt clean</code> command <q>clears out the local repository of retrieved package files</q> (see <code>man apt-get</code>).
In the following example, I combine both commands on one line:</p>
<pre><code>sudo apt autoremove -y &amp;&amp; sudo apt clean
</code></pre>
<p>Next we need to install <code>gnupg2</code> and <code>apt-transport-https</code>.
<code>gnupg2</code> is used to create digital signatures, encrypt data, and aid in secure communication.</p>
<pre><code>sudo apt install gnupg2 apt-transport-https
</code></pre>
<p>At the time of this demo, the update above downloaded a new Linux kernel.
Using the new kernel requires a reboot.
The reboot command will disconnect you from the server.
If you need to reboot your server, use the command below, then wait a minute or so and re-connect.</p>
<pre><code>sudo reboot now
</code></pre>
<h3 id="add-koha-repository"><a class="header" href="#add-koha-repository">Add Koha Repository</a></h3>
<p>When you run the <code>sudo apt update</code> command,
Ubuntu syncs the local repository database with several remote repositories.
These remote repositories contain metadata about the packages they contain.
The syncing process identifies if any new software updates are available.
The remote repositories are also used to retrieve software.</p>
<p>We can add repositories to sync with and to use to download software, and this includes the Koha ILS.
To add the special Koha repository to our system, we use the following command:</p>
<blockquote>
<p>Most of the following commands require administrator access.
Therefore, I will login as the <code>root</code> user to make it a bit easier.
If you do not want to log in as the root user, be sure to use the <code>sudo</code> command.</p>
</blockquote>
<pre><code>sudo su
</code></pre>
<p>Add the Koha repository to our server:</p>
<pre><code>echo 'deb http://debian.koha-community.org/koha stable main' | sudo tee /etc/apt/sources.list.d/koha.list
</code></pre>
<p>Now, download and install the GPG key in the <code>trusted.gpg.d</code> directory:</p>
<pre><code>wget -qO- https://debian.koha-community.org/koha/gpg.asc | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/koha.gpg &gt; /dev/null
</code></pre>
<p>You can inspect the key and make sure it was signed by the relevant people:</p>
<pre><code>gpg --show-keys /etc/apt/trusted.gpg.d/koha.gpg
</code></pre>
<p>The output should include an email from the <code>koha-community.org</code>.</p>
<h2 id="install-koha"><a class="header" href="#install-koha">Install Koha</a></h2>
<p>Next we need to update/sync the new repository with the Koha remote repository.
This just means that we use <code>apt update</code> again.</p>
<pre><code>apt update
</code></pre>
<p>Now we view the package information for Koha:</p>
<pre><code>apt show koha-common
</code></pre>
<p>And install it:</p>
<pre><code>apt install koha-common
</code></pre>
<p>The above command will download and install a lot of additional software, and therefore the process will take several minutes.</p>
<h3 id="configure-koha"><a class="header" href="#configure-koha">Configure Koha</a></h3>
<p>Next we need to edit some configuration files for Koha.
First, create a backup of the default configuration file:</p>
<pre><code>cd /etc/koha/
cp koha-sites.conf koha-sites.conf.backup
</code></pre>
<p>Now open the configuration file in <code>nano</code> or your preferred text editor:</p>
<pre><code>nano /etc/koha/koha-sites.conf
</code></pre>
<p>In the <code>koha-sites.conf</code> file, change the line that contains the following information:</p>
<pre><code>INTRAPORT="80"
</code></pre>
<p>To:</p>
<pre><code>INTRAPORT="8080"
</code></pre>
<p>Next install and setup <code>mysql-server</code>:</p>
<pre><code>apt install mysql-server
</code></pre>
<p>Next we set the root MySQL password.
Replace <code>X</code>s with your password:</p>
<pre><code>mysqladmin -u root password XXXXXXXX
</code></pre>
<p>When we installed Koha, the Apache2 web server was installed with it as a prerequisite.
We need to enable URL rewriting and <a href="https://en.wikipedia.org/wiki/Common_Gateway_Interface">CGI</a> functionality.</p>
<pre><code>a2enmod rewrite
a2enmod cgi 
</code></pre>
<p>Now we need to restart Apache2 in the normal way:</p>
<pre><code>systemctl restart apache2
</code></pre>
<p>Next we create a database for Koha:</p>
<pre><code>koha-create --create-db bibliolib
</code></pre>
<p>We need to tell Apache2 to listen on port 8080:</p>
<pre><code>nano /etc/apache2/ports.conf 
</code></pre>
<p>And under the <code>Listen 80</code> line, add:</p>
<pre><code>Listen 8080
</code></pre>
<p>Make sure Apache configuration changes are valid:</p>
<pre><code>apachectl configtest
</code></pre>
<p>If you get an error message, trace the error in the file and line listed.</p>
<p>Let's restart Apache2.</p>
<pre><code>systemctl restart apache2
</code></pre>
<p>We'll disable the default Apache2 setup, enable traffic compression using <code>deflate</code>,
enable the <strong>bibliolib</strong> site, and then reload Apache2's configurations and restart again:</p>
<pre><code>a2dissite 000-default
a2enmod deflate
a2ensite bibliolib
systemctl reload apache2
systemctl restart apache2
</code></pre>
<h3 id="koha-web-installer"><a class="header" href="#koha-web-installer">Koha Web Installer</a></h3>
<p>All the back end work is complete, and like we did with WordPress and Omeka,
we can complete the installation through a web installer.</p>
<p>First, get Koha username and password in the following file:</p>
<pre><code>nano /etc/koha/sites/bibliolib/koha-conf.xml
</code></pre>
<p>Look for the <code>&lt;config&gt;</code> stanza (line number 252) and
the line beginning with <code>&lt;user&gt;</code> (line number 257).
The password is on the line after (line number 258).
You will need this info to login to the Koha web interface.</p>
<p>Make sure your URL begins with <code>http</code> and not <code>https</code>, and visit the web installer at:</p>
<pre><code>http://IP-ADDRESS:8080
</code></pre>
<p>The documentation for the web installer is helpful.
Enter the username and password from the <code>koha-conf.xml</code> file.
Note that it might take a while to step through the installer.</p>
<p>One thing to do is to add sample libraries and sample patrons during the install.
More generally, be sure to follow instructions as you click through each step.
Add lots of samples to play around with after the install completes.</p>
<p>When you are on the last page of the install, create an <strong>Administrator identity</strong>, and
be sure to save this information.</p>
<p><a href="https://koha-community.org/manual//22.11/en/html/installation.html">Introduction to the Koha installation process</a></p>
<h2 id="public-opac"><a class="header" href="#public-opac">Public OPAC</a></h2>
<p>When the install and setup are complete, you can login with your admin credentials, and
you will have access to the staff interface.
To view the public facing OPAC, you need to make a setting change.
In Koha:</p>
<ul>
<li>Click on <strong>More</strong> in the top drop down box</li>
<li>Click on <strong>Administration</strong></li>
<li>Click on <strong>System Preferences</strong></li>
<li>Click on <strong>OPAC</strong> in the left hand side bar</li>
<li>Scroll down to the <code>OPACBaseURL</code> line.</li>
<li>Enter the IP address of your server: <code>http://IP-ADDRESS</code></li>
<li>Click on <strong>Save all OPAC Preferences</strong></li>
</ul>
<p>Once you save these preferences, you should be able to visit your public facing OPAC at the server IP address.</p>
<h2 id="additional-tasks"><a class="header" href="#additional-tasks">Additional Tasks</a></h2>
<p>Once you've installed and setup Koha, begin to learn the system.
Try the following:</p>
<ul>
<li>Create patron accounts</li>
<li>Create bibliographic records</li>
<li>Check out books to patrons</li>
<li>Delete patron circulation history</li>
</ul>
<h2 id="conclusion-16"><a class="header" href="#conclusion-16">Conclusion</a></h2>
<p>In this final section, you learned how to install and setup a Koha ILS installation on a Linux server.
Your next step is to use your WordPress install to finalize your public facing library website.
This website should include links to your Omeka-based digital library and
to your Koha-based OPAC.</p>
<p>Congratulations!</p>
<h2 id="references-3"><a class="header" href="#references-3">References</a></h2>
<p>Helpful documentation and demos:</p>
<ul>
<li><a href="https://koha-community.org/">Koha ILS</a> documentation.</li>
<li><a href="https://wiki.koha-community.org/wiki/Koha_on_Debian">Koha on Debian</a></li>
<li><a href="https://www.youtube.com/watch?v=mzUop9R4sKc">Install Koha on Google Cloud Platform</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="conclusion-17"><a class="header" href="#conclusion-17">Conclusion</a></h1>
<p>Hopefully this textbook helped you build a strong foundation in server-side systems administration and web development for libraries.
The skills covered here are applicable to all library types, including academic, public, school, special, and more.</p>
<p>To summarize, we began by learning how to navigate the Google Cloud console and use it to set up an Ubuntu Linux server.
From there, we developed fluency with the Linux command line and explored tools like command line text editors,
search utilities like <code>grep</code>, and package managers for updating, installing, and managing software.</p>
<p>We gained experience using Git and GitHub for version control and documentation.
Then we moved into hands-on projects by building a basic LAMP server and creating a bare bones library system.
This led us to develop more complicated library systems using three major platforms—WordPress, Omeka, and Koha—and
connecting them to form a unified web presence that mirrors library practice.</p>
<p>As you wrap up your work, don't forget to stop and delete all of your virtual machines in your Google Cloud project to avoid ongoing billing.
But I hope you continue to explore how to use virtual machines on Google Cloud or other cloud hosting solutions for library purposes.</p>
<p>&amp;mdashSean Burns</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
